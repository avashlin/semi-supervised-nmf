{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All Method\n",
    "\n",
    "Train NMF for each topic separately.\n",
    "\n",
    "Use all Wiki articles as Background Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burki\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "from omterms.interface import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import libs.synonyms as syn\n",
    "import libs.text_preprocess as tp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']\n",
    "\n",
    "def plot_radar_chart(doc_topic_cumul, doc, doc_names):\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variablecategories\n",
    "    \n",
    "    \n",
    "    schwartz_dist = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_dist.append(doc_topic_cumul[doc][categories.index(sch)])\n",
    "    \n",
    "    N = len(schwartz)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], schwartz)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "\n",
    "    # Plot each individual = each line of the data\n",
    "    # I don't do a loop, because plotting more than 3 groups makes the chart unreadable\n",
    "\n",
    "    # Ind1\n",
    "    values = list(schwartz_dist) + list(schwartz_dist[:1])\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "\n",
    "    # Add legend\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Schwartz Chart - \" + doc_names[doc])\n",
    "    plt.savefig(\"Schwartz_Chart_\" + str(doc))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def print_top_words(model, theme, tfidf_vectorizer, n_top_words, n_topics=3):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print(color.CYAN + color.BOLD + categories[theme] + color.END)\n",
    "    for topic_idx, topic in enumerate(model[theme].components_):\n",
    "        if topic_idx / n_topics == 1:\n",
    "            break\n",
    "        message = color.BOLD + \"Topic #%d: \" % topic_idx + color.END\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "def print_cumulative_train_doc_topics(data, doc_topic, doc, n_best):\n",
    "    test_theme = data.iloc[doc]['theme']\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + color.RED +  \" (\" + test_theme + \")\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "    \n",
    "def print_cumulative_test_doc_topics(W_test_norms, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    n_topics = W_test_norms.shape[2]\n",
    "    dt = W_test_norms[:,doc,:].flatten()\n",
    "    for i in dt.argsort()[:n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "\n",
    "        print(color.CYAN + color.BOLD + categories[i//n_topics] + color.END, end='')\n",
    "\n",
    "        print(\" (%d), %.2lf)  \" %(i%n_topics, dt[i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    for i in doc_topic[doc].argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i//3] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, doc_topic[doc][i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_train_results(doc_topic, doc, corpus, data):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_train_doc_topics(data, doc_topic, doc, 11) \n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "    \n",
    "def print_test_results(doc, W_test_high, W_test_norms, tfidf_test, pre_nmf_list, pre_tfidf_vectorizer, word_topic_scores, word_topic_sp,\n",
    "                       doc_names, pre_trained_doc, purity_score, word_count, only_doc_words):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + \": \" + doc_names[doc] + color.END)\n",
    "    #print()\n",
    "    #print(color.BOLD + \"Text: \" + color.END)\n",
    "    #print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    \n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_test_doc_topics(W_test_norms, doc, 11)\n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(W_test_high, doc, doc_names)\n",
    "    print()\n",
    "    \n",
    "    df_scores = schwartz_word_scores(W_test_norms[:,doc,:], tfidf_test[doc], word_topic_scores[:,:,doc,:], word_topic_sp[:,:,doc,:], pre_tfidf_vectorizer, purity_score, word_count, only_doc_words)    \n",
    "    \n",
    "    display(df_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulate_W(W, n_topics):\n",
    "    W_cumul = []\n",
    "    for d in W:\n",
    "        temp = []\n",
    "        for i in range(W.shape[1]//n_topics):\n",
    "            temp.append(d[i*n_topics:(i+1)*n_topics].sum())\n",
    "        W_cumul.append(temp)\n",
    "\n",
    "    W_cumul = np.asarray(W_cumul)\n",
    "    \n",
    "    return W_cumul\n",
    "\n",
    "def normalize_W(W):\n",
    "    W_cumul_norm = W/(W.sum(axis=1).reshape(W.shape[0], 1))\n",
    "    W_cumul_norm *= 100\n",
    "    \n",
    "    return W_cumul_norm\n",
    "\n",
    "def prepare_export(W, docs, doc_names, filepath):\n",
    "    schwartz_dist = []\n",
    "    for doc in range(len(docs)):\n",
    "        temp_dist = []\n",
    "        for sch in schwartz:\n",
    "            temp_dist.append(W[doc][categories.index(sch)])\n",
    "        schwartz_dist.append(temp_dist)\n",
    "    schwartz_dist = np.asarray(schwartz_dist)\n",
    "    \n",
    "    df = pd.DataFrame(data=schwartz_dist,index = range(len(schwartz_dist)), columns=schwartz)\n",
    "    df['Text'] = docs\n",
    "    df[\"name\"] = doc_names\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def export_to_excel(W, docs, doc_names, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put xlsx as file extension '''\n",
    "    \n",
    "    df = prepare_export(W, docs, doc_names, filepath)\n",
    "    df.to_excel(filepath)\n",
    "    return df\n",
    "\n",
    "def export_to_csv(W, docs, doc_names, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put csv as file extension '''\n",
    "    \n",
    "    df = prepare_export(W, docs, doc_names, filepath)\n",
    "    df.to_csv(filepath)\n",
    "    return df\n",
    "\n",
    "def export_word_scores_excel(W_test_norms, W_test_list, tfidf_test, doc_names, pre_trained_doc, filepath, purity_score=False, word_count=10, only_doc_words=False):\n",
    "    writer = pd.ExcelWriter(filepath, engine = 'xlsxwriter')\n",
    "    \n",
    "    pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    word_topic_scores, word_topic_sp = calculate_word_topic_scores(pre_nmf_list, W_test_list)\n",
    "    \n",
    "    for i, dn in enumerate(doc_names):\n",
    "        df = schwartz_word_scores(W_test_norms[:,i,:], tfidf_test[i], word_topic_scores[:,:,i,:], word_topic_sp[:,:,i,:], pre_tfidf_vectorizer, purity_score, word_count, only_doc_words)\n",
    "        dn = re.sub('[\\\\\\:/*?\\[\\]]', '', dn)\n",
    "        df.to_excel(writer, str(i)+'-'+dn[:25])\n",
    "        \n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    \n",
    "def export_doc_tfidf_scores(test_corpusPP, doc_names, pre_trained_doc, filepath = 'tfidf_docs.xlsx', use_model = True):\n",
    "    writer = pd.ExcelWriter(filepath, engine = 'xlsxwriter')\n",
    "    \n",
    "    _, tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    tf_vectorizer= CountVectorizer(min_df=1, ngram_range=(1,3), max_features=50000)\n",
    "    \n",
    "    if use_model:\n",
    "        tfidf = tfidf_vectorizer.transform(test_corpusPP)\n",
    "    else:\n",
    "        tfidf = tfidf_vectorizer.fit_transform(test_corpusPP)\n",
    "    \n",
    "    tf = tf_vectorizer.fit_transform(test_corpusPP)\n",
    "\n",
    "    word_list = []\n",
    "    df_list = []\n",
    "    \n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "        \n",
    "    for i, dn in enumerate(doc_names):\n",
    "        word_list = []\n",
    "        tfidf_doc = tfidf[i].toarray()[0]\n",
    "        tf_doc = tf[i].toarray()[0]\n",
    "        \n",
    "        for idx in list(reversed(tf_doc.argsort())):\n",
    "            if tf_doc[idx] <= 0:\n",
    "                break\n",
    "            if tf_feature_names[idx] not in tfidf_feature_names:\n",
    "                continue\n",
    "            idy = tfidf_feature_names.index(tf_feature_names[idx])\n",
    "            word_list.append((tf_feature_names[idx], np.round(tf_doc[idx], 2), np.round(tfidf_doc[idy], 2), len(tf_feature_names[idx].split())))\n",
    "        \n",
    "        #df = pd.DataFrame(word_list, columns=[\"word (\" + dn + \")\", \"tf (\" + dn + \")\",\n",
    "        #                                                \"tf-idf (\" + dn + \")\", \"ngram (\" + dn + \")\"])\n",
    "        df = pd.DataFrame(word_list, columns=[\"word\", \"tf\", \"tf-idf\", \"ngram\"])\n",
    "        \n",
    "        dn = re.sub('[\\\\\\:/*?\\[\\]]', '', dn)\n",
    "        df.to_excel(writer, str(i)+'-'+dn[:25], index=False)\n",
    "        \n",
    "        #df_list.append(df)\n",
    "        \n",
    "    #score_df = pd.concat(df_list, axis=1)\n",
    "    #score_df.to_excel(filepath)\n",
    "    \n",
    "    writer.save()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinksHTMLaref(page):\n",
    "    \"\"\"\n",
    "\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"a href=\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1: end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "def getLinksHTML(page):\n",
    "    \"\"\"\n",
    "\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"href=\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"htt', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1: end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "def getLinksXML(page):\n",
    "    \"\"\"\n",
    "\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"<link/>\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('http', start_link)\n",
    "    end_quote = page.find('<', start_quote )\n",
    "    url = page[start_quote : end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "\n",
    "def extractFromURL(surl):\n",
    "    response = requests.get(surl)\n",
    "    # parse html\n",
    "    page = str(BeautifulSoup(response.content,\"lxml\"))\n",
    "    is_XML = surl.endswith('xml')\n",
    "    url_list = []\n",
    "    while True:\n",
    "        if is_XML:\n",
    "            url, n = getLinksXML(page)\n",
    "        else:\n",
    "            url, n = getLinksHTML(page)\n",
    "        \n",
    "        page = page[n:]\n",
    "        if url:\n",
    "            if set(url_list).intersection(set(url)) == set() or len(set(url_list).intersection(set(url))) != len(url):\n",
    "                url_list.append(url)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    page = str(BeautifulSoup(response.content,\"lxml\"))\n",
    "    stlink= surl.find(\"//\")\n",
    "    stlink= surl.find(\"/\",stlink+2 )\n",
    "    base = surl[0:stlink]\n",
    "    while True:\n",
    "        if is_XML:\n",
    "            break\n",
    "        else:\n",
    "            url, n = getLinksHTMLaref(page)\n",
    "        page = page[n:]\n",
    "        if url:\n",
    "            url = base+url\n",
    "            if set(url_list).intersection(set(url)) == set() or len(set(url_list).intersection(set(url))) != len(url):\n",
    "                url_list.append(url)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    \n",
    "    PPcorpus = [' '.join(list((extract_terms(doc, extra_process = ['stem'])['Stem']+' ')*extract_terms(doc, \n",
    "                extra_process = ['stem'])['TF'])) if doc != '' else '' for doc in corpus]\n",
    "    return PPcorpus\n",
    "    \n",
    "def evaluate_docs(docs, nmf, tfidf_test, betaloss = 'kullback-leibler'):\n",
    "    X_test = tfidf_test\n",
    "    H_test = nmf.components_\n",
    "    \n",
    "    # Fit the NMF model\n",
    "    t0 = time()\n",
    "\n",
    "    W_test = nmf.transform(X_test)\n",
    "    \n",
    "    return W_test\n",
    "\n",
    "def evaluate_test_corpus(pretrained_filepath, test_corpus, word_replacements):\n",
    "    nmf_list, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    \n",
    "    W_test_list = []\n",
    "    for i, nmf in enumerate(nmf_list):\n",
    "        print(\"Fitting NMF for \" + str(categories[i]))\n",
    "        if word_replacements == []:\n",
    "            tfidf_test = tfidf_vectorizer.transform(test_corpusPP)\n",
    "        else:\n",
    "            docs = [syn.replace_synoyms(test_corpusPP[idx], word_replacements[idx][i]) for idx in range(len(test_corpusPP))]\n",
    "            tfidf_test = tfidf_vectorizer.transform(docs)\n",
    "        \n",
    "        n_features = tfidf_test.shape[1]\n",
    "        W_test = evaluate_docs([], nmf, tfidf_test, betaloss = 'kullback-leibler')\n",
    "        W_test_list.append(W_test)\n",
    "        \n",
    "    # Sum up sub topics\n",
    "    W_test_norms = []\n",
    "    for W_test in W_test_list:\n",
    "        temp_docs = []\n",
    "        for dd in W_test:\n",
    "            temp = []\n",
    "            for w in dd[:-1]:\n",
    "                temp.append(100*w/(w+dd[-1]))\n",
    "            temp_docs.append(temp)\n",
    "        W_test_norms.append(temp_docs)\n",
    "\n",
    "    W_test_norms = np.asarray(W_test_norms)\n",
    "    W_test_norms = np.nan_to_num(W_test_norms)\n",
    "    \n",
    "    W_test_high = W_test_norms.max(axis=2).T\n",
    "    \n",
    "    # cumulated-normalized and raw\n",
    "    return W_test_high, W_test_norms, np.asarray(W_test_list), tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_topics(pretrained_filepath):\n",
    "    nmf_list, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    for i in range(10):\n",
    "        print_top_words(nmf_list, i, tfidf_vectorizer, n_top_words=8, n_topics=3)\n",
    "\n",
    "def add_corpus_txt(filepath, test_corpus):\n",
    "    try:\n",
    "        f = open(filepath, \"r\")\n",
    "        txt = f.read()\n",
    "        test_corpus.append(txt)\n",
    "        f.close()\n",
    "    except:\n",
    "        test_corpus.append(\"\")\n",
    "        print(\"File not found - \" + filepath)\n",
    "\n",
    "\n",
    "def add_corpus_url(url, api_key, test_corpus):\n",
    "    insightIP = 'http://178.62.229.16'\n",
    "    insightPort = '8484'\n",
    "    insightVersion = 'v1.0'\n",
    "\n",
    "    insightSetting = insightIP + ':' + insightPort + '/api/' + insightVersion \n",
    "    request = '/text_analytics/url_scraper?' + 'url=' + url + '&' + 'api_key=' + api_key\n",
    "\n",
    "    # send a request\n",
    "    res = requests.get(insightSetting + request)\n",
    "    if \"Unauthorized Connection\" in res.json():\n",
    "        test_corpus.append(\"\")\n",
    "        print(res.json()[\"Unauthorized Connection\"] + \" - \" + url)\n",
    "    elif \"Error\" in res.json():\n",
    "        test_corpus.append(\"\")\n",
    "        print(res.json()[\"Error\"] + \" - \" + url)\n",
    "    elif \"text\" in res.json():\n",
    "        test_corpus.append(res.json()['text'])\n",
    "        if res.json()['text'] == \"\":\n",
    "            print(\"Empty text - \" + url)\n",
    "    else:\n",
    "        test_corpus.append(\"\")\n",
    "        print(\"Empty text - \" + url)\n",
    "\n",
    "def print_interactive_test_results(W_test_high, W_test_norms, W_test_list, tfidf_test, doc_names, pre_trained_doc, purity_score, word_count, only_doc_words):\n",
    "    pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    word_topic_scores, word_topic_sp = calculate_word_topic_scores(pre_nmf_list, W_test_list)\n",
    "    \n",
    "    \n",
    "    interact(print_test_results,\n",
    "             doc = (0, len(W_test_high)-1, 1),\n",
    "             W_test_high=fixed(W_test_high),\n",
    "             W_test_norms=fixed(W_test_norms),\n",
    "             tfidf_test=fixed(tfidf_test),\n",
    "             pre_nmf_list=fixed(pre_nmf_list),\n",
    "             pre_tfidf_vectorizer=fixed(pre_tfidf_vectorizer),\n",
    "             word_topic_scores=fixed(word_topic_scores),\n",
    "             word_topic_sp=fixed(word_topic_sp),\n",
    "             doc_names=fixed(doc_names),\n",
    "             pre_trained_doc=fixed(pre_trained_doc),\n",
    "             purity_score=fixed(purity_score),\n",
    "             word_count=fixed(word_count),\n",
    "             only_doc_words=fixed(only_doc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonnegative Matrix Factorization (NMF) method was first proposed by Lee and Seung paper1. The NMF is a method of decomposing a given nonnegative *X* matrix into *W* and *H* factors that contain nonnegative values. The value of the product of the two matrices obtained is approximately equal to the value of the decomposed matrix. In NMF, given a $W \\times K$ nonnegative matrix $X = \\left \\{ x_{\\nu, \\tau} \\right \\}$ where $\\nu = 1:V, i = 1:I \\text{ and } \\tau = 1:T$, we seek nonnegative matrices *W* and *H* such that\n",
    "\n",
    "\\begin{align*}\n",
    "x_{\\nu, \\tau} \\approx \\left [ WH \\right ]_{\\nu, \\tau} = \\sum_{i} w_{\\nu,i}h_{i,\\tau}\n",
    "\\end{align*}\n",
    "\n",
    "In this paper, we will refer to the $V\\times I$ matrix W as the *template matrix*, and $I\\times T$ matrix *H* the *excitation matrix*.\n",
    "\n",
    "\n",
    "$X = WH$\n",
    "\n",
    "$X$: documents X vocabulary. tf-idf is used for vocabulary.\n",
    "\n",
    "$W$: documents X topics. Calculate a seperate W for each Schwartz Value using corresponding H.\n",
    "\n",
    "$H$: topics X vocabulary. Calculate a seperate H for each Schwartz Value in the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Schwartz Word Scores\n",
    "\n",
    "* We have a fixed (learned) H matrix for each Schwartz Value that holds word-topic distribution.\n",
    "* We have W matrix for each document's Schwartz Values that holds topic-document distribution.\n",
    "* H matrix gave us an idea about the important words for each Schwartz Value (by providing some kind of weights for each word), but actually the weights of those words can be different for each document.\n",
    "* We propose two different methods to calculate those document spesific weighted word scores.\n",
    " * The summary of the approach is as follows: If a word appears in a document frequently (except stopwords) it can be considered as an important word for this document. If this words only occurs in a specific document then it is even more important. This is basically tf-idf which is our essential feature for this model. Moreover, if this word's tf-idf score obtained more from a specific topic rather than background info then we can accept it as an important indicator of this document and topic.\n",
    " * General equation: $X = WH$. Rather than directly using X or H, we figure in W to the calculation.   \n",
    " * Direct Schwartz: Multiply W and H only through the specific Schwartz Value Topics, excluding backgorund.\n",
    " * Purity Schwartz: Find the Schwartz Value purity of each word by taking the proportions of Direct Schwartz Score of this word to Direct Background Score (exclude Schwartz Value, include Backgroun) for each Schwartz Value. Then multiply this purity score with Direct Schwartz score to obtain Purity Schwartz Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schwartz Value WH carpimi:**\n",
    "\n",
    "Her Schwartz Value icin hangi kelimelerin daha onemli oldugunu anlamak icin H matrisini inceleyebiliriz. Her H matirisi bir Schwartz Value ve backgorund corpus icin birden cok sub-topic seviyesinde kelime dagilimlarini barindirmkata. Yani 3 sub-topic seviyesinde Universalism ornegi dusunursek, modelimiz tek bir cesit universalism degil de 3 farkli universalism cesidi ogrenmeye calisiyor. Bu da bize her universalism ceisidi icin farkli kelime onemleri sunuyor. Fakat universalism'le alakali en onemli kelimeler ne dendigi zaman sub-topic lerden bahsetmek yerine tek bir cati altinda toplamak genel resmi anlamayi cok daha kolaylastirmakta. \n",
    "\n",
    "Fakat burda sadece H matrisi uzerinden bir toplam yaptigggimiz zaman dokumanlarin hangi Universalism sub-topic iyle alakali oldugu bilgisini atmis olmaktayiz. Bu sebeple her dokumanin neden belirli bir Schwartz Value'ya yoneldigini gosteren kelimeleri highlight etmek icin dokumanlarin sub-topic seviyesinde yoneldikleri Schwartz Value degerleri (W) ile kelimelerin sub-topic seviyesinde gruplandigi Schwartz Value (H) degerlerini carpip topluyoruz. Sonuc olarak bir dokuman icin onu siniflandirmamizda en cok etkileyen kelimeleri Schwartz Value lar arasinda da karsilastirma yapabildigimiz bir skorlama vermis oluyor. \n",
    "\n",
    "**Schwartz Value Purity**\n",
    "\n",
    "Yukarida bahsedilen yontem butun Schwartz Value lar ve kelimeler arasinda goreceli bir karsilastirma yontemi saglamakta Fakat kelimeleri modellemekte kullandigimiz tf-idf ten gelen bir kelimenin bir dokumanda cokca gectigi icin onemi (skorunun) daha fazla gozukmekte. Bir yandan bunun etkisini azaltan ve ayni zamanda Schwartz Value purity konseptini uygulayan bir eklenti yapiyoruz. Kelimelerin her dokuman ve her Schwartz Value icin ne kadar saf oldugunu olcuyoruz. Ve bunu da buldugumuz skorla carpiyoruz. Boylece bu kelime sadece istedigimiz Schwartz Value da geciyorsa skoru gorecelei olarak artmis oluyor. Eger bu kelime cogunlukla istedigimiz Schwartz Value da degil de backgorund corpus ta geciorsa goreceli olarak skoru azalmis oluyor. Bu yontem ile aslinda istedigimiz Schwartz Value ile cok ilgili olmasa da sadece belirli dokumanlarda diger dokumanlara gore daha fazla gectigi icin skoru yuksek olan kelimelerin etkisini azaltmis oluyor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schwartz Value Word Scores\n",
    "\n",
    "Understanding the behavior of the model is important to make deductions from it. Our model uses words to match the Schwartz Values with documents. The training of the model forms the $H$ matrix, which holds the word-topic distributions for each Schwartz Value. If we have used a classic, simpler NMF model, then, to find the importance order of the words for each Schwartz Value, we can directly take the marginal of $H$ matrix for each topic. But, our model offers much more information with its sub-topics for each Schwartz Values and semi-supervised nature. \n",
    "\n",
    "#### Direct Word Scores\n",
    "\n",
    "Direct word score exploits the sub-topic structure of the model to come up with different word importance scores and orders for each document. $H$ matrix includes different word-distributions for each sub-topic of both a Schwartz Value and Background Corpus. In other words, if there is three sub-topics for \\textit{Power} Schwartz Value in the $H$ matrix, then our model learns three different concept for Power Schwartz Value which provides different word scores for each concept. However, it is more logical to present a  single set of word scores for a Schwartz Value rather than three different word score sets obtained from sub-topics.\n",
    "\n",
    "We can sum up values under sub-topics of H matrix to come up with a single word distribution with the cost of losing valuable sub-topic information. Thus, rather than finding a unified word-topic distribution for all documents, we calculate separate word scores for each document to highlight the important words that lead a document to be soft-classified as a specific Schwartz Value by dot product of documents' sub-topic level Schwartz Value scores ($W$) and words sub-topic level Schwartz Value scores ($H$). As a result, we obtain scores for all words under each Schwartz Value for each document that can be comparable with each other.\n",
    "\n",
    "\\begin{align*}\n",
    "DWS = \\sum_{i = 1}^{I/2} w_{\\nu,i}h_{i,\\tau}\n",
    "\\end{align*}\n",
    "\n",
    "#### Purity Word Scores\n",
    "\n",
    "\\begin{align*}\n",
    "DWS &= \\sum_{i = 1}^{I/2} w_{\\nu,i}h_{i,\\tau}\\\\\n",
    "BWS &= \\sum_{i = I/2}^{T} w_{\\nu,i}h_{i,\\tau}\\\\\n",
    "Purity &= \\frac{DWS}{DWS+BWS} \\\\\n",
    "PWS &= DWS * Purity\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores are multiplied by 100\n",
    "def calculate_word_topic_scores(pre_nmf_list, W_test_list):\n",
    "    n_topics = pre_nmf_list[0].components_.shape[0]-1\n",
    "    H_list = []\n",
    "\n",
    "    for pnmf in pre_nmf_list:\n",
    "        aa = pnmf.components_\n",
    "        H_list.append(aa/np.sum(aa,axis=1)[:, np.newaxis])\n",
    "        #H_list.append(pnmf.components_)\n",
    "    H_list = np.asarray(H_list)\n",
    "\n",
    "    # [value, doc, word]\n",
    "    word_topic_scores = []\n",
    "    word_background_scores = []\n",
    "\n",
    "    for i in range(10):\n",
    "        temp_ts = []\n",
    "        temp_bs = []\n",
    "        for nt in range(n_topics):\n",
    "            temp_ts.append(np.dot(W_test_list[i][:,nt][:, np.newaxis], H_list[i][nt,:][np.newaxis, :]))\n",
    "            temp_bs.append(np.dot(W_test_list[i][:,n_topics:], H_list[i][n_topics:,:]))\n",
    "        word_topic_scores.append(temp_ts)\n",
    "        word_background_scores.append(temp_bs)\n",
    "\n",
    "    word_topic_scores = np.asarray(word_topic_scores)\n",
    "    word_background_scores = np.asarray(word_background_scores)\n",
    "\n",
    "    word_topic_purity = np.nan_to_num(np.divide(word_topic_scores,word_topic_scores+word_background_scores))\n",
    "    word_topic_sp = word_topic_scores*word_topic_purity\n",
    "\n",
    "    word_topic_scores *= 100000\n",
    "    word_topic_sp *= 100000\n",
    "    \n",
    "    return word_topic_scores, word_topic_sp\n",
    "\n",
    "def find_top_word_scores(pre_tfidf_vectorizer, word_topic, word_count, tfidf_test_doc, only_doc_words):\n",
    "    word_list = []\n",
    "    feature_names = pre_tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    for theme in range(10):\n",
    "        tmp_word_list = []\n",
    "        for nt in range(word_topic.shape[1]):\n",
    "            tmp_list = []\n",
    "            i = 0 \n",
    "            for idx in list(reversed(word_topic[theme][nt].argsort())):\n",
    "                if i == word_count:\n",
    "                    break\n",
    "                if not(only_doc_words and (tfidf_test_doc[0,idx] == 0)):\n",
    "                    tmp_list.append((feature_names[idx], np.round(word_topic[theme][nt][idx], 3)))\n",
    "                else:\n",
    "                    i -= 1\n",
    "                i += 1\n",
    "            tmp_word_list.append(tmp_list)\n",
    "        word_list.append(tmp_word_list)\n",
    "    return word_list\n",
    "\n",
    "def schwartz_word_scores(W_test_doc, tfidf_test_doc, word_topic_scores, word_topic_sp, pre_tfidf_vectorizer, purity_score, word_count, only_doc_words):\n",
    "    if purity_score:\n",
    "        top_scores = find_top_word_scores(pre_tfidf_vectorizer, word_topic_sp, word_count, tfidf_test_doc, only_doc_words)\n",
    "    else:\n",
    "        top_scores = find_top_word_scores(pre_tfidf_vectorizer, word_topic_scores, word_count, tfidf_test_doc, only_doc_words)\n",
    "    \n",
    "    schwartz_word_score = []\n",
    "    schwartz_W_test = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_word_score.append(top_scores[categories.index(sch)])            \n",
    "        schwartz_W_test.append((sch.upper(), np.round(W_test_doc[categories.index(sch)], 2)))\n",
    "        \n",
    "    df_list = []\n",
    "    for i, sws in enumerate(schwartz_word_score):\n",
    "        for j, s in enumerate(sws):\n",
    "            df_list.append(pd.DataFrame([(\"% \" + schwartz_W_test[i][0]+\" (\" + str(j) + \")\", schwartz_W_test[i][1][j])]+s,\n",
    "                                        columns=[schwartz[i]+ \" (\" + str(j)+ \") - word\",\n",
    "                                                 schwartz[i]+ \" (\" + str(j)+ \") - score\"]))\n",
    "        score_df = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Pretrained Model's Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nmf2_pretrained.p** or **nmf2_pretrained_pruned.p** includes pretrained NMF model generated using **Semi-Supervised-NMF-train-v2.ipynb** notebook. It has the nmf model and tfidf_vectorizer.\n",
    "\n",
    "for the details of purned version see also **\"OMTermz HZ.ipynb\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_words(pre_trained_doc, word_count, normalized=False, anti=0):\n",
    "    pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    \n",
    "    n_topics = pre_nmf_list[0].components_.shape[0]-1\n",
    "    word_list = []\n",
    "    feature_names = pre_tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    nmf_comps = []\n",
    "    for pnmf in pre_nmf_list:\n",
    "        aa = pnmf.components_\n",
    "        nmf_comps.append(1000*aa/np.sum(aa,axis=1)[:, np.newaxis])\n",
    "    \n",
    "    for theme in range(10):\n",
    "        #word_topic = cumulate_W(pre_nmf_list[theme].components_.T,n_topics).T[anti]\n",
    "        for nt in range(n_topics):\n",
    "            if normalized:\n",
    "                word_topic = nmf_comps[theme][nt]\n",
    "            else:\n",
    "                word_topic = pre_nmf_list[theme].components_[nt]\n",
    "            tmp_list = []\n",
    "            for i, idx in enumerate(list(reversed(word_topic.argsort()))):\n",
    "                if i == word_count:\n",
    "                    break\n",
    "                tmp_list.append((feature_names[idx], np.round(word_topic[idx], 2)))\n",
    "            word_list.append(tmp_list)\n",
    "    \n",
    "    schwartz_word_score = []\n",
    "    for sch in schwartz:\n",
    "        for nt in range(n_topics):\n",
    "            schwartz_word_score.append(word_list[n_topics*categories.index(sch)+nt])\n",
    "    \n",
    "    df_list = []\n",
    "    for i, a in enumerate(schwartz_word_score):\n",
    "        df_list.append(pd.DataFrame(a, columns=[schwartz[i//n_topics]+ \" (\" + str(i%n_topics)+ \") - word\",\n",
    "                                                schwartz[i//n_topics]+ \" (\" + str(i%n_topics)+ \") - score\"]))\n",
    "    score_df = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    return score_df\n",
    "\n",
    "def export_pretrained_excel(pre_trained_doc, filepath, word_count=-1, anti=0):\n",
    "    df = get_pretrained_words(pre_trained_doc, word_count, anti)\n",
    "    df.to_excel(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0menvironmental movement state marriage social party samesex green\n",
      "\u001b[1mTopic #1: \u001b[0mright environmental law social human peace war state\n",
      "\u001b[1mTopic #2: \u001b[0menergy ecology peace use human think system one\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mpain love orgasm one empathy people may happiness\n",
      "\u001b[1mTopic #1: \u001b[0mone happiness pleasure social desire life also may\n",
      "\u001b[1mTopic #2: \u001b[0mmay one experience also emotion shame person pleasure\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msocial class capital society labour inequality work system\n",
      "\u001b[1mTopic #1: \u001b[0mwork hour individual social goal high management time\n",
      "\u001b[1mTopic #2: \u001b[0mcapital status social human need individual people high\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mpower use experiment milgram make control process machine\n",
      "\u001b[1mTopic #1: \u001b[0mtime state wealth power collapse class may also\n",
      "\u001b[1mTopic #2: \u001b[0mauthority power veto bill social state individual may\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mcreativity play creative intelligence new process theory work\n",
      "\u001b[1mTopic #1: \u001b[0minnovation idea unite intelligence territory state new group\n",
      "\u001b[1mTopic #2: \u001b[0myes independence invention bully positive task individual emotion\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mlaw truth ethic forgiveness theory good one natural\n",
      "\u001b[1mTopic #1: \u001b[0mgood evil one justice pardon lie trust individual\n",
      "\u001b[1mTopic #2: \u001b[0mmiss little little miss one moral god love good\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mgod church commandment politeness use face catechism act\n",
      "\u001b[1mTopic #1: \u001b[0mchild group behavior discipline one social parent positive\n",
      "\u001b[1mTopic #2: \u001b[0mgod one use name commandment shall parent may\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mfolklore christian sin one group folk pride artifact\n",
      "\u001b[1mTopic #1: \u001b[0mvirtue tradition humility one practice ascetic asceticism ethic\n",
      "\u001b[1mTopic #2: \u001b[0mtemperance virtue one moral character myth movement state\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mfiction genre story often music include game fantasy\n",
      "\u001b[1mTopic #1: \u001b[0mtourism travel million tourist international country billion world\n",
      "\u001b[1mTopic #2: \u001b[0msport travel adventure exploration use include game may\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msecurity risk human peace social human security state international\n",
      "\u001b[1mTopic #1: \u001b[0mwaste hygiene pollution use norm may include water\n",
      "\u001b[1mTopic #2: \u001b[0msecurity reciprocity social state national behaviour safety national security\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_trained_doc = \"pretrained_v3_t3_h10_1409.p\"\n",
    "print_training_topics(pre_trained_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If normalized is True, then word-topic matrix is normalized on topics and values are multiplied by 1000 to make them readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>universalism (0) - word</th>\n",
       "      <th>universalism (0) - score</th>\n",
       "      <th>universalism (1) - word</th>\n",
       "      <th>universalism (1) - score</th>\n",
       "      <th>universalism (2) - word</th>\n",
       "      <th>universalism (2) - score</th>\n",
       "      <th>benevolence (0) - word</th>\n",
       "      <th>benevolence (0) - score</th>\n",
       "      <th>benevolence (1) - word</th>\n",
       "      <th>benevolence (1) - score</th>\n",
       "      <th>...</th>\n",
       "      <th>stimulation (1) - word</th>\n",
       "      <th>stimulation (1) - score</th>\n",
       "      <th>stimulation (2) - word</th>\n",
       "      <th>stimulation (2) - score</th>\n",
       "      <th>self-direction (0) - word</th>\n",
       "      <th>self-direction (0) - score</th>\n",
       "      <th>self-direction (1) - word</th>\n",
       "      <th>self-direction (1) - score</th>\n",
       "      <th>self-direction (2) - word</th>\n",
       "      <th>self-direction (2) - score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>environmental</td>\n",
       "      <td>6.07</td>\n",
       "      <td>right</td>\n",
       "      <td>5.45</td>\n",
       "      <td>energy</td>\n",
       "      <td>5.33</td>\n",
       "      <td>law</td>\n",
       "      <td>8.46</td>\n",
       "      <td>good</td>\n",
       "      <td>6.02</td>\n",
       "      <td>...</td>\n",
       "      <td>tourism</td>\n",
       "      <td>35.28</td>\n",
       "      <td>sport</td>\n",
       "      <td>26.21</td>\n",
       "      <td>creativity</td>\n",
       "      <td>19.15</td>\n",
       "      <td>innovation</td>\n",
       "      <td>8.64</td>\n",
       "      <td>yes</td>\n",
       "      <td>11.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movement</td>\n",
       "      <td>4.95</td>\n",
       "      <td>environmental</td>\n",
       "      <td>4.90</td>\n",
       "      <td>ecology</td>\n",
       "      <td>4.84</td>\n",
       "      <td>truth</td>\n",
       "      <td>7.45</td>\n",
       "      <td>evil</td>\n",
       "      <td>5.85</td>\n",
       "      <td>...</td>\n",
       "      <td>travel</td>\n",
       "      <td>13.86</td>\n",
       "      <td>travel</td>\n",
       "      <td>7.28</td>\n",
       "      <td>play</td>\n",
       "      <td>8.73</td>\n",
       "      <td>idea</td>\n",
       "      <td>6.75</td>\n",
       "      <td>independence</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state</td>\n",
       "      <td>4.77</td>\n",
       "      <td>law</td>\n",
       "      <td>4.03</td>\n",
       "      <td>peace</td>\n",
       "      <td>4.70</td>\n",
       "      <td>ethic</td>\n",
       "      <td>6.61</td>\n",
       "      <td>one</td>\n",
       "      <td>5.41</td>\n",
       "      <td>...</td>\n",
       "      <td>million</td>\n",
       "      <td>8.09</td>\n",
       "      <td>adventure</td>\n",
       "      <td>6.45</td>\n",
       "      <td>creative</td>\n",
       "      <td>8.35</td>\n",
       "      <td>unite</td>\n",
       "      <td>6.06</td>\n",
       "      <td>invention</td>\n",
       "      <td>5.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marriage</td>\n",
       "      <td>4.28</td>\n",
       "      <td>social</td>\n",
       "      <td>3.85</td>\n",
       "      <td>use</td>\n",
       "      <td>4.08</td>\n",
       "      <td>forgiveness</td>\n",
       "      <td>6.51</td>\n",
       "      <td>justice</td>\n",
       "      <td>4.89</td>\n",
       "      <td>...</td>\n",
       "      <td>tourist</td>\n",
       "      <td>7.75</td>\n",
       "      <td>exploration</td>\n",
       "      <td>6.39</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>4.58</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>5.43</td>\n",
       "      <td>bully</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>social</td>\n",
       "      <td>4.27</td>\n",
       "      <td>human</td>\n",
       "      <td>3.68</td>\n",
       "      <td>human</td>\n",
       "      <td>3.98</td>\n",
       "      <td>theory</td>\n",
       "      <td>6.18</td>\n",
       "      <td>pardon</td>\n",
       "      <td>4.89</td>\n",
       "      <td>...</td>\n",
       "      <td>international</td>\n",
       "      <td>7.63</td>\n",
       "      <td>use</td>\n",
       "      <td>5.54</td>\n",
       "      <td>new</td>\n",
       "      <td>4.15</td>\n",
       "      <td>territory</td>\n",
       "      <td>5.36</td>\n",
       "      <td>positive</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>party</td>\n",
       "      <td>4.11</td>\n",
       "      <td>peace</td>\n",
       "      <td>3.65</td>\n",
       "      <td>think</td>\n",
       "      <td>3.64</td>\n",
       "      <td>good</td>\n",
       "      <td>5.79</td>\n",
       "      <td>lie</td>\n",
       "      <td>4.74</td>\n",
       "      <td>...</td>\n",
       "      <td>country</td>\n",
       "      <td>7.42</td>\n",
       "      <td>include</td>\n",
       "      <td>5.29</td>\n",
       "      <td>process</td>\n",
       "      <td>4.00</td>\n",
       "      <td>state</td>\n",
       "      <td>4.92</td>\n",
       "      <td>task</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>samesex</td>\n",
       "      <td>4.05</td>\n",
       "      <td>war</td>\n",
       "      <td>3.45</td>\n",
       "      <td>system</td>\n",
       "      <td>3.62</td>\n",
       "      <td>one</td>\n",
       "      <td>5.47</td>\n",
       "      <td>trust</td>\n",
       "      <td>4.43</td>\n",
       "      <td>...</td>\n",
       "      <td>billion</td>\n",
       "      <td>6.32</td>\n",
       "      <td>game</td>\n",
       "      <td>5.09</td>\n",
       "      <td>theory</td>\n",
       "      <td>3.96</td>\n",
       "      <td>new</td>\n",
       "      <td>4.75</td>\n",
       "      <td>individual</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>green</td>\n",
       "      <td>3.74</td>\n",
       "      <td>state</td>\n",
       "      <td>3.44</td>\n",
       "      <td>one</td>\n",
       "      <td>3.42</td>\n",
       "      <td>natural</td>\n",
       "      <td>4.96</td>\n",
       "      <td>individual</td>\n",
       "      <td>4.27</td>\n",
       "      <td>...</td>\n",
       "      <td>world</td>\n",
       "      <td>6.11</td>\n",
       "      <td>may</td>\n",
       "      <td>5.00</td>\n",
       "      <td>work</td>\n",
       "      <td>3.87</td>\n",
       "      <td>group</td>\n",
       "      <td>4.72</td>\n",
       "      <td>emotion</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>woman</td>\n",
       "      <td>3.27</td>\n",
       "      <td>use</td>\n",
       "      <td>3.23</td>\n",
       "      <td>social</td>\n",
       "      <td>3.39</td>\n",
       "      <td>may</td>\n",
       "      <td>3.91</td>\n",
       "      <td>moral</td>\n",
       "      <td>4.07</td>\n",
       "      <td>...</td>\n",
       "      <td>destination</td>\n",
       "      <td>5.29</td>\n",
       "      <td>also</td>\n",
       "      <td>4.86</td>\n",
       "      <td>also</td>\n",
       "      <td>3.58</td>\n",
       "      <td>curiosity</td>\n",
       "      <td>4.35</td>\n",
       "      <td>yes yes</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>right</td>\n",
       "      <td>3.26</td>\n",
       "      <td>specie</td>\n",
       "      <td>3.22</td>\n",
       "      <td>theory</td>\n",
       "      <td>3.20</td>\n",
       "      <td>natural law</td>\n",
       "      <td>3.67</td>\n",
       "      <td>social</td>\n",
       "      <td>3.99</td>\n",
       "      <td>...</td>\n",
       "      <td>unite</td>\n",
       "      <td>5.21</td>\n",
       "      <td>explorer</td>\n",
       "      <td>4.81</td>\n",
       "      <td>state</td>\n",
       "      <td>3.53</td>\n",
       "      <td>music</td>\n",
       "      <td>4.11</td>\n",
       "      <td>performance</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  universalism (0) - word  universalism (0) - score universalism (1) - word  \\\n",
       "0           environmental                      6.07                   right   \n",
       "1                movement                      4.95           environmental   \n",
       "2                   state                      4.77                     law   \n",
       "3                marriage                      4.28                  social   \n",
       "4                  social                      4.27                   human   \n",
       "5                   party                      4.11                   peace   \n",
       "6                 samesex                      4.05                     war   \n",
       "7                   green                      3.74                   state   \n",
       "8                   woman                      3.27                     use   \n",
       "9                   right                      3.26                  specie   \n",
       "\n",
       "   universalism (1) - score universalism (2) - word  universalism (2) - score  \\\n",
       "0                      5.45                  energy                      5.33   \n",
       "1                      4.90                 ecology                      4.84   \n",
       "2                      4.03                   peace                      4.70   \n",
       "3                      3.85                     use                      4.08   \n",
       "4                      3.68                   human                      3.98   \n",
       "5                      3.65                   think                      3.64   \n",
       "6                      3.45                  system                      3.62   \n",
       "7                      3.44                     one                      3.42   \n",
       "8                      3.23                  social                      3.39   \n",
       "9                      3.22                  theory                      3.20   \n",
       "\n",
       "  benevolence (0) - word  benevolence (0) - score benevolence (1) - word  \\\n",
       "0                    law                     8.46                   good   \n",
       "1                  truth                     7.45                   evil   \n",
       "2                  ethic                     6.61                    one   \n",
       "3            forgiveness                     6.51                justice   \n",
       "4                 theory                     6.18                 pardon   \n",
       "5                   good                     5.79                    lie   \n",
       "6                    one                     5.47                  trust   \n",
       "7                natural                     4.96             individual   \n",
       "8                    may                     3.91                  moral   \n",
       "9            natural law                     3.67                 social   \n",
       "\n",
       "   benevolence (1) - score             ...             stimulation (1) - word  \\\n",
       "0                     6.02             ...                            tourism   \n",
       "1                     5.85             ...                             travel   \n",
       "2                     5.41             ...                            million   \n",
       "3                     4.89             ...                            tourist   \n",
       "4                     4.89             ...                      international   \n",
       "5                     4.74             ...                            country   \n",
       "6                     4.43             ...                            billion   \n",
       "7                     4.27             ...                              world   \n",
       "8                     4.07             ...                        destination   \n",
       "9                     3.99             ...                              unite   \n",
       "\n",
       "   stimulation (1) - score stimulation (2) - word  stimulation (2) - score  \\\n",
       "0                    35.28                  sport                    26.21   \n",
       "1                    13.86                 travel                     7.28   \n",
       "2                     8.09              adventure                     6.45   \n",
       "3                     7.75            exploration                     6.39   \n",
       "4                     7.63                    use                     5.54   \n",
       "5                     7.42                include                     5.29   \n",
       "6                     6.32                   game                     5.09   \n",
       "7                     6.11                    may                     5.00   \n",
       "8                     5.29                   also                     4.86   \n",
       "9                     5.21               explorer                     4.81   \n",
       "\n",
       "  self-direction (0) - word  self-direction (0) - score  \\\n",
       "0                creativity                       19.15   \n",
       "1                      play                        8.73   \n",
       "2                  creative                        8.35   \n",
       "3              intelligence                        4.58   \n",
       "4                       new                        4.15   \n",
       "5                   process                        4.00   \n",
       "6                    theory                        3.96   \n",
       "7                      work                        3.87   \n",
       "8                      also                        3.58   \n",
       "9                     state                        3.53   \n",
       "\n",
       "  self-direction (1) - word  self-direction (1) - score  \\\n",
       "0                innovation                        8.64   \n",
       "1                      idea                        6.75   \n",
       "2                     unite                        6.06   \n",
       "3              intelligence                        5.43   \n",
       "4                 territory                        5.36   \n",
       "5                     state                        4.92   \n",
       "6                       new                        4.75   \n",
       "7                     group                        4.72   \n",
       "8                 curiosity                        4.35   \n",
       "9                     music                        4.11   \n",
       "\n",
       "  self-direction (2) - word  self-direction (2) - score  \n",
       "0                       yes                       11.11  \n",
       "1              independence                        7.19  \n",
       "2                 invention                        5.59  \n",
       "3                     bully                        5.02  \n",
       "4                  positive                        4.75  \n",
       "5                      task                        4.70  \n",
       "6                individual                        4.54  \n",
       "7                   emotion                        4.38  \n",
       "8                   yes yes                        4.33  \n",
       "9               performance                        4.22  \n",
       "\n",
       "[10 rows x 60 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pretrained_words(pre_trained_doc, normalized=True, word_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exports all word-score pairs in vocabulary (50000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_pretrained_excel(pre_trained_doc, filepath='pretrained_words.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Different Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two example documents to the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pope ted talk, https://www.ted.com/speakers/pope_francis\n",
    "# US Department of Defense, https://www.defense.gov/About/\n",
    "doc_names = [\"pope.txt\", \"dod.txt\", \"https://www.nationalgeographic.com/science/space/solar-system/earth/\", \"https://sadasd\", \"asdasd\"]\n",
    "#doc_names = [\"pope.txt\", \"dod.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corpus_docs(doc_names, test_corpus, insigth_api_key):\n",
    "    for doc in doc_names:\n",
    "        if re.match(\"^(http|https)://\", doc) is None:\n",
    "            add_corpus_txt(doc, test_corpus)\n",
    "        else:\n",
    "            add_corpus_url(doc, insigth_api_key, test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling a website using InSight API and adding its text to test_corpus.\n",
    "\n",
    "Always check the text, added to the corpus via add_corpus_url. Because websites can have unexpected embedded texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Not Found - https://sadasd\n",
      "File not found - asdasd\n"
     ]
    }
   ],
   "source": [
    "test_corpus = []\n",
    "insigth_api_key = \"\" #needs to be filled\n",
    "add_corpus_docs(doc_names, test_corpus, insigth_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix bad wording:  0.005984067916870117 s\n",
      "Tokenize:  0.006982564926147461 s\n",
      "Remove stopwords and Lemmatize:  2.978036880493164 s\n",
      "\n",
      "Fix bad wording:  0.0019948482513427734 s\n",
      "Tokenize:  0.003989696502685547 s\n",
      "Remove stopwords and Lemmatize:  0.009973764419555664 s\n",
      "\n",
      "Fix bad wording:  0.0019948482513427734 s\n",
      "Tokenize:  0.0029921531677246094 s\n",
      "Remove stopwords and Lemmatize:  0.0069811344146728516 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean Corpus\n",
    "test_corpusPP = [tp.clean_text(doc) for doc in test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count = 150\n",
    "# synonym_count = 10\n",
    "# similarity_span = 10\n",
    "\n",
    "# df_words = get_pretrained_words(pre_trained_doc, word_count)\n",
    "# cols = df_words.columns\n",
    "# for c_id in range(1, len(cols), 2):\n",
    "#     df_words.drop(columns=[cols[c_id]], inplace=True)\n",
    "# synonyms_df = syn.important_train_synonyms(df_words, word_count, synonym_count, similarity_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "# trained_vocabulary = pre_tfidf_vectorizer.get_feature_names()\n",
    "# word_replacements = syn.find_doc_word_synonyms(test_corpusPP, synonyms_df, trained_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model for the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NMF for universalism\n",
      "Fitting NMF for hedonism\n",
      "Fitting NMF for achievement\n",
      "Fitting NMF for power\n",
      "Fitting NMF for self-direction\n",
      "Fitting NMF for benevolence\n",
      "Fitting NMF for conformity\n",
      "Fitting NMF for tradition\n",
      "Fitting NMF for stimulation\n",
      "Fitting NMF for security\n"
     ]
    }
   ],
   "source": [
    "W_test_high, W_test_norms, W_test_list, tfidf_test = evaluate_test_corpus(pre_trained_doc, test_corpusPP, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for test_corpus\n",
    "\n",
    "(Word scores are calculated after the word-topic matrix is normalized on topics)\n",
    "\n",
    "(Word scores are multiplied by 100000 to make values more readeable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When \"only_doc_words\" parameter set to True, the table will only show words from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a871083e8ac944c1b96a367765b3f264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='doc', max=4), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_interactive_test_results(W_test_high, W_test_norms, W_test_list, tfidf_test, doc_names, pre_trained_doc, purity_score = False, word_count = 10, only_doc_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>universalism</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>security</th>\n",
       "      <th>power</th>\n",
       "      <th>achievement</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope.txt</td>\n",
       "      <td>4.145273</td>\n",
       "      <td>70.850015</td>\n",
       "      <td>69.740629</td>\n",
       "      <td>65.097958</td>\n",
       "      <td>12.640265</td>\n",
       "      <td>39.438785</td>\n",
       "      <td>12.978302</td>\n",
       "      <td>51.905642</td>\n",
       "      <td>42.852234</td>\n",
       "      <td>7.511727</td>\n",
       "      <td>Good evening â€“ or, good morning, I am not su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dod.txt</td>\n",
       "      <td>79.515827</td>\n",
       "      <td>0.563155</td>\n",
       "      <td>41.912356</td>\n",
       "      <td>0.170443</td>\n",
       "      <td>89.551388</td>\n",
       "      <td>58.914988</td>\n",
       "      <td>45.434234</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>52.781536</td>\n",
       "      <td>66.028535</td>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nationalgeographic.com/science/spa...</td>\n",
       "      <td>89.320613</td>\n",
       "      <td>0.024123</td>\n",
       "      <td>6.440324</td>\n",
       "      <td>0.259553</td>\n",
       "      <td>59.058197</td>\n",
       "      <td>62.087888</td>\n",
       "      <td>15.350474</td>\n",
       "      <td>9.623754</td>\n",
       "      <td>80.428628</td>\n",
       "      <td>0.037698</td>\n",
       "      <td>Earth, our home planet, is the only planet in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://sadasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asdasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  universalism  \\\n",
       "0                                           pope.txt      4.145273   \n",
       "1                                            dod.txt     79.515827   \n",
       "2  https://www.nationalgeographic.com/science/spa...     89.320613   \n",
       "3                                     https://sadasd      0.000000   \n",
       "4                                             asdasd      0.000000   \n",
       "\n",
       "   benevolence  conformity  tradition   security      power  achievement  \\\n",
       "0    70.850015   69.740629  65.097958  12.640265  39.438785    12.978302   \n",
       "1     0.563155   41.912356   0.170443  89.551388  58.914988    45.434234   \n",
       "2     0.024123    6.440324   0.259553  59.058197  62.087888    15.350474   \n",
       "3     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "4     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "\n",
       "    hedonism  stimulation  self-direction  \\\n",
       "0  51.905642    42.852234        7.511727   \n",
       "1   0.000031    52.781536       66.028535   \n",
       "2   9.623754    80.428628        0.037698   \n",
       "3   0.000000     0.000000        0.000000   \n",
       "4   0.000000     0.000000        0.000000   \n",
       "\n",
       "                                                Text  \n",
       "0  Good evening â€“ or, good morning, I am not su...  \n",
       "1  \\nOn behalf of the Secretary of Defense and De...  \n",
       "2  Earth, our home planet, is the only planet in ...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_excel(W_test_high, test_corpus, doc_names, filepath = 'output.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>universalism</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>security</th>\n",
       "      <th>power</th>\n",
       "      <th>achievement</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope.txt</td>\n",
       "      <td>27.988363</td>\n",
       "      <td>67.763382</td>\n",
       "      <td>61.138390</td>\n",
       "      <td>51.092589</td>\n",
       "      <td>32.598644</td>\n",
       "      <td>26.016194</td>\n",
       "      <td>28.216559</td>\n",
       "      <td>45.193225</td>\n",
       "      <td>52.891968</td>\n",
       "      <td>46.685189</td>\n",
       "      <td>Good evening â€“ or, good morning, I am not su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dod.txt</td>\n",
       "      <td>63.816557</td>\n",
       "      <td>12.494627</td>\n",
       "      <td>16.752545</td>\n",
       "      <td>38.126135</td>\n",
       "      <td>81.431774</td>\n",
       "      <td>59.355575</td>\n",
       "      <td>21.740743</td>\n",
       "      <td>5.620166</td>\n",
       "      <td>30.827866</td>\n",
       "      <td>28.070313</td>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nationalgeographic.com/science/spa...</td>\n",
       "      <td>53.513475</td>\n",
       "      <td>32.123116</td>\n",
       "      <td>20.938325</td>\n",
       "      <td>3.036925</td>\n",
       "      <td>65.892862</td>\n",
       "      <td>55.517597</td>\n",
       "      <td>0.250744</td>\n",
       "      <td>44.063394</td>\n",
       "      <td>78.876438</td>\n",
       "      <td>13.837296</td>\n",
       "      <td>Earth, our home planet, is the only planet in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://sadasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asdasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  universalism  \\\n",
       "0                                           pope.txt     27.988363   \n",
       "1                                            dod.txt     63.816557   \n",
       "2  https://www.nationalgeographic.com/science/spa...     53.513475   \n",
       "3                                     https://sadasd      0.000000   \n",
       "4                                             asdasd      0.000000   \n",
       "\n",
       "   benevolence  conformity  tradition   security      power  achievement  \\\n",
       "0    67.763382   61.138390  51.092589  32.598644  26.016194    28.216559   \n",
       "1    12.494627   16.752545  38.126135  81.431774  59.355575    21.740743   \n",
       "2    32.123116   20.938325   3.036925  65.892862  55.517597     0.250744   \n",
       "3     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "4     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "\n",
       "    hedonism  stimulation  self-direction  \\\n",
       "0  45.193225    52.891968       46.685189   \n",
       "1   5.620166    30.827866       28.070313   \n",
       "2  44.063394    78.876438       13.837296   \n",
       "3   0.000000     0.000000        0.000000   \n",
       "4   0.000000     0.000000        0.000000   \n",
       "\n",
       "                                                Text  \n",
       "0  Good evening â€“ or, good morning, I am not su...  \n",
       "1  \\nOn behalf of the Secretary of Defense and De...  \n",
       "2  Earth, our home planet, is the only planet in ...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_csv(W_test_norm, test_corpus, doc_names, filepath = 'output.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When word_count is -1, it exports all the words\n",
    "# When only_doc_words is set to True, it exports only the words used in the documents\n",
    "\n",
    "# if you want proper document names in the output file change 'doc_names' list.\n",
    "export_word_scores_excel(W_test_norms, W_test_list, tfidf_test, doc_names, pre_trained_doc, filepath = 'ssnmf_words.xlsx', purity_score=False, word_count=-1, only_doc_words=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
