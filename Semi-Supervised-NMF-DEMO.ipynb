{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "from omterms.interface import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "def plot_radar_chart(doc_topic_cumul, doc):\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variablecategories\n",
    "    schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']\n",
    "    \n",
    "    schwartz_dist = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_dist.append(doc_topic_cumul[doc][categories.index(sch)])\n",
    "    \n",
    "    N = len(schwartz)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], schwartz)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "\n",
    "    # Plot each individual = each line of the data\n",
    "    # I don't do a loop, because plotting more than 3 groups makes the chart unreadable\n",
    "\n",
    "    # Ind1\n",
    "    values = list(schwartz_dist) + list(schwartz_dist[:1])\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "\n",
    "    # Add legend\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Schwartz Chart - Doc \" + str(doc))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def print_top_words(model, tfidf_vectorizer, n_top_words, n_topics=3):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx % n_topics == 0:\n",
    "            try:\n",
    "                print(color.CYAN + color.BOLD + categories[topic_idx//3] + color.END)\n",
    "            except:\n",
    "                print(color.CYAN + color.BOLD + \"General\" + color.END)\n",
    "        message = color.BOLD + \"Topic #%d: \" % topic_idx + color.END\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        if (topic_idx+1) % n_topics == 0:\n",
    "            print()\n",
    "    print()\n",
    "    \n",
    "def print_cumulative_train_doc_topics(data, doc_topic, doc, n_best):\n",
    "    test_theme = data.iloc[doc]['theme']\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + color.RED +  \" (\" + test_theme + \")\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "    \n",
    "def print_cumulative_test_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    for i in doc_topic[doc].argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i//3] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, doc_topic[doc][i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_train_results(doc_topic, doc, corpus, data):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_train_doc_topics(data, doc_topic, doc, 11) \n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "def print_test_results(doc_topic, doc, corpus):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    \n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_test_doc_topics(doc_topic, doc, 11)\n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulate_W(W, n_topics):\n",
    "    W_cumul = []\n",
    "    for d in W:\n",
    "        temp = []\n",
    "        for i in range(W.shape[1]//n_topics):\n",
    "            temp.append(d[i*n_topics:(i+1)*n_topics].sum())\n",
    "        W_cumul.append(temp)\n",
    "\n",
    "    W_cumul = np.asarray(W_cumul)\n",
    "    \n",
    "    return W_cumul\n",
    "\n",
    "def normalize_W(W):\n",
    "    W_cumul_norm = W/(W.sum(axis=1).reshape(W.shape[0], 1))\n",
    "    W_cumul_norm *= 100\n",
    "    \n",
    "    return W_cumul_norm\n",
    "\n",
    "def export_to_excel(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put xlsx as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories+['general'])\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_excel(filepath)\n",
    "    return df\n",
    "\n",
    "def export_to_csv(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put csv as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories+['general'])\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    PPcorpus = [' '.join(list((extract_terms(doc, extra_process = ['stem'])['Stem']+' ')*extract_terms(doc, extra_process = ['stem'])['TF'])) for doc in corpus]\n",
    "    return PPcorpus\n",
    "    \n",
    "def evaluate_docs(docs, nmf, tfidf_vectorizer, betaloss = 'kullback-leibler'):\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    t0 = time()\n",
    "    tfidf_test = tfidf_vectorizer.transform(docs)\n",
    "    #tfidf = tfidf_vectorizer.transform(corpusX)\n",
    "    n_features = tfidf_test.shape[1]\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    X_test = tfidf_test\n",
    "    H_test = nmf.components_\n",
    "    \n",
    "    \n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (\" + betaloss + \") with tf-idf features, \")\n",
    "    t0 = time()\n",
    "\n",
    "    W_test = nmf.transform(X_test)\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    return W_test, tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_topics(pretrained_filepath):\n",
    "    nmf, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    print_top_words(nmf, tfidf_vectorizer, n_top_words=5, n_topics=3)\n",
    "\n",
    "def add_corpus_txt(filepath, test_corpus):\n",
    "    f = open(filepath, \"r\") #Pope ted talk, https://www.ted.com/speakers/pope_francis\n",
    "    pope = f.read()\n",
    "    test_corpus.append(pope)\n",
    "    f.close()\n",
    "    \n",
    "def evaluate_test_corpus(pretrained_filepath, test_corpus):\n",
    "    nmf, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    test_corpusPP = preprocess_corpus(test_corpus)\n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    W_test, tfidf_test = evaluate_docs(test_corpusPP, nmf, tfidf_vectorizer, betaloss = 'kullback-leibler')\n",
    "    W_test_cumul = cumulate_W(W_test, n_topics=3)\n",
    "    W_test_norm = normalize_W(W_test_cumul)\n",
    "    \n",
    "    return W_test_norm\n",
    "\n",
    "def print_interactive_test_results(W_test_norm, test_corpus):\n",
    "    interact(print_test_results, doc_topic=fixed(W_test_norm), doc = (0, len(W_test_norm)-1, 1), corpus=fixed(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Pretrained Model's Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nmf_pretrained.p** includes pretrained NMF model generated using **Semi-Supervised-NMF-train.ipynb** notebook. It has the nmf model and tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mdisarma organ spoke explor outsid\n",
      "\u001b[1mTopic #1: \u001b[0mspecif matter enhanc publish tend\n",
      "\u001b[1mTopic #2: \u001b[0methic index crew particip new\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #3: \u001b[0mtime philosophi sever thing piti\n",
      "\u001b[1mTopic #4: \u001b[0mpleasur self outrag rise refer\n",
      "\u001b[1mTopic #5: \u001b[0muse see jealousi shown self\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #6: \u001b[0mgreater use return offer recent\n",
      "\u001b[1mTopic #7: \u001b[0minterest mean place machin sang\n",
      "\u001b[1mTopic #8: \u001b[0msatisfact set use properti stalin\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #9: \u001b[0marticl highli compos technic idea\n",
      "\u001b[1mTopic #10: \u001b[0mwangchuck option toxic method power\n",
      "\u001b[1mTopic #11: \u001b[0mpart moham troubl valu use\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #12: \u001b[0mbenedek project ilinx variou great\n",
      "\u001b[1mTopic #13: \u001b[0msecess sever non photographi character\n",
      "\u001b[1mTopic #14: \u001b[0mliberti gener burkina train carrol\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #15: \u001b[0mseek major map shown surcharg\n",
      "\u001b[1mTopic #16: \u001b[0mlook siqe life taken polici\n",
      "\u001b[1mTopic #17: \u001b[0midea automat user answer discrimin\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #18: \u001b[0msanford collectivist suffer proverb secondari\n",
      "\u001b[1mTopic #19: \u001b[0mgreat two undesir degre spoken\n",
      "\u001b[1mTopic #20: \u001b[0msimilar peopl moral wear ms\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #21: \u001b[0mantiquitatem squar sophrosyn sourc judgment\n",
      "\u001b[1mTopic #22: \u001b[0mthing similarli inlin spain read\n",
      "\u001b[1mTopic #23: \u001b[0mmoral sens gener swing moderna\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #24: \u001b[0mseasid ruc lumpur sight english\n",
      "\u001b[1mTopic #25: \u001b[0mseller indonesia panama shown follow\n",
      "\u001b[1mTopic #26: \u001b[0mpublic fun prior irish skin\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #27: \u001b[0mjustic thousand special egg technologist\n",
      "\u001b[1mTopic #28: \u001b[0mimprov sever firefight potenti discov\n",
      "\u001b[1mTopic #29: \u001b[0moften variat peer type let\n",
      "\n",
      "\u001b[96m\u001b[1mGeneral\u001b[0m\n",
      "\u001b[1mTopic #30: \u001b[0mgroup relat organ law therefor\n",
      "\u001b[1mTopic #31: \u001b[0muse suggest tradit individu two\n",
      "\u001b[1mTopic #32: \u001b[0mhelp templat concern scienc explain\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_training_topics(\"nmf_pretrained.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Different Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two example documents to the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = []\n",
    "add_corpus_txt(\"pope.txt\", test_corpus) #Pope ted talk, https://www.ted.com/speakers/pope_francis\n",
    "add_corpus_txt(\"dod.txt\", test_corpus) # US Department of Defense, https://www.defense.gov/About/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model for the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.00s.\n",
      "Fitting the NMF model (kullback-leibler) with tf-idf features, \n",
      "done in 0.30s.\n"
     ]
    }
   ],
   "source": [
    "W_test_norm = evaluate_test_corpus(\"nmf_pretrained.p\", test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cd19ed5a214f8ebe21d7b02bf9e624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc', max=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_interactive_test_results(W_test_norm, test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>universalism</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>power</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "      <th>general</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good evening â€“ or, good morning, I am not su...</td>\n",
       "      <td>4.826219</td>\n",
       "      <td>8.687874</td>\n",
       "      <td>3.294627</td>\n",
       "      <td>5.759052</td>\n",
       "      <td>1.643831</td>\n",
       "      <td>17.700747</td>\n",
       "      <td>31.610477</td>\n",
       "      <td>8.554336</td>\n",
       "      <td>3.615432</td>\n",
       "      <td>2.883951</td>\n",
       "      <td>11.423454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "      <td>22.309187</td>\n",
       "      <td>0.978004</td>\n",
       "      <td>1.372212</td>\n",
       "      <td>23.710624</td>\n",
       "      <td>13.864289</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>16.909238</td>\n",
       "      <td>15.306420</td>\n",
       "      <td>5.544305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  universalism  hedonism  \\\n",
       "0  Good evening â€“ or, good morning, I am not su...      4.826219  8.687874   \n",
       "1  \\nOn behalf of the Secretary of Defense and De...     22.309187  0.978004   \n",
       "\n",
       "   achievement      power  self-direction  benevolence  conformity  tradition  \\\n",
       "0     3.294627   5.759052        1.643831    17.700747   31.610477   8.554336   \n",
       "1     1.372212  23.710624       13.864289     0.000009    0.000202   0.005510   \n",
       "\n",
       "   stimulation   security    general  \n",
       "0     3.615432   2.883951  11.423454  \n",
       "1    16.909238  15.306420   5.544305  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_excel(W_test_norm, test_corpus, filepath = 'output.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>universalism</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>power</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "      <th>general</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good evening â€“ or, good morning, I am not su...</td>\n",
       "      <td>4.826219</td>\n",
       "      <td>8.687874</td>\n",
       "      <td>3.294627</td>\n",
       "      <td>5.759052</td>\n",
       "      <td>1.643831</td>\n",
       "      <td>17.700747</td>\n",
       "      <td>31.610477</td>\n",
       "      <td>8.554336</td>\n",
       "      <td>3.615432</td>\n",
       "      <td>2.883951</td>\n",
       "      <td>11.423454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "      <td>22.309187</td>\n",
       "      <td>0.978004</td>\n",
       "      <td>1.372212</td>\n",
       "      <td>23.710624</td>\n",
       "      <td>13.864289</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>16.909238</td>\n",
       "      <td>15.306420</td>\n",
       "      <td>5.544305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  universalism  hedonism  \\\n",
       "0  Good evening â€“ or, good morning, I am not su...      4.826219  8.687874   \n",
       "1  \\nOn behalf of the Secretary of Defense and De...     22.309187  0.978004   \n",
       "\n",
       "   achievement      power  self-direction  benevolence  conformity  tradition  \\\n",
       "0     3.294627   5.759052        1.643831    17.700747   31.610477   8.554336   \n",
       "1     1.372212  23.710624       13.864289     0.000009    0.000202   0.005510   \n",
       "\n",
       "   stimulation   security    general  \n",
       "0     3.615432   2.883951  11.423454  \n",
       "1    16.909238  15.306420   5.544305  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_csv(W_test_norm, test_corpus, filepath = 'output.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
