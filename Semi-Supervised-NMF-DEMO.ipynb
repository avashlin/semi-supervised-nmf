{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "from omterms.interface import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "def plot_radar_chart(doc_topic_cumul, doc):\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variablecategories\n",
    "    schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']\n",
    "    \n",
    "    schwartz_dist = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_dist.append(doc_topic_cumul[doc][categories.index(sch)])\n",
    "    \n",
    "    N = len(schwartz)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], schwartz)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "\n",
    "    # Plot each individual = each line of the data\n",
    "    # I don't do a loop, because plotting more than 3 groups makes the chart unreadable\n",
    "\n",
    "    # Ind1\n",
    "    values = list(schwartz_dist) + list(schwartz_dist[:1])\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "\n",
    "    # Add legend\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Schwartz Chart - Doc \" + str(doc))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def print_top_words(model, tfidf_vectorizer, n_top_words, n_topics=3):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx % n_topics == 0:\n",
    "            try:\n",
    "                print(color.CYAN + color.BOLD + categories[topic_idx//3] + color.END)\n",
    "            except:\n",
    "                print(color.CYAN + color.BOLD + \"General\" + color.END)\n",
    "        message = color.BOLD + \"Topic #%d: \" % topic_idx + color.END\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        if (topic_idx+1) % n_topics == 0:\n",
    "            print()\n",
    "    print()\n",
    "    \n",
    "def print_cumulative_train_doc_topics(data, doc_topic, doc, n_best):\n",
    "    test_theme = data.iloc[doc]['theme']\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + color.RED +  \" (\" + test_theme + \")\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "    \n",
    "def print_cumulative_test_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    for i in doc_topic[doc].argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i//3] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, doc_topic[doc][i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_train_results(doc_topic, doc, corpus, data):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_train_doc_topics(data, doc_topic, doc, 11) \n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "def print_test_results(doc_topic, doc, corpus):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    \n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_test_doc_topics(doc_topic, doc, 11)\n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulate_W(W, n_topics):\n",
    "    W_cumul = []\n",
    "    for d in W:\n",
    "        temp = []\n",
    "        for i in range(W.shape[1]//n_topics):\n",
    "            temp.append(d[i*n_topics:(i+1)*n_topics].sum())\n",
    "        W_cumul.append(temp)\n",
    "\n",
    "    W_cumul = np.asarray(W_cumul)\n",
    "    \n",
    "    return W_cumul\n",
    "\n",
    "def normalize_W(W):\n",
    "    W_cumul_norm = W/(W.sum(axis=1).reshape(W.shape[0], 1))\n",
    "    W_cumul_norm *= 100\n",
    "    \n",
    "    return W_cumul_norm\n",
    "\n",
    "def export_to_excel(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put xlsx as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories+['general'])\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_excel(filepath)\n",
    "    return df\n",
    "\n",
    "def export_to_csv(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put csv as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories+['general'])\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    PPcorpus = [' '.join(list((extract_terms(doc, extra_process = ['stem'])['Stem']+' ')*extract_terms(doc, extra_process = ['stem'])['TF'])) for doc in corpus]\n",
    "    return PPcorpus\n",
    "    \n",
    "def evaluate_docs(docs, nmf, tfidf_vectorizer, betaloss = 'kullback-leibler'):\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    t0 = time()\n",
    "    tfidf_test = tfidf_vectorizer.transform(docs)\n",
    "    #tfidf = tfidf_vectorizer.transform(corpusX)\n",
    "    n_features = tfidf_test.shape[1]\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    X_test = tfidf_test\n",
    "    H_test = nmf.components_\n",
    "    \n",
    "    \n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (\" + betaloss + \") with tf-idf features, \")\n",
    "    t0 = time()\n",
    "\n",
    "    W_test = nmf.transform(X_test)\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    return W_test, tfidf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_topics(pretrained_filepath):\n",
    "    nmf, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    print_top_words(nmf, tfidf_vectorizer, n_top_words=5, n_topics=3)\n",
    "\n",
    "def add_corpus_txt(filepath, test_corpus):\n",
    "    f = open(filepath, \"r\")\n",
    "    txt = f.read()\n",
    "    test_corpus.append(txt)\n",
    "    f.close()\n",
    "    \n",
    "def add_corpus_url(url, api_key, test_corpus):\n",
    "    insightIP = 'http://178.62.229.16'\n",
    "    insightPort = '8484'\n",
    "    insightVersion = 'v1.0'\n",
    "\n",
    "    insightSetting = insightIP + ':' + insightPort + '/api/' + insightVersion \n",
    "    request = '/text_analytics/url_scraper?' + 'url=' + url + '&' + 'api_key=' + api_key\n",
    "\n",
    "    # send a request\n",
    "    res = requests.get(insightSetting + request)\n",
    "    test_corpus.append(res.json()['text'])\n",
    "    \n",
    "def evaluate_test_corpus(pretrained_filepath, test_corpus):\n",
    "    nmf, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    test_corpusPP = preprocess_corpus(test_corpus)\n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    W_test, tfidf_test = evaluate_docs(test_corpusPP, nmf, tfidf_vectorizer, betaloss = 'kullback-leibler')\n",
    "    W_test_cumul = cumulate_W(W_test, n_topics=3)\n",
    "    W_test_norm = normalize_W(W_test_cumul)\n",
    "    \n",
    "    return W_test_norm\n",
    "\n",
    "def print_interactive_test_results(W_test_norm, test_corpus):\n",
    "    interact(print_test_results, doc_topic=fixed(W_test_norm), doc = (0, len(W_test_norm)-1, 1), corpus=fixed(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Pretrained Model's Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nmf_pretrained.p** or **nmf_pretrained_pruned.p** includes pretrained NMF model generated using **Semi-Supervised-NMF-train.ipynb** notebook. It has the nmf model and tfidf_vectorizer.\n",
    "\n",
    "for the details of purned version see also **\"OMTermz HZ.ipynb\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mindividu topic analyt found help\n",
      "\u001b[1mTopic #1: \u001b[0mpeopl buell particl feder intern\n",
      "\u001b[1mTopic #2: \u001b[0mdisarma renew idol delight habitat\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #3: \u001b[0mtime reaction simpli import ohatsu\n",
      "\u001b[1mTopic #4: \u001b[0mstudi shock see psycholog research\n",
      "\u001b[1mTopic #5: \u001b[0mrepres problem success induc scopophobia\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #6: \u001b[0mtheori peopl relat lower top\n",
      "\u001b[1mTopic #7: \u001b[0mtribe humanist motiv introduc practic\n",
      "\u001b[1mTopic #8: \u001b[0msocial role merchant term other\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #9: \u001b[0marticl lower may specialti peopl\n",
      "\u001b[1mTopic #10: \u001b[0mpartner idea bia belong use\n",
      "\u001b[1mTopic #11: \u001b[0mleadership tool toxic environ guid\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #12: \u001b[0muse made gener liberti ratifi\n",
      "\u001b[1mTopic #13: \u001b[0mknown romantic resourc domin resent\n",
      "\u001b[1mTopic #14: \u001b[0mbenedek take olivero carrol interperson\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #15: \u001b[0msometim seem natur one turkey\n",
      "\u001b[1mTopic #16: \u001b[0mthought automat upon realiti answer\n",
      "\u001b[1mTopic #17: \u001b[0mtheori need renew shuv exampl\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #18: \u001b[0mshow thu uncodifi collectivist paint\n",
      "\u001b[1mTopic #19: \u001b[0mother great rewritten problem use\n",
      "\u001b[1mTopic #20: \u001b[0mtime mose voltag undesir day\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #21: \u001b[0mparticularli preciou success gener passion\n",
      "\u001b[1mTopic #22: \u001b[0mantiquitatem law televis mahavatar potenti\n",
      "\u001b[1mTopic #23: \u001b[0mthing sinc sourc help three\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #24: \u001b[0mrisk fun suffer psycholog remov\n",
      "\u001b[1mTopic #25: \u001b[0mland miyazaki declin feud previous\n",
      "\u001b[1mTopic #26: \u001b[0mreason indic exo fare mainli\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #27: \u001b[0mland pollut thousand rest spread\n",
      "\u001b[1mTopic #28: \u001b[0mpesticid someth poorer two relat\n",
      "\u001b[1mTopic #29: \u001b[0mthu signific evid play fals\n",
      "\n",
      "\u001b[96m\u001b[1mGeneral\u001b[0m\n",
      "\u001b[1mTopic #30: \u001b[0mknown interest unsourc state unit\n",
      "\u001b[1mTopic #31: \u001b[0mgroup theori typic school form\n",
      "\u001b[1mTopic #32: \u001b[0mcreation english member portal kingdom\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_trained_doc = \"nmf_pretrained_pruned.p\"\n",
    "print_training_topics(pre_trained_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Different Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two example documents to the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = []\n",
    "add_corpus_txt(\"pope.txt\", test_corpus) #Pope ted talk, https://www.ted.com/speakers/pope_francis\n",
    "add_corpus_txt(\"dod.txt\", test_corpus) # US Department of Defense, https://www.defense.gov/About/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling a website using InSight API and adding its text to test_corpus.\n",
    "\n",
    "Always check the text, added to the corpus via add_corpus_url. Because websites can have unexpected embedded texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "insigth_api_key = \"\" #needs to be filled\n",
    "url = \"https://www.nationalgeographic.com/science/space/solar-system/earth/\"\n",
    "add_corpus_url(url, insigth_api_key, test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model for the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 554\n",
      "Cleaning process: Initial size of tokens = 554\n",
      "Reduction due to punctuations and stopwords = 316.\n",
      "Reduction due to all numeral terms = 36\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 352\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 554\n",
      "Cleaning process: Initial size of tokens = 554\n",
      "Reduction due to punctuations and stopwords = 316.\n",
      "Reduction due to all numeral terms = 36\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 352\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 0.00s.\n",
      "Fitting the NMF model (kullback-leibler) with tf-idf features, \n",
      "done in 0.29s.\n"
     ]
    }
   ],
   "source": [
    "W_test_norm = evaluate_test_corpus(pre_trained_doc, test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d7b95973854fda8076559914a4a3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='doc', max=2), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_interactive_test_results(W_test_norm, test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>universalism</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>power</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "      <th>general</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good evening â€“ or, good morning, I am not su...</td>\n",
       "      <td>4.024334</td>\n",
       "      <td>8.704034</td>\n",
       "      <td>6.412505</td>\n",
       "      <td>9.568149</td>\n",
       "      <td>2.022214</td>\n",
       "      <td>15.938469</td>\n",
       "      <td>31.645763</td>\n",
       "      <td>5.662532</td>\n",
       "      <td>5.513720</td>\n",
       "      <td>5.292175</td>\n",
       "      <td>5.216106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "      <td>40.971534</td>\n",
       "      <td>1.119135</td>\n",
       "      <td>3.567248</td>\n",
       "      <td>5.178333</td>\n",
       "      <td>6.909340</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>3.523228</td>\n",
       "      <td>0.025827</td>\n",
       "      <td>0.498702</td>\n",
       "      <td>28.338515</td>\n",
       "      <td>9.868110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Earth, our home planet, is the only planet in ...</td>\n",
       "      <td>21.073571</td>\n",
       "      <td>1.817401</td>\n",
       "      <td>0.093295</td>\n",
       "      <td>15.418328</td>\n",
       "      <td>5.277123</td>\n",
       "      <td>1.962361</td>\n",
       "      <td>2.451394</td>\n",
       "      <td>0.019308</td>\n",
       "      <td>48.444014</td>\n",
       "      <td>2.293843</td>\n",
       "      <td>1.149362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  universalism  hedonism  \\\n",
       "0  Good evening â€“ or, good morning, I am not su...      4.024334  8.704034   \n",
       "1  \\nOn behalf of the Secretary of Defense and De...     40.971534  1.119135   \n",
       "2  Earth, our home planet, is the only planet in ...     21.073571  1.817401   \n",
       "\n",
       "   achievement      power  self-direction  benevolence  conformity  tradition  \\\n",
       "0     6.412505   9.568149        2.022214    15.938469   31.645763   5.662532   \n",
       "1     3.567248   5.178333        6.909340     0.000029    3.523228   0.025827   \n",
       "2     0.093295  15.418328        5.277123     1.962361    2.451394   0.019308   \n",
       "\n",
       "   stimulation   security   general  \n",
       "0     5.513720   5.292175  5.216106  \n",
       "1     0.498702  28.338515  9.868110  \n",
       "2    48.444014   2.293843  1.149362  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_excel(W_test_norm, test_corpus, filepath = 'output.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>universalism</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>power</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "      <th>general</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good evening â€“ or, good morning, I am not su...</td>\n",
       "      <td>4.024334</td>\n",
       "      <td>8.704034</td>\n",
       "      <td>6.412505</td>\n",
       "      <td>9.568149</td>\n",
       "      <td>2.022214</td>\n",
       "      <td>15.938469</td>\n",
       "      <td>31.645763</td>\n",
       "      <td>5.662532</td>\n",
       "      <td>5.513720</td>\n",
       "      <td>5.292175</td>\n",
       "      <td>5.216106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "      <td>40.971534</td>\n",
       "      <td>1.119135</td>\n",
       "      <td>3.567248</td>\n",
       "      <td>5.178333</td>\n",
       "      <td>6.909340</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>3.523228</td>\n",
       "      <td>0.025827</td>\n",
       "      <td>0.498702</td>\n",
       "      <td>28.338515</td>\n",
       "      <td>9.868110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Earth, our home planet, is the only planet in ...</td>\n",
       "      <td>21.073571</td>\n",
       "      <td>1.817401</td>\n",
       "      <td>0.093295</td>\n",
       "      <td>15.418328</td>\n",
       "      <td>5.277123</td>\n",
       "      <td>1.962361</td>\n",
       "      <td>2.451394</td>\n",
       "      <td>0.019308</td>\n",
       "      <td>48.444014</td>\n",
       "      <td>2.293843</td>\n",
       "      <td>1.149362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  universalism  hedonism  \\\n",
       "0  Good evening â€“ or, good morning, I am not su...      4.024334  8.704034   \n",
       "1  \\nOn behalf of the Secretary of Defense and De...     40.971534  1.119135   \n",
       "2  Earth, our home planet, is the only planet in ...     21.073571  1.817401   \n",
       "\n",
       "   achievement      power  self-direction  benevolence  conformity  tradition  \\\n",
       "0     6.412505   9.568149        2.022214    15.938469   31.645763   5.662532   \n",
       "1     3.567248   5.178333        6.909340     0.000029    3.523228   0.025827   \n",
       "2     0.093295  15.418328        5.277123     1.962361    2.451394   0.019308   \n",
       "\n",
       "   stimulation   security   general  \n",
       "0     5.513720   5.292175  5.216106  \n",
       "1     0.498702  28.338515  9.868110  \n",
       "2    48.444014   2.293843  1.149362  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_csv(W_test_norm, test_corpus, filepath = 'output.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
