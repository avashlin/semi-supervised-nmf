{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "from omterms.interface import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "def plot_radar_chart(doc_topic_cumul, doc):\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variablecategories\n",
    "    schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']\n",
    "    \n",
    "    schwartz_dist = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_dist.append(doc_topic_cumul[doc][categories.index(sch)])\n",
    "    \n",
    "    N = len(schwartz)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], schwartz)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "\n",
    "    # Plot each individual = each line of the data\n",
    "    # I don't do a loop, because plotting more than 3 groups makes the chart unreadable\n",
    "\n",
    "    # Ind1\n",
    "    values = list(schwartz_dist) + list(schwartz_dist[:1])\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "\n",
    "    # Add legend\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Schwartz Chart - Doc \" + str(doc))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def print_top_words(model, tfidf_vectorizer, n_top_words, n_topics=3):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx % n_topics == 0:\n",
    "            try:\n",
    "                print(color.CYAN + color.BOLD + categories[topic_idx//3] + color.END)\n",
    "            except:\n",
    "                print(color.CYAN + color.BOLD + \"General\" + color.END)\n",
    "        message = color.BOLD + \"Topic #%d: \" % topic_idx + color.END\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        if (topic_idx+1) % n_topics == 0:\n",
    "            print()\n",
    "    print()\n",
    "    \n",
    "def print_cumulative_train_doc_topics(data, doc_topic, doc, n_best):\n",
    "    test_theme = data.iloc[doc]['theme']\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + color.RED +  \" (\" + test_theme + \")\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "    \n",
    "def print_cumulative_test_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    for i in doc_topic[doc].argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i//3] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, doc_topic[doc][i]), end='')    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_W(N, n_topics, n_themes, theme_counts):\n",
    "    rands = np.random.random( N * n_topics * (n_themes+1))\n",
    "    W = np.zeros((N, n_topics * n_themes))\n",
    "\n",
    "    cum_doc_count = 0\n",
    "    idx = 0\n",
    "    for theme, doc_count in theme_counts.items():\n",
    "        #print(\"Theme: \" + str(theme) + \" Doc_count: \" + str(doc_count))\n",
    "        start = cum_doc_count\n",
    "        end = start + doc_count\n",
    "        W[start:end, idx*n_topics:(idx+1)*n_topics] = rands[:(end-start)*n_topics].reshape((end-start, n_topics))\n",
    "        listrands = list(rands)\n",
    "        del listrands[:(end-start)*n_topics]\n",
    "        rands = np.array(listrands)\n",
    "        \n",
    "        cum_doc_count += doc_count\n",
    "        idx +=1\n",
    "\n",
    "    last_column = rands[- N * n_topics:].reshape((N, n_topics))\n",
    "    \n",
    "    return np.column_stack((W, last_column))\n",
    "\n",
    "def cumulate_W(W, n_topics):\n",
    "    W_cumul = []\n",
    "    for d in W:\n",
    "        temp = []\n",
    "        for i in range(W.shape[1]//n_topics):\n",
    "            temp.append(d[i*n_topics:(i+1)*n_topics].sum())\n",
    "        W_cumul.append(temp)\n",
    "\n",
    "    W_cumul = np.asarray(W_cumul)\n",
    "    \n",
    "    return W_cumul\n",
    "\n",
    "def normalize_W(W):\n",
    "    W_cumul_norm = W/(W.sum(axis=1).reshape(W.shape[0], 1))\n",
    "    W_cumul_norm *= 100\n",
    "    \n",
    "    return W_cumul_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data = pd.read_json(filepath)\n",
    "    data = data[data['text']!=\"\"]\n",
    "    data = data.sort_values('theme.id')\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def extract_corpus(data):    \n",
    "    corpus = list(data['text'])\n",
    "    return corpus\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    PPcorpus = [' '.join(list((extract_terms(doc, extra_process = ['stem'])['Stem']+' ')*extract_terms(doc, extra_process = ['stem'])['TF'])) for doc in corpus]\n",
    "    return PPcorpus\n",
    "\n",
    "def train_corpus(corpus, data, n_topics=3, betaloss = 'kullback-leibler'):\n",
    "    N = len(data)\n",
    "    \n",
    "    theme_counts = data.groupby(['theme.id','theme']).count().iloc[:,1]\n",
    "    pd_theme_counts = pd.DataFrame(theme_counts)\n",
    "    n_themes = len(theme_counts)\n",
    "    \n",
    "    n_top_words = 5\n",
    "    n_components = n_topics*(n_themes)\n",
    "    \n",
    "    \n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer() # optionally add maxfeatures = n_features to enforce number of features\n",
    "    t0 = time()\n",
    "    tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "    n_features = tfidf.shape[1]\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    X = tfidf \n",
    "    W = build_W(N, n_topics, n_themes, theme_counts)\n",
    "    H = np.random.rand(n_components+n_topics, n_features)\n",
    "    \n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (\" + betaloss + \") with tf-idf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (N, n_features))\n",
    "    t0 = time()\n",
    "\n",
    "    nmf = NMF(n_components= n_components+n_topics, solver='mu', beta_loss=betaloss,\n",
    "              alpha=.1, l1_ratio=.5, init = 'custom')\n",
    "\n",
    "    nmf.fit_transform(X=X,W=W,H=H)\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    return nmf, W, tfidf, tfidf_vectorizer\n",
    "    \n",
    "def evaluate_docs(docs, nmf, tfidf_vectorizer, betaloss = 'kullback-leibler'):\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    t0 = time()\n",
    "    tfidf_test = tfidf_vectorizer.transform(docs)\n",
    "    #tfidf = tfidf_vectorizer.transform(corpusX)\n",
    "    n_features = tfidf_test.shape[1]\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    X_test = tfidf_test\n",
    "    H_test = nmf.components_\n",
    "    \n",
    "    \n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (\" + betaloss + \") with tf-idf features, \")\n",
    "    t0 = time()\n",
    "\n",
    "    W_test = nmf.transform(X_test)\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    return W_test, tfidf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3909\n",
      "Cleaning process: Initial size of tokens = 3909\n",
      "Reduction due to punctuations and stopwords = 2804.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2824\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3909\n",
      "Cleaning process: Initial size of tokens = 3909\n",
      "Reduction due to punctuations and stopwords = 2804.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2824\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10721\n",
      "Cleaning process: Initial size of tokens = 10721\n",
      "Reduction due to punctuations and stopwords = 8558.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8585\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10721\n",
      "Cleaning process: Initial size of tokens = 10721\n",
      "Reduction due to punctuations and stopwords = 8558.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8585\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2248\n",
      "Cleaning process: Initial size of tokens = 2248\n",
      "Reduction due to punctuations and stopwords = 1567.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1574\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2248\n",
      "Cleaning process: Initial size of tokens = 2248\n",
      "Reduction due to punctuations and stopwords = 1567.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1574\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1722\n",
      "Cleaning process: Initial size of tokens = 1722\n",
      "Reduction due to punctuations and stopwords = 1179.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1187\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1722\n",
      "Cleaning process: Initial size of tokens = 1722\n",
      "Reduction due to punctuations and stopwords = 1179.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1187\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 242\n",
      "Cleaning process: Initial size of tokens = 242\n",
      "Reduction due to punctuations and stopwords = 149.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 151\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 242\n",
      "Cleaning process: Initial size of tokens = 242\n",
      "Reduction due to punctuations and stopwords = 149.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 151\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 276\n",
      "Cleaning process: Initial size of tokens = 276\n",
      "Reduction due to punctuations and stopwords = 152.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 155\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 276\n",
      "Cleaning process: Initial size of tokens = 276\n",
      "Reduction due to punctuations and stopwords = 152.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 155\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6955\n",
      "Cleaning process: Initial size of tokens = 6955\n",
      "Reduction due to punctuations and stopwords = 5074.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5094\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6955\n",
      "Cleaning process: Initial size of tokens = 6955\n",
      "Reduction due to punctuations and stopwords = 5074.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5094\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1562\n",
      "Cleaning process: Initial size of tokens = 1562\n",
      "Reduction due to punctuations and stopwords = 1070.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1074\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1562\n",
      "Cleaning process: Initial size of tokens = 1562\n",
      "Reduction due to punctuations and stopwords = 1070.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1074\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2696\n",
      "Cleaning process: Initial size of tokens = 2696\n",
      "Reduction due to punctuations and stopwords = 2067.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2075\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2696\n",
      "Cleaning process: Initial size of tokens = 2696\n",
      "Reduction due to punctuations and stopwords = 2067.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2075\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1914\n",
      "Cleaning process: Initial size of tokens = 1914\n",
      "Reduction due to punctuations and stopwords = 1355.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1365\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1914\n",
      "Cleaning process: Initial size of tokens = 1914\n",
      "Reduction due to punctuations and stopwords = 1355.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1365\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 479\n",
      "Cleaning process: Initial size of tokens = 479\n",
      "Reduction due to punctuations and stopwords = 285.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 286\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 479\n",
      "Cleaning process: Initial size of tokens = 479\n",
      "Reduction due to punctuations and stopwords = 285.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 286\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4859\n",
      "Cleaning process: Initial size of tokens = 4859\n",
      "Reduction due to punctuations and stopwords = 3487.\n",
      "Reduction due to all numeral terms = 19\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3513\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4859\n",
      "Cleaning process: Initial size of tokens = 4859\n",
      "Reduction due to punctuations and stopwords = 3487.\n",
      "Reduction due to all numeral terms = 19\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3513\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7319\n",
      "Cleaning process: Initial size of tokens = 7319\n",
      "Reduction due to punctuations and stopwords = 5362.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5386\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7319\n",
      "Cleaning process: Initial size of tokens = 7319\n",
      "Reduction due to punctuations and stopwords = 5362.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5386\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 757\n",
      "Cleaning process: Initial size of tokens = 757\n",
      "Reduction due to punctuations and stopwords = 518.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 520\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 757\n",
      "Cleaning process: Initial size of tokens = 757\n",
      "Reduction due to punctuations and stopwords = 518.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 520\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2857\n",
      "Cleaning process: Initial size of tokens = 2857\n",
      "Reduction due to punctuations and stopwords = 1955.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1974\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2857\n",
      "Cleaning process: Initial size of tokens = 2857\n",
      "Reduction due to punctuations and stopwords = 1955.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1974\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 14427\n",
      "Cleaning process: Initial size of tokens = 14427\n",
      "Reduction due to punctuations and stopwords = 11343.\n",
      "Reduction due to all numeral terms = 41\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 11407\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 14427\n",
      "Cleaning process: Initial size of tokens = 14427\n",
      "Reduction due to punctuations and stopwords = 11343.\n",
      "Reduction due to all numeral terms = 41\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 11407\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8801\n",
      "Cleaning process: Initial size of tokens = 8801\n",
      "Reduction due to punctuations and stopwords = 6514.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6536\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8801\n",
      "Cleaning process: Initial size of tokens = 8801\n",
      "Reduction due to punctuations and stopwords = 6514.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6536\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1516\n",
      "Cleaning process: Initial size of tokens = 1516\n",
      "Reduction due to punctuations and stopwords = 983.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1003\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1516\n",
      "Cleaning process: Initial size of tokens = 1516\n",
      "Reduction due to punctuations and stopwords = 983.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1003\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 363\n",
      "Cleaning process: Initial size of tokens = 363\n",
      "Reduction due to punctuations and stopwords = 227.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 227\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 363\n",
      "Cleaning process: Initial size of tokens = 363\n",
      "Reduction due to punctuations and stopwords = 227.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 227\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1049\n",
      "Cleaning process: Initial size of tokens = 1049\n",
      "Reduction due to punctuations and stopwords = 657.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 657\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1049\n",
      "Cleaning process: Initial size of tokens = 1049\n",
      "Reduction due to punctuations and stopwords = 657.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 657\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2330\n",
      "Cleaning process: Initial size of tokens = 2330\n",
      "Reduction due to punctuations and stopwords = 1856.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1863\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2330\n",
      "Cleaning process: Initial size of tokens = 2330\n",
      "Reduction due to punctuations and stopwords = 1856.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1863\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 965\n",
      "Cleaning process: Initial size of tokens = 965\n",
      "Reduction due to punctuations and stopwords = 613.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 616\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 965\n",
      "Cleaning process: Initial size of tokens = 965\n",
      "Reduction due to punctuations and stopwords = 613.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 616\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 9641\n",
      "Cleaning process: Initial size of tokens = 9641\n",
      "Reduction due to punctuations and stopwords = 7338.\n",
      "Reduction due to all numeral terms = 26\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 14\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7387\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 9641\n",
      "Cleaning process: Initial size of tokens = 9641\n",
      "Reduction due to punctuations and stopwords = 7338.\n",
      "Reduction due to all numeral terms = 26\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 14\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7387\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5416\n",
      "Cleaning process: Initial size of tokens = 5416\n",
      "Reduction due to punctuations and stopwords = 3717.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3734\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5416\n",
      "Cleaning process: Initial size of tokens = 5416\n",
      "Reduction due to punctuations and stopwords = 3717.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3734\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6013\n",
      "Cleaning process: Initial size of tokens = 6013\n",
      "Reduction due to punctuations and stopwords = 4494.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4521\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6013\n",
      "Cleaning process: Initial size of tokens = 6013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to punctuations and stopwords = 4494.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4521\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3199\n",
      "Cleaning process: Initial size of tokens = 3199\n",
      "Reduction due to punctuations and stopwords = 2253.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2259\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3199\n",
      "Cleaning process: Initial size of tokens = 3199\n",
      "Reduction due to punctuations and stopwords = 2253.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2259\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5734\n",
      "Cleaning process: Initial size of tokens = 5734\n",
      "Reduction due to punctuations and stopwords = 4205.\n",
      "Reduction due to all numeral terms = 51\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4269\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5734\n",
      "Cleaning process: Initial size of tokens = 5734\n",
      "Reduction due to punctuations and stopwords = 4205.\n",
      "Reduction due to all numeral terms = 51\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4269\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5944\n",
      "Cleaning process: Initial size of tokens = 5944\n",
      "Reduction due to punctuations and stopwords = 4206.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 12\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4248\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5944\n",
      "Cleaning process: Initial size of tokens = 5944\n",
      "Reduction due to punctuations and stopwords = 4206.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 12\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4248\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11064\n",
      "Cleaning process: Initial size of tokens = 11064\n",
      "Reduction due to punctuations and stopwords = 8687.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8709\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11064\n",
      "Cleaning process: Initial size of tokens = 11064\n",
      "Reduction due to punctuations and stopwords = 8687.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8709\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6998\n",
      "Cleaning process: Initial size of tokens = 6998\n",
      "Reduction due to punctuations and stopwords = 5259.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5282\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6998\n",
      "Cleaning process: Initial size of tokens = 6998\n",
      "Reduction due to punctuations and stopwords = 5259.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5282\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11032\n",
      "Cleaning process: Initial size of tokens = 11032\n",
      "Reduction due to punctuations and stopwords = 8651.\n",
      "Reduction due to all numeral terms = 19\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8689\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11032\n",
      "Cleaning process: Initial size of tokens = 11032\n",
      "Reduction due to punctuations and stopwords = 8651.\n",
      "Reduction due to all numeral terms = 19\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8689\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3563\n",
      "Cleaning process: Initial size of tokens = 3563\n",
      "Reduction due to punctuations and stopwords = 2503.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2515\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3563\n",
      "Cleaning process: Initial size of tokens = 3563\n",
      "Reduction due to punctuations and stopwords = 2503.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2515\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7196\n",
      "Cleaning process: Initial size of tokens = 7196\n",
      "Reduction due to punctuations and stopwords = 5422.\n",
      "Reduction due to all numeral terms = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5445\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7196\n",
      "Cleaning process: Initial size of tokens = 7196\n",
      "Reduction due to punctuations and stopwords = 5422.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5445\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 329\n",
      "Cleaning process: Initial size of tokens = 329\n",
      "Reduction due to punctuations and stopwords = 199.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 199\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 329\n",
      "Cleaning process: Initial size of tokens = 329\n",
      "Reduction due to punctuations and stopwords = 199.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 199\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 868\n",
      "Cleaning process: Initial size of tokens = 868\n",
      "Reduction due to punctuations and stopwords = 546.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 547\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 868\n",
      "Cleaning process: Initial size of tokens = 868\n",
      "Reduction due to punctuations and stopwords = 546.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 547\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3851\n",
      "Cleaning process: Initial size of tokens = 3851\n",
      "Reduction due to punctuations and stopwords = 2813.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2817\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3851\n",
      "Cleaning process: Initial size of tokens = 3851\n",
      "Reduction due to punctuations and stopwords = 2813.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2817\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5976\n",
      "Cleaning process: Initial size of tokens = 5976\n",
      "Reduction due to punctuations and stopwords = 4493.\n",
      "Reduction due to all numeral terms = 29\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4535\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5976\n",
      "Cleaning process: Initial size of tokens = 5976\n",
      "Reduction due to punctuations and stopwords = 4493.\n",
      "Reduction due to all numeral terms = 29\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4535\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 668\n",
      "Cleaning process: Initial size of tokens = 668\n",
      "Reduction due to punctuations and stopwords = 429.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 430\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 668\n",
      "Cleaning process: Initial size of tokens = 668\n",
      "Reduction due to punctuations and stopwords = 429.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 430\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1635\n",
      "Cleaning process: Initial size of tokens = 1635\n",
      "Reduction due to punctuations and stopwords = 938.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 941\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1635\n",
      "Cleaning process: Initial size of tokens = 1635\n",
      "Reduction due to punctuations and stopwords = 938.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 941\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 17289\n",
      "Cleaning process: Initial size of tokens = 17289\n",
      "Reduction due to punctuations and stopwords = 14582.\n",
      "Reduction due to all numeral terms = 85\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 14682\n",
      "Percentage = 85%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 17289\n",
      "Cleaning process: Initial size of tokens = 17289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to punctuations and stopwords = 14582.\n",
      "Reduction due to all numeral terms = 85\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 14682\n",
      "Percentage = 85%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6698\n",
      "Cleaning process: Initial size of tokens = 6698\n",
      "Reduction due to punctuations and stopwords = 4796.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4817\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6698\n",
      "Cleaning process: Initial size of tokens = 6698\n",
      "Reduction due to punctuations and stopwords = 4796.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4817\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6499\n",
      "Cleaning process: Initial size of tokens = 6499\n",
      "Reduction due to punctuations and stopwords = 4723.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4734\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6499\n",
      "Cleaning process: Initial size of tokens = 6499\n",
      "Reduction due to punctuations and stopwords = 4723.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4734\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10519\n",
      "Cleaning process: Initial size of tokens = 10519\n",
      "Reduction due to punctuations and stopwords = 8014.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 14\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8054\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10519\n",
      "Cleaning process: Initial size of tokens = 10519\n",
      "Reduction due to punctuations and stopwords = 8014.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 14\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8054\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8313\n",
      "Cleaning process: Initial size of tokens = 8313\n",
      "Reduction due to punctuations and stopwords = 6455.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6471\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8313\n",
      "Cleaning process: Initial size of tokens = 8313\n",
      "Reduction due to punctuations and stopwords = 6455.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6471\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1188\n",
      "Cleaning process: Initial size of tokens = 1188\n",
      "Reduction due to punctuations and stopwords = 762.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 768\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1188\n",
      "Cleaning process: Initial size of tokens = 1188\n",
      "Reduction due to punctuations and stopwords = 762.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 768\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2106\n",
      "Cleaning process: Initial size of tokens = 2106\n",
      "Reduction due to punctuations and stopwords = 1484.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1485\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2106\n",
      "Cleaning process: Initial size of tokens = 2106\n",
      "Reduction due to punctuations and stopwords = 1484.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1485\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11229\n",
      "Cleaning process: Initial size of tokens = 11229\n",
      "Reduction due to punctuations and stopwords = 8581.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 17\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8607\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11229\n",
      "Cleaning process: Initial size of tokens = 11229\n",
      "Reduction due to punctuations and stopwords = 8581.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 17\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8607\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1010\n",
      "Cleaning process: Initial size of tokens = 1010\n",
      "Reduction due to punctuations and stopwords = 655.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 664\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1010\n",
      "Cleaning process: Initial size of tokens = 1010\n",
      "Reduction due to punctuations and stopwords = 655.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 664\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 933\n",
      "Cleaning process: Initial size of tokens = 933\n",
      "Reduction due to punctuations and stopwords = 575.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 582\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Number of terms: 933\n",
      "Cleaning process: Initial size of tokens = 933\n",
      "Reduction due to punctuations and stopwords = 575.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 582\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 17070\n",
      "Cleaning process: Initial size of tokens = 17070\n",
      "Reduction due to punctuations and stopwords = 13629.\n",
      "Reduction due to all numeral terms = 92\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 15\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 13751\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 17070\n",
      "Cleaning process: Initial size of tokens = 17070\n",
      "Reduction due to punctuations and stopwords = 13629.\n",
      "Reduction due to all numeral terms = 92\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 15\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 13751\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 215\n",
      "Cleaning process: Initial size of tokens = 215\n",
      "Reduction due to punctuations and stopwords = 115.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 115\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 215\n",
      "Cleaning process: Initial size of tokens = 215\n",
      "Reduction due to punctuations and stopwords = 115.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 115\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3190\n",
      "Cleaning process: Initial size of tokens = 3190\n",
      "Reduction due to punctuations and stopwords = 2102.\n",
      "Reduction due to all numeral terms = 38\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2149\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3190\n",
      "Cleaning process: Initial size of tokens = 3190\n",
      "Reduction due to punctuations and stopwords = 2102.\n",
      "Reduction due to all numeral terms = 38\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2149\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1842\n",
      "Cleaning process: Initial size of tokens = 1842\n",
      "Reduction due to punctuations and stopwords = 1248.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1257\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1842\n",
      "Cleaning process: Initial size of tokens = 1842\n",
      "Reduction due to punctuations and stopwords = 1248.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1257\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4289\n",
      "Cleaning process: Initial size of tokens = 4289\n",
      "Reduction due to punctuations and stopwords = 3005.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3016\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4289\n",
      "Cleaning process: Initial size of tokens = 4289\n",
      "Reduction due to punctuations and stopwords = 3005.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3016\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1207\n",
      "Cleaning process: Initial size of tokens = 1207\n",
      "Reduction due to punctuations and stopwords = 780.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 781\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1207\n",
      "Cleaning process: Initial size of tokens = 1207\n",
      "Reduction due to punctuations and stopwords = 780.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 781\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6385\n",
      "Cleaning process: Initial size of tokens = 6385\n",
      "Reduction due to punctuations and stopwords = 4820.\n",
      "Reduction due to all numeral terms = 24\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4862\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6385\n",
      "Cleaning process: Initial size of tokens = 6385\n",
      "Reduction due to punctuations and stopwords = 4820.\n",
      "Reduction due to all numeral terms = 24\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4862\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 724\n",
      "Cleaning process: Initial size of tokens = 724\n",
      "Reduction due to punctuations and stopwords = 500.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 503\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 724\n",
      "Cleaning process: Initial size of tokens = 724\n",
      "Reduction due to punctuations and stopwords = 500.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 503\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 252\n",
      "Cleaning process: Initial size of tokens = 252\n",
      "Reduction due to punctuations and stopwords = 141.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 147\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 252\n",
      "Cleaning process: Initial size of tokens = 252\n",
      "Reduction due to punctuations and stopwords = 141.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 147\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1331\n",
      "Cleaning process: Initial size of tokens = 1331\n",
      "Reduction due to punctuations and stopwords = 825.\n",
      "Reduction due to all numeral terms = 20\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 852\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1331\n",
      "Cleaning process: Initial size of tokens = 1331\n",
      "Reduction due to punctuations and stopwords = 825.\n",
      "Reduction due to all numeral terms = 20\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 852\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3630\n",
      "Cleaning process: Initial size of tokens = 3630\n",
      "Reduction due to punctuations and stopwords = 2693.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2710\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3630\n",
      "Cleaning process: Initial size of tokens = 3630\n",
      "Reduction due to punctuations and stopwords = 2693.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2710\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 32877\n",
      "Cleaning process: Initial size of tokens = 32877\n",
      "Reduction due to punctuations and stopwords = 27380.\n",
      "Reduction due to all numeral terms = 164\n",
      "Reduction due to short terms = 17\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 27\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 27588\n",
      "Percentage = 84%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 32877\n",
      "Cleaning process: Initial size of tokens = 32877\n",
      "Reduction due to punctuations and stopwords = 27380.\n",
      "Reduction due to all numeral terms = 164\n",
      "Reduction due to short terms = 17\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 27\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 27588\n",
      "Percentage = 84%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 13100\n",
      "Cleaning process: Initial size of tokens = 13100\n",
      "Reduction due to punctuations and stopwords = 9659.\n",
      "Reduction due to all numeral terms = 25\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 17\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9714\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 13100\n",
      "Cleaning process: Initial size of tokens = 13100\n",
      "Reduction due to punctuations and stopwords = 9659.\n",
      "Reduction due to all numeral terms = 25\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 17\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9714\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8872\n",
      "Cleaning process: Initial size of tokens = 8872\n",
      "Reduction due to punctuations and stopwords = 6542.\n",
      "Reduction due to all numeral terms = 76\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 24\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6654\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8872\n",
      "Cleaning process: Initial size of tokens = 8872\n",
      "Reduction due to punctuations and stopwords = 6542.\n",
      "Reduction due to all numeral terms = 76\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 24\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6654\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1128\n",
      "Cleaning process: Initial size of tokens = 1128\n",
      "Reduction due to punctuations and stopwords = 872.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 874\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1128\n",
      "Cleaning process: Initial size of tokens = 1128\n",
      "Reduction due to punctuations and stopwords = 872.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 874\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3183\n",
      "Cleaning process: Initial size of tokens = 3183\n",
      "Reduction due to punctuations and stopwords = 2435.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2438\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3183\n",
      "Cleaning process: Initial size of tokens = 3183\n",
      "Reduction due to punctuations and stopwords = 2435.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2438\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 442\n",
      "Cleaning process: Initial size of tokens = 442\n",
      "Reduction due to punctuations and stopwords = 267.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 267\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 442\n",
      "Cleaning process: Initial size of tokens = 442\n",
      "Reduction due to punctuations and stopwords = 267.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 267\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4354\n",
      "Cleaning process: Initial size of tokens = 4354\n",
      "Reduction due to punctuations and stopwords = 3337.\n",
      "Reduction due to all numeral terms = 11\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3357\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4354\n",
      "Cleaning process: Initial size of tokens = 4354\n",
      "Reduction due to punctuations and stopwords = 3337.\n",
      "Reduction due to all numeral terms = 11\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3357\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1616\n",
      "Cleaning process: Initial size of tokens = 1616\n",
      "Reduction due to punctuations and stopwords = 1026.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1028\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1616\n",
      "Cleaning process: Initial size of tokens = 1616\n",
      "Reduction due to punctuations and stopwords = 1026.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1028\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7324\n",
      "Cleaning process: Initial size of tokens = 7324\n",
      "Reduction due to punctuations and stopwords = 5413.\n",
      "Reduction due to all numeral terms = 23\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5457\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7324\n",
      "Cleaning process: Initial size of tokens = 7324\n",
      "Reduction due to punctuations and stopwords = 5413.\n",
      "Reduction due to all numeral terms = 23\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5457\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8275\n",
      "Cleaning process: Initial size of tokens = 8275\n",
      "Reduction due to punctuations and stopwords = 6421.\n",
      "Reduction due to all numeral terms = 20\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6465\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8275\n",
      "Cleaning process: Initial size of tokens = 8275\n",
      "Reduction due to punctuations and stopwords = 6421.\n",
      "Reduction due to all numeral terms = 20\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6465\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10531\n",
      "Cleaning process: Initial size of tokens = 10531\n",
      "Reduction due to punctuations and stopwords = 8732.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8740\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10531\n",
      "Cleaning process: Initial size of tokens = 10531\n",
      "Reduction due to punctuations and stopwords = 8732.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8740\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 545\n",
      "Cleaning process: Initial size of tokens = 545\n",
      "Reduction due to punctuations and stopwords = 311.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 314\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 545\n",
      "Cleaning process: Initial size of tokens = 545\n",
      "Reduction due to punctuations and stopwords = 311.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 314\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 695\n",
      "Cleaning process: Initial size of tokens = 695\n",
      "Reduction due to punctuations and stopwords = 394.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 397\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 695\n",
      "Cleaning process: Initial size of tokens = 695\n",
      "Reduction due to punctuations and stopwords = 394.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 397\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3351\n",
      "Cleaning process: Initial size of tokens = 3351\n",
      "Reduction due to punctuations and stopwords = 2365.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2375\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3351\n",
      "Cleaning process: Initial size of tokens = 3351\n",
      "Reduction due to punctuations and stopwords = 2365.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2375\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1755\n",
      "Cleaning process: Initial size of tokens = 1755\n",
      "Reduction due to punctuations and stopwords = 1157.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1178\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1755\n",
      "Cleaning process: Initial size of tokens = 1755\n",
      "Reduction due to punctuations and stopwords = 1157.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1178\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Number of terms: 10533\n",
      "Cleaning process: Initial size of tokens = 10533\n",
      "Reduction due to punctuations and stopwords = 8734.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8742\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10533\n",
      "Cleaning process: Initial size of tokens = 10533\n",
      "Reduction due to punctuations and stopwords = 8734.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8742\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2497\n",
      "Cleaning process: Initial size of tokens = 2497\n",
      "Reduction due to punctuations and stopwords = 1695.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1715\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2497\n",
      "Cleaning process: Initial size of tokens = 2497\n",
      "Reduction due to punctuations and stopwords = 1695.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1715\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3924\n",
      "Cleaning process: Initial size of tokens = 3924\n",
      "Reduction due to punctuations and stopwords = 2857.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2869\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3924\n",
      "Cleaning process: Initial size of tokens = 3924\n",
      "Reduction due to punctuations and stopwords = 2857.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2869\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 274\n",
      "Cleaning process: Initial size of tokens = 274\n",
      "Reduction due to punctuations and stopwords = 188.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 188\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 274\n",
      "Cleaning process: Initial size of tokens = 274\n",
      "Reduction due to punctuations and stopwords = 188.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 188\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1493\n",
      "Cleaning process: Initial size of tokens = 1493\n",
      "Reduction due to punctuations and stopwords = 957.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 966\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1493\n",
      "Cleaning process: Initial size of tokens = 1493\n",
      "Reduction due to punctuations and stopwords = 957.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 966\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11078\n",
      "Cleaning process: Initial size of tokens = 11078\n",
      "Reduction due to punctuations and stopwords = 8891.\n",
      "Reduction due to all numeral terms = 17\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 20\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8936\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11078\n",
      "Cleaning process: Initial size of tokens = 11078\n",
      "Reduction due to punctuations and stopwords = 8891.\n",
      "Reduction due to all numeral terms = 17\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 20\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8936\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8176\n",
      "Cleaning process: Initial size of tokens = 8176\n",
      "Reduction due to punctuations and stopwords = 5950.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5969\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8176\n",
      "Cleaning process: Initial size of tokens = 8176\n",
      "Reduction due to punctuations and stopwords = 5950.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5969\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 12651\n",
      "Cleaning process: Initial size of tokens = 12651\n",
      "Reduction due to punctuations and stopwords = 9939.\n",
      "Reduction due to all numeral terms = 12\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9973\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 12651\n",
      "Cleaning process: Initial size of tokens = 12651\n",
      "Reduction due to punctuations and stopwords = 9939.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to all numeral terms = 12\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9973\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 315\n",
      "Cleaning process: Initial size of tokens = 315\n",
      "Reduction due to punctuations and stopwords = 190.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 192\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 315\n",
      "Cleaning process: Initial size of tokens = 315\n",
      "Reduction due to punctuations and stopwords = 190.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 192\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3490\n",
      "Cleaning process: Initial size of tokens = 3490\n",
      "Reduction due to punctuations and stopwords = 2565.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2570\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3490\n",
      "Cleaning process: Initial size of tokens = 3490\n",
      "Reduction due to punctuations and stopwords = 2565.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2570\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 142\n",
      "Cleaning process: Initial size of tokens = 142\n",
      "Reduction due to punctuations and stopwords = 75.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 75\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 142\n",
      "Cleaning process: Initial size of tokens = 142\n",
      "Reduction due to punctuations and stopwords = 75.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 75\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4021\n",
      "Cleaning process: Initial size of tokens = 4021\n",
      "Reduction due to punctuations and stopwords = 2932.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2961\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4021\n",
      "Cleaning process: Initial size of tokens = 4021\n",
      "Reduction due to punctuations and stopwords = 2932.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2961\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3878\n",
      "Cleaning process: Initial size of tokens = 3878\n",
      "Reduction due to punctuations and stopwords = 2796.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2817\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3878\n",
      "Cleaning process: Initial size of tokens = 3878\n",
      "Reduction due to punctuations and stopwords = 2796.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2817\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 825\n",
      "Cleaning process: Initial size of tokens = 825\n",
      "Reduction due to punctuations and stopwords = 503.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 18\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 525\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 825\n",
      "Cleaning process: Initial size of tokens = 825\n",
      "Reduction due to punctuations and stopwords = 503.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 18\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 525\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 291\n",
      "Cleaning process: Initial size of tokens = 291\n",
      "Reduction due to punctuations and stopwords = 163.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 164\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 291\n",
      "Cleaning process: Initial size of tokens = 291\n",
      "Reduction due to punctuations and stopwords = 163.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 164\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10019\n",
      "Cleaning process: Initial size of tokens = 10019\n",
      "Reduction due to punctuations and stopwords = 7584.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7608\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10019\n",
      "Cleaning process: Initial size of tokens = 10019\n",
      "Reduction due to punctuations and stopwords = 7584.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7608\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 727\n",
      "Cleaning process: Initial size of tokens = 727\n",
      "Reduction due to punctuations and stopwords = 469.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 472\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 727\n",
      "Cleaning process: Initial size of tokens = 727\n",
      "Reduction due to punctuations and stopwords = 469.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 472\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1650\n",
      "Cleaning process: Initial size of tokens = 1650\n",
      "Reduction due to punctuations and stopwords = 1274.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1317\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1650\n",
      "Cleaning process: Initial size of tokens = 1650\n",
      "Reduction due to punctuations and stopwords = 1274.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1317\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1652\n",
      "Cleaning process: Initial size of tokens = 1652\n",
      "Reduction due to punctuations and stopwords = 1090.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1096\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1652\n",
      "Cleaning process: Initial size of tokens = 1652\n",
      "Reduction due to punctuations and stopwords = 1090.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1096\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2567\n",
      "Cleaning process: Initial size of tokens = 2567\n",
      "Reduction due to punctuations and stopwords = 1685.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1700\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2567\n",
      "Cleaning process: Initial size of tokens = 2567\n",
      "Reduction due to punctuations and stopwords = 1685.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1700\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 796\n",
      "Cleaning process: Initial size of tokens = 796\n",
      "Reduction due to punctuations and stopwords = 480.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 484\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 796\n",
      "Cleaning process: Initial size of tokens = 796\n",
      "Reduction due to punctuations and stopwords = 480.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 484\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1847\n",
      "Cleaning process: Initial size of tokens = 1847\n",
      "Reduction due to punctuations and stopwords = 1236.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1243\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1847\n",
      "Cleaning process: Initial size of tokens = 1847\n",
      "Reduction due to punctuations and stopwords = 1236.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1243\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2773\n",
      "Cleaning process: Initial size of tokens = 2773\n",
      "Reduction due to punctuations and stopwords = 1918.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1956\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2773\n",
      "Cleaning process: Initial size of tokens = 2773\n",
      "Reduction due to punctuations and stopwords = 1918.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1956\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8750\n",
      "Cleaning process: Initial size of tokens = 8750\n",
      "Reduction due to punctuations and stopwords = 7112.\n",
      "Reduction due to all numeral terms = 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7166\n",
      "Percentage = 82%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8750\n",
      "Cleaning process: Initial size of tokens = 8750\n",
      "Reduction due to punctuations and stopwords = 7112.\n",
      "Reduction due to all numeral terms = 31\n",
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7166\n",
      "Percentage = 82%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 209\n",
      "Cleaning process: Initial size of tokens = 209\n",
      "Reduction due to punctuations and stopwords = 121.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 122\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 209\n",
      "Cleaning process: Initial size of tokens = 209\n",
      "Reduction due to punctuations and stopwords = 121.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 122\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11095\n",
      "Cleaning process: Initial size of tokens = 11095\n",
      "Reduction due to punctuations and stopwords = 8784.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8801\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11095\n",
      "Cleaning process: Initial size of tokens = 11095\n",
      "Reduction due to punctuations and stopwords = 8784.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8801\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1848\n",
      "Cleaning process: Initial size of tokens = 1848\n",
      "Reduction due to punctuations and stopwords = 1214.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1223\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1848\n",
      "Cleaning process: Initial size of tokens = 1848\n",
      "Reduction due to punctuations and stopwords = 1214.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1223\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5034\n",
      "Cleaning process: Initial size of tokens = 5034\n",
      "Reduction due to punctuations and stopwords = 3740.\n",
      "Reduction due to all numeral terms = 44\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3796\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5034\n",
      "Cleaning process: Initial size of tokens = 5034\n",
      "Reduction due to punctuations and stopwords = 3740.\n",
      "Reduction due to all numeral terms = 44\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3796\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3626\n",
      "Cleaning process: Initial size of tokens = 3626\n",
      "Reduction due to punctuations and stopwords = 2475.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2489\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3626\n",
      "Cleaning process: Initial size of tokens = 3626\n",
      "Reduction due to punctuations and stopwords = 2475.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2489\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4118\n",
      "Cleaning process: Initial size of tokens = 4118\n",
      "Reduction due to punctuations and stopwords = 2831.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2852\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4118\n",
      "Cleaning process: Initial size of tokens = 4118\n",
      "Reduction due to punctuations and stopwords = 2831.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2852\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1635\n",
      "Cleaning process: Initial size of tokens = 1635\n",
      "Reduction due to punctuations and stopwords = 1156.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1162\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1635\n",
      "Cleaning process: Initial size of tokens = 1635\n",
      "Reduction due to punctuations and stopwords = 1156.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1162\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 706\n",
      "Cleaning process: Initial size of tokens = 706\n",
      "Reduction due to punctuations and stopwords = 469.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 472\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 706\n",
      "Cleaning process: Initial size of tokens = 706\n",
      "Reduction due to punctuations and stopwords = 469.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 472\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5948\n",
      "Cleaning process: Initial size of tokens = 5948\n",
      "Reduction due to punctuations and stopwords = 4209.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 12\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4251\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5948\n",
      "Cleaning process: Initial size of tokens = 5948\n",
      "Reduction due to punctuations and stopwords = 4209.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 12\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4251\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6709\n",
      "Cleaning process: Initial size of tokens = 6709\n",
      "Reduction due to punctuations and stopwords = 5036.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5053\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6709\n",
      "Cleaning process: Initial size of tokens = 6709\n",
      "Reduction due to punctuations and stopwords = 5036.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5053\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7069\n",
      "Cleaning process: Initial size of tokens = 7069\n",
      "Reduction due to punctuations and stopwords = 5251.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5269\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7069\n",
      "Cleaning process: Initial size of tokens = 7069\n",
      "Reduction due to punctuations and stopwords = 5251.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5269\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 556\n",
      "Cleaning process: Initial size of tokens = 556\n",
      "Reduction due to punctuations and stopwords = 344.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 345\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 556\n",
      "Cleaning process: Initial size of tokens = 556\n",
      "Reduction due to punctuations and stopwords = 344.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 345\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 738\n",
      "Cleaning process: Initial size of tokens = 738\n",
      "Reduction due to punctuations and stopwords = 491.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 493\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 738\n",
      "Cleaning process: Initial size of tokens = 738\n",
      "Reduction due to punctuations and stopwords = 491.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 493\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1961\n",
      "Cleaning process: Initial size of tokens = 1961\n",
      "Reduction due to punctuations and stopwords = 1299.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1306\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1961\n",
      "Cleaning process: Initial size of tokens = 1961\n",
      "Reduction due to punctuations and stopwords = 1299.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1306\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8373\n",
      "Cleaning process: Initial size of tokens = 8373\n",
      "Reduction due to punctuations and stopwords = 6349.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6366\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8373\n",
      "Cleaning process: Initial size of tokens = 8373\n",
      "Reduction due to punctuations and stopwords = 6349.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6366\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3097\n",
      "Cleaning process: Initial size of tokens = 3097\n",
      "Reduction due to punctuations and stopwords = 2041.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2051\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3097\n",
      "Cleaning process: Initial size of tokens = 3097\n",
      "Reduction due to punctuations and stopwords = 2041.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2051\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1881\n",
      "Cleaning process: Initial size of tokens = 1881\n",
      "Reduction due to punctuations and stopwords = 1320.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1324\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1881\n",
      "Cleaning process: Initial size of tokens = 1881\n",
      "Reduction due to punctuations and stopwords = 1320.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1324\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 952\n",
      "Cleaning process: Initial size of tokens = 952\n",
      "Reduction due to punctuations and stopwords = 629.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 635\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 952\n",
      "Cleaning process: Initial size of tokens = 952\n",
      "Reduction due to punctuations and stopwords = 629.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 635\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2128\n",
      "Cleaning process: Initial size of tokens = 2128\n",
      "Reduction due to punctuations and stopwords = 1581.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1589\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2128\n",
      "Cleaning process: Initial size of tokens = 2128\n",
      "Reduction due to punctuations and stopwords = 1581.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1589\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 800\n",
      "Cleaning process: Initial size of tokens = 800\n",
      "Reduction due to punctuations and stopwords = 492.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 499\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 800\n",
      "Cleaning process: Initial size of tokens = 800\n",
      "Reduction due to punctuations and stopwords = 492.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 499\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1000\n",
      "Cleaning process: Initial size of tokens = 1000\n",
      "Reduction due to punctuations and stopwords = 575.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 578\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1000\n",
      "Cleaning process: Initial size of tokens = 1000\n",
      "Reduction due to punctuations and stopwords = 575.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 578\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 995\n",
      "Cleaning process: Initial size of tokens = 995\n",
      "Reduction due to punctuations and stopwords = 638.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 641\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 995\n",
      "Cleaning process: Initial size of tokens = 995\n",
      "Reduction due to punctuations and stopwords = 638.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 641\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6439\n",
      "Cleaning process: Initial size of tokens = 6439\n",
      "Reduction due to punctuations and stopwords = 4738.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4759\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6439\n",
      "Cleaning process: Initial size of tokens = 6439\n",
      "Reduction due to punctuations and stopwords = 4738.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4759\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 464\n",
      "Cleaning process: Initial size of tokens = 464\n",
      "Reduction due to punctuations and stopwords = 251.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 253\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 464\n",
      "Cleaning process: Initial size of tokens = 464\n",
      "Reduction due to punctuations and stopwords = 251.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 253\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3771\n",
      "Cleaning process: Initial size of tokens = 3771\n",
      "Reduction due to punctuations and stopwords = 2819.\n",
      "Reduction due to all numeral terms = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2830\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3771\n",
      "Cleaning process: Initial size of tokens = 3771\n",
      "Reduction due to punctuations and stopwords = 2819.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2830\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 260\n",
      "Cleaning process: Initial size of tokens = 260\n",
      "Reduction due to punctuations and stopwords = 156.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 157\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 260\n",
      "Cleaning process: Initial size of tokens = 260\n",
      "Reduction due to punctuations and stopwords = 156.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 157\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 416\n",
      "Cleaning process: Initial size of tokens = 416\n",
      "Reduction due to punctuations and stopwords = 223.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 225\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 416\n",
      "Cleaning process: Initial size of tokens = 416\n",
      "Reduction due to punctuations and stopwords = 223.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 225\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4125\n",
      "Cleaning process: Initial size of tokens = 4125\n",
      "Reduction due to punctuations and stopwords = 2838.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2864\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4125\n",
      "Cleaning process: Initial size of tokens = 4125\n",
      "Reduction due to punctuations and stopwords = 2838.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2864\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 899\n",
      "Cleaning process: Initial size of tokens = 899\n",
      "Reduction due to punctuations and stopwords = 523.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 528\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 899\n",
      "Cleaning process: Initial size of tokens = 899\n",
      "Reduction due to punctuations and stopwords = 523.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 528\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 548\n",
      "Cleaning process: Initial size of tokens = 548\n",
      "Reduction due to punctuations and stopwords = 335.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 336\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 548\n",
      "Cleaning process: Initial size of tokens = 548\n",
      "Reduction due to punctuations and stopwords = 335.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 336\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 678\n",
      "Cleaning process: Initial size of tokens = 678\n",
      "Reduction due to punctuations and stopwords = 387.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 390\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 678\n",
      "Cleaning process: Initial size of tokens = 678\n",
      "Reduction due to punctuations and stopwords = 387.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 390\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1195\n",
      "Cleaning process: Initial size of tokens = 1195\n",
      "Reduction due to punctuations and stopwords = 715.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 720\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1195\n",
      "Cleaning process: Initial size of tokens = 1195\n",
      "Reduction due to punctuations and stopwords = 715.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 720\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1385\n",
      "Cleaning process: Initial size of tokens = 1385\n",
      "Reduction due to punctuations and stopwords = 951.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 957\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1385\n",
      "Cleaning process: Initial size of tokens = 1385\n",
      "Reduction due to punctuations and stopwords = 951.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 957\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8564\n",
      "Cleaning process: Initial size of tokens = 8564\n",
      "Reduction due to punctuations and stopwords = 6523.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6542\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8564\n",
      "Cleaning process: Initial size of tokens = 8564\n",
      "Reduction due to punctuations and stopwords = 6523.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6542\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3464\n",
      "Cleaning process: Initial size of tokens = 3464\n",
      "Reduction due to punctuations and stopwords = 2608.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2621\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3464\n",
      "Cleaning process: Initial size of tokens = 3464\n",
      "Reduction due to punctuations and stopwords = 2608.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2621\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1739\n",
      "Cleaning process: Initial size of tokens = 1739\n",
      "Reduction due to punctuations and stopwords = 1217.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1227\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1739\n",
      "Cleaning process: Initial size of tokens = 1739\n",
      "Reduction due to punctuations and stopwords = 1217.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1227\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1785\n",
      "Cleaning process: Initial size of tokens = 1785\n",
      "Reduction due to punctuations and stopwords = 1200.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1210\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1785\n",
      "Cleaning process: Initial size of tokens = 1785\n",
      "Reduction due to punctuations and stopwords = 1200.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1210\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 959\n",
      "Cleaning process: Initial size of tokens = 959\n",
      "Reduction due to punctuations and stopwords = 608.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 611\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 959\n",
      "Cleaning process: Initial size of tokens = 959\n",
      "Reduction due to punctuations and stopwords = 608.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 611\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 12560\n",
      "Cleaning process: Initial size of tokens = 12560\n",
      "Reduction due to punctuations and stopwords = 9888.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 18\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9927\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 12560\n",
      "Cleaning process: Initial size of tokens = 12560\n",
      "Reduction due to punctuations and stopwords = 9888.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 18\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9927\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3955\n",
      "Cleaning process: Initial size of tokens = 3955\n",
      "Reduction due to punctuations and stopwords = 2770.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2780\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3955\n",
      "Cleaning process: Initial size of tokens = 3955\n",
      "Reduction due to punctuations and stopwords = 2770.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2780\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2940\n",
      "Cleaning process: Initial size of tokens = 2940\n",
      "Reduction due to punctuations and stopwords = 2200.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2210\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2940\n",
      "Cleaning process: Initial size of tokens = 2940\n",
      "Reduction due to punctuations and stopwords = 2200.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2210\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 681\n",
      "Cleaning process: Initial size of tokens = 681\n",
      "Reduction due to punctuations and stopwords = 380.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 382\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 681\n",
      "Cleaning process: Initial size of tokens = 681\n",
      "Reduction due to punctuations and stopwords = 380.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 382\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 371\n",
      "Cleaning process: Initial size of tokens = 371\n",
      "Reduction due to punctuations and stopwords = 208.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 208\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 371\n",
      "Cleaning process: Initial size of tokens = 371\n",
      "Reduction due to punctuations and stopwords = 208.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 208\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1488\n",
      "Cleaning process: Initial size of tokens = 1488\n",
      "Reduction due to punctuations and stopwords = 982.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 992\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1488\n",
      "Cleaning process: Initial size of tokens = 1488\n",
      "Reduction due to punctuations and stopwords = 982.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 992\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 680\n",
      "Cleaning process: Initial size of tokens = 680\n",
      "Reduction due to punctuations and stopwords = 369.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 374\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 680\n",
      "Cleaning process: Initial size of tokens = 680\n",
      "Reduction due to punctuations and stopwords = 369.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 374\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2451\n",
      "Cleaning process: Initial size of tokens = 2451\n",
      "Reduction due to punctuations and stopwords = 1630.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1640\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2451\n",
      "Cleaning process: Initial size of tokens = 2451\n",
      "Reduction due to punctuations and stopwords = 1630.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1640\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7489\n",
      "Cleaning process: Initial size of tokens = 7489\n",
      "Reduction due to punctuations and stopwords = 6042.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 22\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6086\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7489\n",
      "Cleaning process: Initial size of tokens = 7489\n",
      "Reduction due to punctuations and stopwords = 6042.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 22\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6086\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 882\n",
      "Cleaning process: Initial size of tokens = 882\n",
      "Reduction due to punctuations and stopwords = 667.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 668\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 882\n",
      "Cleaning process: Initial size of tokens = 882\n",
      "Reduction due to punctuations and stopwords = 667.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 668\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 591\n",
      "Cleaning process: Initial size of tokens = 591\n",
      "Reduction due to punctuations and stopwords = 338.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 340\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 591\n",
      "Cleaning process: Initial size of tokens = 591\n",
      "Reduction due to punctuations and stopwords = 338.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 340\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2960\n",
      "Cleaning process: Initial size of tokens = 2960\n",
      "Reduction due to punctuations and stopwords = 1971.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1987\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2960\n",
      "Cleaning process: Initial size of tokens = 2960\n",
      "Reduction due to punctuations and stopwords = 1971.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1987\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3751\n",
      "Cleaning process: Initial size of tokens = 3751\n",
      "Reduction due to punctuations and stopwords = 2737.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2749\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3751\n",
      "Cleaning process: Initial size of tokens = 3751\n",
      "Reduction due to punctuations and stopwords = 2737.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2749\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1986\n",
      "Cleaning process: Initial size of tokens = 1986\n",
      "Reduction due to punctuations and stopwords = 1410.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1410\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1986\n",
      "Cleaning process: Initial size of tokens = 1986\n",
      "Reduction due to punctuations and stopwords = 1410.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1410\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 407\n",
      "Cleaning process: Initial size of tokens = 407\n",
      "Reduction due to punctuations and stopwords = 281.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 285\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 407\n",
      "Cleaning process: Initial size of tokens = 407\n",
      "Reduction due to punctuations and stopwords = 281.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 285\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3670\n",
      "Cleaning process: Initial size of tokens = 3670\n",
      "Reduction due to punctuations and stopwords = 2712.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2714\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3670\n",
      "Cleaning process: Initial size of tokens = 3670\n",
      "Reduction due to punctuations and stopwords = 2712.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2714\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 545\n",
      "Cleaning process: Initial size of tokens = 545\n",
      "Reduction due to punctuations and stopwords = 298.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 300\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 545\n",
      "Cleaning process: Initial size of tokens = 545\n",
      "Reduction due to punctuations and stopwords = 298.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 300\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 761\n",
      "Cleaning process: Initial size of tokens = 761\n",
      "Reduction due to punctuations and stopwords = 489.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 493\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 761\n",
      "Cleaning process: Initial size of tokens = 761\n",
      "Reduction due to punctuations and stopwords = 489.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 493\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 194\n",
      "Cleaning process: Initial size of tokens = 194\n",
      "Reduction due to punctuations and stopwords = 109.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 109\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 194\n",
      "Cleaning process: Initial size of tokens = 194\n",
      "Reduction due to punctuations and stopwords = 109.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 109\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1849\n",
      "Cleaning process: Initial size of tokens = 1849\n",
      "Reduction due to punctuations and stopwords = 1143.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1152\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1849\n",
      "Cleaning process: Initial size of tokens = 1849\n",
      "Reduction due to punctuations and stopwords = 1143.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1152\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 14299\n",
      "Cleaning process: Initial size of tokens = 14299\n",
      "Reduction due to punctuations and stopwords = 11205.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 21\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 11235\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 14299\n",
      "Cleaning process: Initial size of tokens = 14299\n",
      "Reduction due to punctuations and stopwords = 11205.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 21\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 11235\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2933\n",
      "Cleaning process: Initial size of tokens = 2933\n",
      "Reduction due to punctuations and stopwords = 2014.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2025\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2933\n",
      "Cleaning process: Initial size of tokens = 2933\n",
      "Reduction due to punctuations and stopwords = 2014.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2025\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4033\n",
      "Cleaning process: Initial size of tokens = 4033\n",
      "Reduction due to punctuations and stopwords = 2851.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2865\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4033\n",
      "Cleaning process: Initial size of tokens = 4033\n",
      "Reduction due to punctuations and stopwords = 2851.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2865\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2698\n",
      "Cleaning process: Initial size of tokens = 2698\n",
      "Reduction due to punctuations and stopwords = 1714.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1723\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2698\n",
      "Cleaning process: Initial size of tokens = 2698\n",
      "Reduction due to punctuations and stopwords = 1714.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1723\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3760\n",
      "Cleaning process: Initial size of tokens = 3760\n",
      "Reduction due to punctuations and stopwords = 2685.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2694\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3760\n",
      "Cleaning process: Initial size of tokens = 3760\n",
      "Reduction due to punctuations and stopwords = 2685.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2694\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3787\n",
      "Cleaning process: Initial size of tokens = 3787\n",
      "Reduction due to punctuations and stopwords = 2610.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2618\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3787\n",
      "Cleaning process: Initial size of tokens = 3787\n",
      "Reduction due to punctuations and stopwords = 2610.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2618\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1932\n",
      "Cleaning process: Initial size of tokens = 1932\n",
      "Reduction due to punctuations and stopwords = 1245.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1249\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1932\n",
      "Cleaning process: Initial size of tokens = 1932\n",
      "Reduction due to punctuations and stopwords = 1245.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1249\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3760\n",
      "Cleaning process: Initial size of tokens = 3760\n",
      "Reduction due to punctuations and stopwords = 2600.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2610\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3760\n",
      "Cleaning process: Initial size of tokens = 3760\n",
      "Reduction due to punctuations and stopwords = 2600.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2610\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3440\n",
      "Cleaning process: Initial size of tokens = 3440\n",
      "Reduction due to punctuations and stopwords = 2459.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2488\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3440\n",
      "Cleaning process: Initial size of tokens = 3440\n",
      "Reduction due to punctuations and stopwords = 2459.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2488\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1813\n",
      "Cleaning process: Initial size of tokens = 1813\n",
      "Reduction due to punctuations and stopwords = 1299.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1302\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1813\n",
      "Cleaning process: Initial size of tokens = 1813\n",
      "Reduction due to punctuations and stopwords = 1299.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1302\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6519\n",
      "Cleaning process: Initial size of tokens = 6519\n",
      "Reduction due to punctuations and stopwords = 4649.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4701\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6519\n",
      "Cleaning process: Initial size of tokens = 6519\n",
      "Reduction due to punctuations and stopwords = 4649.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4701\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5300\n",
      "Cleaning process: Initial size of tokens = 5300\n",
      "Reduction due to punctuations and stopwords = 3772.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3785\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5300\n",
      "Cleaning process: Initial size of tokens = 5300\n",
      "Reduction due to punctuations and stopwords = 3772.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3785\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3839\n",
      "Cleaning process: Initial size of tokens = 3839\n",
      "Reduction due to punctuations and stopwords = 2414.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2421\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3839\n",
      "Cleaning process: Initial size of tokens = 3839\n",
      "Reduction due to punctuations and stopwords = 2414.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2421\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 582\n",
      "Cleaning process: Initial size of tokens = 582\n",
      "Reduction due to punctuations and stopwords = 269.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 281\n",
      "Percentage = 48%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 582\n",
      "Cleaning process: Initial size of tokens = 582\n",
      "Reduction due to punctuations and stopwords = 269.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 281\n",
      "Percentage = 48%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2054\n",
      "Cleaning process: Initial size of tokens = 2054\n",
      "Reduction due to punctuations and stopwords = 1396.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1400\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2054\n",
      "Cleaning process: Initial size of tokens = 2054\n",
      "Reduction due to punctuations and stopwords = 1396.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1400\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2272\n",
      "Cleaning process: Initial size of tokens = 2272\n",
      "Reduction due to punctuations and stopwords = 1241.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1244\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2272\n",
      "Cleaning process: Initial size of tokens = 2272\n",
      "Reduction due to punctuations and stopwords = 1241.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1244\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 787\n",
      "Cleaning process: Initial size of tokens = 787\n",
      "Reduction due to punctuations and stopwords = 502.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 509\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 787\n",
      "Cleaning process: Initial size of tokens = 787\n",
      "Reduction due to punctuations and stopwords = 502.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 509\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2079\n",
      "Cleaning process: Initial size of tokens = 2079\n",
      "Reduction due to punctuations and stopwords = 1436.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1441\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2079\n",
      "Cleaning process: Initial size of tokens = 2079\n",
      "Reduction due to punctuations and stopwords = 1436.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1441\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2231\n",
      "Cleaning process: Initial size of tokens = 2231\n",
      "Reduction due to punctuations and stopwords = 1638.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1646\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2231\n",
      "Cleaning process: Initial size of tokens = 2231\n",
      "Reduction due to punctuations and stopwords = 1638.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1646\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2271\n",
      "Cleaning process: Initial size of tokens = 2271\n",
      "Reduction due to punctuations and stopwords = 1576.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1593\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2271\n",
      "Cleaning process: Initial size of tokens = 2271\n",
      "Reduction due to punctuations and stopwords = 1576.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1593\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1605\n",
      "Cleaning process: Initial size of tokens = 1605\n",
      "Reduction due to punctuations and stopwords = 1040.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1045\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1605\n",
      "Cleaning process: Initial size of tokens = 1605\n",
      "Reduction due to punctuations and stopwords = 1040.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1045\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4603\n",
      "Cleaning process: Initial size of tokens = 4603\n",
      "Reduction due to punctuations and stopwords = 3334.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3346\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4603\n",
      "Cleaning process: Initial size of tokens = 4603\n",
      "Reduction due to punctuations and stopwords = 3334.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3346\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1298\n",
      "Cleaning process: Initial size of tokens = 1298\n",
      "Reduction due to punctuations and stopwords = 1064.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1069\n",
      "Percentage = 82%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1298\n",
      "Cleaning process: Initial size of tokens = 1298\n",
      "Reduction due to punctuations and stopwords = 1064.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1069\n",
      "Percentage = 82%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 792\n",
      "Cleaning process: Initial size of tokens = 792\n",
      "Reduction due to punctuations and stopwords = 477.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 481\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 792\n",
      "Cleaning process: Initial size of tokens = 792\n",
      "Reduction due to punctuations and stopwords = 477.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 481\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3886\n",
      "Cleaning process: Initial size of tokens = 3886\n",
      "Reduction due to punctuations and stopwords = 2570.\n",
      "Reduction due to all numeral terms = 30\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2621\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3886\n",
      "Cleaning process: Initial size of tokens = 3886\n",
      "Reduction due to punctuations and stopwords = 2570.\n",
      "Reduction due to all numeral terms = 30\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2621\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1669\n",
      "Cleaning process: Initial size of tokens = 1669\n",
      "Reduction due to punctuations and stopwords = 1031.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1040\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1669\n",
      "Cleaning process: Initial size of tokens = 1669\n",
      "Reduction due to punctuations and stopwords = 1031.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1040\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1907\n",
      "Cleaning process: Initial size of tokens = 1907\n",
      "Reduction due to punctuations and stopwords = 1255.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1263\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1907\n",
      "Cleaning process: Initial size of tokens = 1907\n",
      "Reduction due to punctuations and stopwords = 1255.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1263\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3704\n",
      "Cleaning process: Initial size of tokens = 3704\n",
      "Reduction due to punctuations and stopwords = 2525.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2537\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3704\n",
      "Cleaning process: Initial size of tokens = 3704\n",
      "Reduction due to punctuations and stopwords = 2525.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2537\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3113\n",
      "Cleaning process: Initial size of tokens = 3113\n",
      "Reduction due to punctuations and stopwords = 2079.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2085\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3113\n",
      "Cleaning process: Initial size of tokens = 3113\n",
      "Reduction due to punctuations and stopwords = 2079.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2085\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3851\n",
      "Cleaning process: Initial size of tokens = 3851\n",
      "Reduction due to punctuations and stopwords = 2707.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2717\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3851\n",
      "Cleaning process: Initial size of tokens = 3851\n",
      "Reduction due to punctuations and stopwords = 2707.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2717\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 956\n",
      "Cleaning process: Initial size of tokens = 956\n",
      "Reduction due to punctuations and stopwords = 510.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 516\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 956\n",
      "Cleaning process: Initial size of tokens = 956\n",
      "Reduction due to punctuations and stopwords = 510.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 516\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2519\n",
      "Cleaning process: Initial size of tokens = 2519\n",
      "Reduction due to punctuations and stopwords = 1693.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1699\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2519\n",
      "Cleaning process: Initial size of tokens = 2519\n",
      "Reduction due to punctuations and stopwords = 1693.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1699\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4567\n",
      "Cleaning process: Initial size of tokens = 4567\n",
      "Reduction due to punctuations and stopwords = 3529.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3547\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4567\n",
      "Cleaning process: Initial size of tokens = 4567\n",
      "Reduction due to punctuations and stopwords = 3529.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3547\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2973\n",
      "Cleaning process: Initial size of tokens = 2973\n",
      "Reduction due to punctuations and stopwords = 1978.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2006\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2973\n",
      "Cleaning process: Initial size of tokens = 2973\n",
      "Reduction due to punctuations and stopwords = 1978.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2006\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3506\n",
      "Cleaning process: Initial size of tokens = 3506\n",
      "Reduction due to punctuations and stopwords = 2377.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2385\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3506\n",
      "Cleaning process: Initial size of tokens = 3506\n",
      "Reduction due to punctuations and stopwords = 2377.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2385\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3816\n",
      "Cleaning process: Initial size of tokens = 3816\n",
      "Reduction due to punctuations and stopwords = 2627.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2642\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3816\n",
      "Cleaning process: Initial size of tokens = 3816\n",
      "Reduction due to punctuations and stopwords = 2627.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2642\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3083\n",
      "Cleaning process: Initial size of tokens = 3083\n",
      "Reduction due to punctuations and stopwords = 2046.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2057\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3083\n",
      "Cleaning process: Initial size of tokens = 3083\n",
      "Reduction due to punctuations and stopwords = 2046.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2057\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1815\n",
      "Cleaning process: Initial size of tokens = 1815\n",
      "Reduction due to punctuations and stopwords = 1214.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1216\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1815\n",
      "Cleaning process: Initial size of tokens = 1815\n",
      "Reduction due to punctuations and stopwords = 1214.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1216\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1156\n",
      "Cleaning process: Initial size of tokens = 1156\n",
      "Reduction due to punctuations and stopwords = 635.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 641\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1156\n",
      "Cleaning process: Initial size of tokens = 1156\n",
      "Reduction due to punctuations and stopwords = 635.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 641\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 176\n",
      "Cleaning process: Initial size of tokens = 176\n",
      "Reduction due to punctuations and stopwords = 94.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 94\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 176\n",
      "Cleaning process: Initial size of tokens = 176\n",
      "Reduction due to punctuations and stopwords = 94.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 94\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2342\n",
      "Cleaning process: Initial size of tokens = 2342\n",
      "Reduction due to punctuations and stopwords = 1496.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1503\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2342\n",
      "Cleaning process: Initial size of tokens = 2342\n",
      "Reduction due to punctuations and stopwords = 1496.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1503\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 311\n",
      "Cleaning process: Initial size of tokens = 311\n",
      "Reduction due to punctuations and stopwords = 164.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 165\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 311\n",
      "Cleaning process: Initial size of tokens = 311\n",
      "Reduction due to punctuations and stopwords = 164.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 165\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 511\n",
      "Cleaning process: Initial size of tokens = 511\n",
      "Reduction due to punctuations and stopwords = 306.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 308\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 511\n",
      "Cleaning process: Initial size of tokens = 511\n",
      "Reduction due to punctuations and stopwords = 306.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 308\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 9639\n",
      "Cleaning process: Initial size of tokens = 9639\n",
      "Reduction due to punctuations and stopwords = 7569.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7605\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 9639\n",
      "Cleaning process: Initial size of tokens = 9639\n",
      "Reduction due to punctuations and stopwords = 7569.\n",
      "Reduction due to all numeral terms = 21\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7605\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3063\n",
      "Cleaning process: Initial size of tokens = 3063\n",
      "Reduction due to punctuations and stopwords = 2090.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2111\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3063\n",
      "Cleaning process: Initial size of tokens = 3063\n",
      "Reduction due to punctuations and stopwords = 2090.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2111\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1804\n",
      "Cleaning process: Initial size of tokens = 1804\n",
      "Reduction due to punctuations and stopwords = 1187.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1198\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1804\n",
      "Cleaning process: Initial size of tokens = 1804\n",
      "Reduction due to punctuations and stopwords = 1187.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1198\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2002\n",
      "Cleaning process: Initial size of tokens = 2002\n",
      "Reduction due to punctuations and stopwords = 1306.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1311\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2002\n",
      "Cleaning process: Initial size of tokens = 2002\n",
      "Reduction due to punctuations and stopwords = 1306.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1311\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5785\n",
      "Cleaning process: Initial size of tokens = 5785\n",
      "Reduction due to punctuations and stopwords = 4382.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4390\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5785\n",
      "Cleaning process: Initial size of tokens = 5785\n",
      "Reduction due to punctuations and stopwords = 4382.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4390\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2090\n",
      "Cleaning process: Initial size of tokens = 2090\n",
      "Reduction due to punctuations and stopwords = 1469.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1471\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2090\n",
      "Cleaning process: Initial size of tokens = 2090\n",
      "Reduction due to punctuations and stopwords = 1469.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1471\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6022\n",
      "Cleaning process: Initial size of tokens = 6022\n",
      "Reduction due to punctuations and stopwords = 4365.\n",
      "Reduction due to all numeral terms = 19\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 15\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4407\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6022\n",
      "Cleaning process: Initial size of tokens = 6022\n",
      "Reduction due to punctuations and stopwords = 4365.\n",
      "Reduction due to all numeral terms = 19\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 15\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4407\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2649\n",
      "Cleaning process: Initial size of tokens = 2649\n",
      "Reduction due to punctuations and stopwords = 1683.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1703\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2649\n",
      "Cleaning process: Initial size of tokens = 2649\n",
      "Reduction due to punctuations and stopwords = 1683.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1703\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3245\n",
      "Cleaning process: Initial size of tokens = 3245\n",
      "Reduction due to punctuations and stopwords = 2308.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2316\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3245\n",
      "Cleaning process: Initial size of tokens = 3245\n",
      "Reduction due to punctuations and stopwords = 2308.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2316\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5329\n",
      "Cleaning process: Initial size of tokens = 5329\n",
      "Reduction due to punctuations and stopwords = 3868.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 57\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3951\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5329\n",
      "Cleaning process: Initial size of tokens = 5329\n",
      "Reduction due to punctuations and stopwords = 3868.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 57\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3951\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4092\n",
      "Cleaning process: Initial size of tokens = 4092\n",
      "Reduction due to punctuations and stopwords = 2936.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2954\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4092\n",
      "Cleaning process: Initial size of tokens = 4092\n",
      "Reduction due to punctuations and stopwords = 2936.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2954\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 740\n",
      "Cleaning process: Initial size of tokens = 740\n",
      "Reduction due to punctuations and stopwords = 429.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 435\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 740\n",
      "Cleaning process: Initial size of tokens = 740\n",
      "Reduction due to punctuations and stopwords = 429.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 435\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 430\n",
      "Cleaning process: Initial size of tokens = 430\n",
      "Reduction due to punctuations and stopwords = 207.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 210\n",
      "Percentage = 49%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 430\n",
      "Cleaning process: Initial size of tokens = 430\n",
      "Reduction due to punctuations and stopwords = 207.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 210\n",
      "Percentage = 49%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2825\n",
      "Cleaning process: Initial size of tokens = 2825\n",
      "Reduction due to punctuations and stopwords = 1935.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1944\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2825\n",
      "Cleaning process: Initial size of tokens = 2825\n",
      "Reduction due to punctuations and stopwords = 1935.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1944\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Number of terms: 14293\n",
      "Cleaning process: Initial size of tokens = 14293\n",
      "Reduction due to punctuations and stopwords = 11774.\n",
      "Reduction due to all numeral terms = 69\n",
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 11868\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 14293\n",
      "Cleaning process: Initial size of tokens = 14293\n",
      "Reduction due to punctuations and stopwords = 11774.\n",
      "Reduction due to all numeral terms = 69\n",
      "Reduction due to short terms = 14\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 11\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 11868\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 28\n",
      "Cleaning process: Initial size of tokens = 28\n",
      "Reduction due to punctuations and stopwords = 14.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 14\n",
      "Percentage = 50%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 28\n",
      "Cleaning process: Initial size of tokens = 28\n",
      "Reduction due to punctuations and stopwords = 14.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 14\n",
      "Percentage = 50%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 681\n",
      "Cleaning process: Initial size of tokens = 681\n",
      "Reduction due to punctuations and stopwords = 389.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 392\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 681\n",
      "Cleaning process: Initial size of tokens = 681\n",
      "Reduction due to punctuations and stopwords = 389.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 392\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1535\n",
      "Cleaning process: Initial size of tokens = 1535\n",
      "Reduction due to punctuations and stopwords = 1011.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1017\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1535\n",
      "Cleaning process: Initial size of tokens = 1535\n",
      "Reduction due to punctuations and stopwords = 1011.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1017\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11279\n",
      "Cleaning process: Initial size of tokens = 11279\n",
      "Reduction due to punctuations and stopwords = 8983.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9000\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11279\n",
      "Cleaning process: Initial size of tokens = 11279\n",
      "Reduction due to punctuations and stopwords = 8983.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9000\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1435\n",
      "Cleaning process: Initial size of tokens = 1435\n",
      "Reduction due to punctuations and stopwords = 734.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 741\n",
      "Percentage = 52%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1435\n",
      "Cleaning process: Initial size of tokens = 1435\n",
      "Reduction due to punctuations and stopwords = 734.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 741\n",
      "Percentage = 52%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 606\n",
      "Cleaning process: Initial size of tokens = 606\n",
      "Reduction due to punctuations and stopwords = 357.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 360\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 606\n",
      "Cleaning process: Initial size of tokens = 606\n",
      "Reduction due to punctuations and stopwords = 357.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 360\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 596\n",
      "Cleaning process: Initial size of tokens = 596\n",
      "Reduction due to punctuations and stopwords = 338.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 344\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 596\n",
      "Cleaning process: Initial size of tokens = 596\n",
      "Reduction due to punctuations and stopwords = 338.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 344\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4474\n",
      "Cleaning process: Initial size of tokens = 4474\n",
      "Reduction due to punctuations and stopwords = 3461.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3481\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4474\n",
      "Cleaning process: Initial size of tokens = 4474\n",
      "Reduction due to punctuations and stopwords = 3461.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3481\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1215\n",
      "Cleaning process: Initial size of tokens = 1215\n",
      "Reduction due to punctuations and stopwords = 750.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 752\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1215\n",
      "Cleaning process: Initial size of tokens = 1215\n",
      "Reduction due to punctuations and stopwords = 750.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 752\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2503\n",
      "Cleaning process: Initial size of tokens = 2503\n",
      "Reduction due to punctuations and stopwords = 1491.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1502\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2503\n",
      "Cleaning process: Initial size of tokens = 2503\n",
      "Reduction due to punctuations and stopwords = 1491.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1502\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2959\n",
      "Cleaning process: Initial size of tokens = 2959\n",
      "Reduction due to punctuations and stopwords = 2128.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2145\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2959\n",
      "Cleaning process: Initial size of tokens = 2959\n",
      "Reduction due to punctuations and stopwords = 2128.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2145\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 783\n",
      "Cleaning process: Initial size of tokens = 783\n",
      "Reduction due to punctuations and stopwords = 473.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 492\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 783\n",
      "Cleaning process: Initial size of tokens = 783\n",
      "Reduction due to punctuations and stopwords = 473.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 492\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 488\n",
      "Cleaning process: Initial size of tokens = 488\n",
      "Reduction due to punctuations and stopwords = 301.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 301\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 488\n",
      "Cleaning process: Initial size of tokens = 488\n",
      "Reduction due to punctuations and stopwords = 301.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 301\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1580\n",
      "Cleaning process: Initial size of tokens = 1580\n",
      "Reduction due to punctuations and stopwords = 947.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 949\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1580\n",
      "Cleaning process: Initial size of tokens = 1580\n",
      "Reduction due to punctuations and stopwords = 947.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 949\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6884\n",
      "Cleaning process: Initial size of tokens = 6884\n",
      "Reduction due to punctuations and stopwords = 5098.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5111\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6884\n",
      "Cleaning process: Initial size of tokens = 6884\n",
      "Reduction due to punctuations and stopwords = 5098.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5111\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1256\n",
      "Cleaning process: Initial size of tokens = 1256\n",
      "Reduction due to punctuations and stopwords = 798.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 802\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1256\n",
      "Cleaning process: Initial size of tokens = 1256\n",
      "Reduction due to punctuations and stopwords = 798.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 802\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 369\n",
      "Cleaning process: Initial size of tokens = 369\n",
      "Reduction due to punctuations and stopwords = 232.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 232\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 369\n",
      "Cleaning process: Initial size of tokens = 369\n",
      "Reduction due to punctuations and stopwords = 232.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 232\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3240\n",
      "Cleaning process: Initial size of tokens = 3240\n",
      "Reduction due to punctuations and stopwords = 2359.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2368\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3240\n",
      "Cleaning process: Initial size of tokens = 3240\n",
      "Reduction due to punctuations and stopwords = 2359.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2368\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5373\n",
      "Cleaning process: Initial size of tokens = 5373\n",
      "Reduction due to punctuations and stopwords = 4072.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4096\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5373\n",
      "Cleaning process: Initial size of tokens = 5373\n",
      "Reduction due to punctuations and stopwords = 4072.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4096\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1338\n",
      "Cleaning process: Initial size of tokens = 1338\n",
      "Reduction due to punctuations and stopwords = 929.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 933\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1338\n",
      "Cleaning process: Initial size of tokens = 1338\n",
      "Reduction due to punctuations and stopwords = 929.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 933\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 790\n",
      "Cleaning process: Initial size of tokens = 790\n",
      "Reduction due to punctuations and stopwords = 460.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 465\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 790\n",
      "Cleaning process: Initial size of tokens = 790\n",
      "Reduction due to punctuations and stopwords = 460.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 465\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7628\n",
      "Cleaning process: Initial size of tokens = 7628\n",
      "Reduction due to punctuations and stopwords = 5879.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5900\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Number of terms: 7628\n",
      "Cleaning process: Initial size of tokens = 7628\n",
      "Reduction due to punctuations and stopwords = 5879.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5900\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 503\n",
      "Cleaning process: Initial size of tokens = 503\n",
      "Reduction due to punctuations and stopwords = 251.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 255\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 503\n",
      "Cleaning process: Initial size of tokens = 503\n",
      "Reduction due to punctuations and stopwords = 251.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 255\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1434\n",
      "Cleaning process: Initial size of tokens = 1434\n",
      "Reduction due to punctuations and stopwords = 869.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 873\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1434\n",
      "Cleaning process: Initial size of tokens = 1434\n",
      "Reduction due to punctuations and stopwords = 869.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 873\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 525\n",
      "Cleaning process: Initial size of tokens = 525\n",
      "Reduction due to punctuations and stopwords = 274.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 277\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 525\n",
      "Cleaning process: Initial size of tokens = 525\n",
      "Reduction due to punctuations and stopwords = 274.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 277\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1098\n",
      "Cleaning process: Initial size of tokens = 1098\n",
      "Reduction due to punctuations and stopwords = 615.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 620\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1098\n",
      "Cleaning process: Initial size of tokens = 1098\n",
      "Reduction due to punctuations and stopwords = 615.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 620\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3482\n",
      "Cleaning process: Initial size of tokens = 3482\n",
      "Reduction due to punctuations and stopwords = 2455.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2461\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3482\n",
      "Cleaning process: Initial size of tokens = 3482\n",
      "Reduction due to punctuations and stopwords = 2455.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2461\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1461\n",
      "Cleaning process: Initial size of tokens = 1461\n",
      "Reduction due to punctuations and stopwords = 951.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 956\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1461\n",
      "Cleaning process: Initial size of tokens = 1461\n",
      "Reduction due to punctuations and stopwords = 951.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 956\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1262\n",
      "Cleaning process: Initial size of tokens = 1262\n",
      "Reduction due to punctuations and stopwords = 771.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 777\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1262\n",
      "Cleaning process: Initial size of tokens = 1262\n",
      "Reduction due to punctuations and stopwords = 771.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 777\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 276\n",
      "Cleaning process: Initial size of tokens = 276\n",
      "Reduction due to punctuations and stopwords = 174.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 175\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 276\n",
      "Cleaning process: Initial size of tokens = 276\n",
      "Reduction due to punctuations and stopwords = 174.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 175\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 801\n",
      "Cleaning process: Initial size of tokens = 801\n",
      "Reduction due to punctuations and stopwords = 444.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 448\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 801\n",
      "Cleaning process: Initial size of tokens = 801\n",
      "Reduction due to punctuations and stopwords = 444.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 448\n",
      "Percentage = 56%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2955\n",
      "Cleaning process: Initial size of tokens = 2955\n",
      "Reduction due to punctuations and stopwords = 1936.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1950\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2955\n",
      "Cleaning process: Initial size of tokens = 2955\n",
      "Reduction due to punctuations and stopwords = 1936.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1950\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2518\n",
      "Cleaning process: Initial size of tokens = 2518\n",
      "Reduction due to punctuations and stopwords = 1730.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1732\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2518\n",
      "Cleaning process: Initial size of tokens = 2518\n",
      "Reduction due to punctuations and stopwords = 1730.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1732\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1201\n",
      "Cleaning process: Initial size of tokens = 1201\n",
      "Reduction due to punctuations and stopwords = 652.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 660\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1201\n",
      "Cleaning process: Initial size of tokens = 1201\n",
      "Reduction due to punctuations and stopwords = 652.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 660\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4228\n",
      "Cleaning process: Initial size of tokens = 4228\n",
      "Reduction due to punctuations and stopwords = 2906.\n",
      "Reduction due to all numeral terms = 25\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2947\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4228\n",
      "Cleaning process: Initial size of tokens = 4228\n",
      "Reduction due to punctuations and stopwords = 2906.\n",
      "Reduction due to all numeral terms = 25\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2947\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 399\n",
      "Cleaning process: Initial size of tokens = 399\n",
      "Reduction due to punctuations and stopwords = 201.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 205\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 399\n",
      "Cleaning process: Initial size of tokens = 399\n",
      "Reduction due to punctuations and stopwords = 201.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 205\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5018\n",
      "Cleaning process: Initial size of tokens = 5018\n",
      "Reduction due to punctuations and stopwords = 3669.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3680\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5018\n",
      "Cleaning process: Initial size of tokens = 5018\n",
      "Reduction due to punctuations and stopwords = 3669.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3680\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2432\n",
      "Cleaning process: Initial size of tokens = 2432\n",
      "Reduction due to punctuations and stopwords = 1693.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1701\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2432\n",
      "Cleaning process: Initial size of tokens = 2432\n",
      "Reduction due to punctuations and stopwords = 1693.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1701\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3786\n",
      "Cleaning process: Initial size of tokens = 3786\n",
      "Reduction due to punctuations and stopwords = 2688.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2699\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3786\n",
      "Cleaning process: Initial size of tokens = 3786\n",
      "Reduction due to punctuations and stopwords = 2688.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2699\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 885\n",
      "Cleaning process: Initial size of tokens = 885\n",
      "Reduction due to punctuations and stopwords = 583.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 584\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 885\n",
      "Cleaning process: Initial size of tokens = 885\n",
      "Reduction due to punctuations and stopwords = 583.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 584\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 498\n",
      "Cleaning process: Initial size of tokens = 498\n",
      "Reduction due to punctuations and stopwords = 239.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 244\n",
      "Percentage = 49%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 498\n",
      "Cleaning process: Initial size of tokens = 498\n",
      "Reduction due to punctuations and stopwords = 239.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 244\n",
      "Percentage = 49%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2593\n",
      "Cleaning process: Initial size of tokens = 2593\n",
      "Reduction due to punctuations and stopwords = 1703.\n",
      "Reduction due to all numeral terms = 36\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1744\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2593\n",
      "Cleaning process: Initial size of tokens = 2593\n",
      "Reduction due to punctuations and stopwords = 1703.\n",
      "Reduction due to all numeral terms = 36\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1744\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1760\n",
      "Cleaning process: Initial size of tokens = 1760\n",
      "Reduction due to punctuations and stopwords = 1103.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1106\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1760\n",
      "Cleaning process: Initial size of tokens = 1760\n",
      "Reduction due to punctuations and stopwords = 1103.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1106\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2002\n",
      "Cleaning process: Initial size of tokens = 2002\n",
      "Reduction due to punctuations and stopwords = 1299.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1307\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2002\n",
      "Cleaning process: Initial size of tokens = 2002\n",
      "Reduction due to punctuations and stopwords = 1299.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1307\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 672\n",
      "Cleaning process: Initial size of tokens = 672\n",
      "Reduction due to punctuations and stopwords = 434.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 434\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 672\n",
      "Cleaning process: Initial size of tokens = 672\n",
      "Reduction due to punctuations and stopwords = 434.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 434\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1604\n",
      "Cleaning process: Initial size of tokens = 1604\n",
      "Reduction due to punctuations and stopwords = 1068.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1068\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1604\n",
      "Cleaning process: Initial size of tokens = 1604\n",
      "Reduction due to punctuations and stopwords = 1068.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1068\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2316\n",
      "Cleaning process: Initial size of tokens = 2316\n",
      "Reduction due to punctuations and stopwords = 1631.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1638\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2316\n",
      "Cleaning process: Initial size of tokens = 2316\n",
      "Reduction due to punctuations and stopwords = 1631.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1638\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 201\n",
      "Cleaning process: Initial size of tokens = 201\n",
      "Reduction due to punctuations and stopwords = 135.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 141\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 201\n",
      "Cleaning process: Initial size of tokens = 201\n",
      "Reduction due to punctuations and stopwords = 135.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 141\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 226\n",
      "Cleaning process: Initial size of tokens = 226\n",
      "Reduction due to punctuations and stopwords = 116.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 116\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 226\n",
      "Cleaning process: Initial size of tokens = 226\n",
      "Reduction due to punctuations and stopwords = 116.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 116\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3137\n",
      "Cleaning process: Initial size of tokens = 3137\n",
      "Reduction due to punctuations and stopwords = 2393.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2395\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3137\n",
      "Cleaning process: Initial size of tokens = 3137\n",
      "Reduction due to punctuations and stopwords = 2393.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2395\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5688\n",
      "Cleaning process: Initial size of tokens = 5688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to punctuations and stopwords = 4321.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4339\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5688\n",
      "Cleaning process: Initial size of tokens = 5688\n",
      "Reduction due to punctuations and stopwords = 4321.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4339\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3082\n",
      "Cleaning process: Initial size of tokens = 3082\n",
      "Reduction due to punctuations and stopwords = 2157.\n",
      "Reduction due to all numeral terms = 12\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2176\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3082\n",
      "Cleaning process: Initial size of tokens = 3082\n",
      "Reduction due to punctuations and stopwords = 2157.\n",
      "Reduction due to all numeral terms = 12\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2176\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 256\n",
      "Cleaning process: Initial size of tokens = 256\n",
      "Reduction due to punctuations and stopwords = 133.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 135\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 256\n",
      "Cleaning process: Initial size of tokens = 256\n",
      "Reduction due to punctuations and stopwords = 133.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 135\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3274\n",
      "Cleaning process: Initial size of tokens = 3274\n",
      "Reduction due to punctuations and stopwords = 2362.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2369\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3274\n",
      "Cleaning process: Initial size of tokens = 3274\n",
      "Reduction due to punctuations and stopwords = 2362.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2369\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 158\n",
      "Cleaning process: Initial size of tokens = 158\n",
      "Reduction due to punctuations and stopwords = 87.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 87\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 158\n",
      "Cleaning process: Initial size of tokens = 158\n",
      "Reduction due to punctuations and stopwords = 87.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 87\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1487\n",
      "Cleaning process: Initial size of tokens = 1487\n",
      "Reduction due to punctuations and stopwords = 932.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 949\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1487\n",
      "Cleaning process: Initial size of tokens = 1487\n",
      "Reduction due to punctuations and stopwords = 932.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 949\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2237\n",
      "Cleaning process: Initial size of tokens = 2237\n",
      "Reduction due to punctuations and stopwords = 1564.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1567\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2237\n",
      "Cleaning process: Initial size of tokens = 2237\n",
      "Reduction due to punctuations and stopwords = 1564.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1567\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5352\n",
      "Cleaning process: Initial size of tokens = 5352\n",
      "Reduction due to punctuations and stopwords = 3806.\n",
      "Reduction due to all numeral terms = 52\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3873\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5352\n",
      "Cleaning process: Initial size of tokens = 5352\n",
      "Reduction due to punctuations and stopwords = 3806.\n",
      "Reduction due to all numeral terms = 52\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3873\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5361\n",
      "Cleaning process: Initial size of tokens = 5361\n",
      "Reduction due to punctuations and stopwords = 4142.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4162\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5361\n",
      "Cleaning process: Initial size of tokens = 5361\n",
      "Reduction due to punctuations and stopwords = 4142.\n",
      "Reduction due to all numeral terms = 14\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4162\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1665\n",
      "Cleaning process: Initial size of tokens = 1665\n",
      "Reduction due to punctuations and stopwords = 1156.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1162\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1665\n",
      "Cleaning process: Initial size of tokens = 1665\n",
      "Reduction due to punctuations and stopwords = 1156.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1162\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1968\n",
      "Cleaning process: Initial size of tokens = 1968\n",
      "Reduction due to punctuations and stopwords = 1365.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1370\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1968\n",
      "Cleaning process: Initial size of tokens = 1968\n",
      "Reduction due to punctuations and stopwords = 1365.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1370\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2687\n",
      "Cleaning process: Initial size of tokens = 2687\n",
      "Reduction due to punctuations and stopwords = 1960.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1966\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2687\n",
      "Cleaning process: Initial size of tokens = 2687\n",
      "Reduction due to punctuations and stopwords = 1960.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1966\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2789\n",
      "Cleaning process: Initial size of tokens = 2789\n",
      "Reduction due to punctuations and stopwords = 1962.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1966\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2789\n",
      "Cleaning process: Initial size of tokens = 2789\n",
      "Reduction due to punctuations and stopwords = 1962.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1966\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4913\n",
      "Cleaning process: Initial size of tokens = 4913\n",
      "Reduction due to punctuations and stopwords = 3611.\n",
      "Reduction due to all numeral terms = 31\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3660\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4913\n",
      "Cleaning process: Initial size of tokens = 4913\n",
      "Reduction due to punctuations and stopwords = 3611.\n",
      "Reduction due to all numeral terms = 31\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3660\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 267\n",
      "Cleaning process: Initial size of tokens = 267\n",
      "Reduction due to punctuations and stopwords = 161.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 161\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 267\n",
      "Cleaning process: Initial size of tokens = 267\n",
      "Reduction due to punctuations and stopwords = 161.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 161\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7608\n",
      "Cleaning process: Initial size of tokens = 7608\n",
      "Reduction due to punctuations and stopwords = 5940.\n",
      "Reduction due to all numeral terms = 11\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5959\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7608\n",
      "Cleaning process: Initial size of tokens = 7608\n",
      "Reduction due to punctuations and stopwords = 5940.\n",
      "Reduction due to all numeral terms = 11\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5959\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1059\n",
      "Cleaning process: Initial size of tokens = 1059\n",
      "Reduction due to punctuations and stopwords = 739.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 747\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1059\n",
      "Cleaning process: Initial size of tokens = 1059\n",
      "Reduction due to punctuations and stopwords = 739.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 747\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1489\n",
      "Cleaning process: Initial size of tokens = 1489\n",
      "Reduction due to punctuations and stopwords = 956.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 962\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1489\n",
      "Cleaning process: Initial size of tokens = 1489\n",
      "Reduction due to punctuations and stopwords = 956.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 962\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2160\n",
      "Cleaning process: Initial size of tokens = 2160\n",
      "Reduction due to punctuations and stopwords = 1449.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1453\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2160\n",
      "Cleaning process: Initial size of tokens = 2160\n",
      "Reduction due to punctuations and stopwords = 1449.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1453\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8244\n",
      "Cleaning process: Initial size of tokens = 8244\n",
      "Reduction due to punctuations and stopwords = 6580.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6601\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8244\n",
      "Cleaning process: Initial size of tokens = 8244\n",
      "Reduction due to punctuations and stopwords = 6580.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6601\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1081\n",
      "Cleaning process: Initial size of tokens = 1081\n",
      "Reduction due to punctuations and stopwords = 631.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 633\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1081\n",
      "Cleaning process: Initial size of tokens = 1081\n",
      "Reduction due to punctuations and stopwords = 631.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 633\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8643\n",
      "Cleaning process: Initial size of tokens = 8643\n",
      "Reduction due to punctuations and stopwords = 6351.\n",
      "Reduction due to all numeral terms = 72\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6447\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8643\n",
      "Cleaning process: Initial size of tokens = 8643\n",
      "Reduction due to punctuations and stopwords = 6351.\n",
      "Reduction due to all numeral terms = 72\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6447\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 174\n",
      "Cleaning process: Initial size of tokens = 174\n",
      "Reduction due to punctuations and stopwords = 109.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 109\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 174\n",
      "Cleaning process: Initial size of tokens = 174\n",
      "Reduction due to punctuations and stopwords = 109.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 109\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6167\n",
      "Cleaning process: Initial size of tokens = 6167\n",
      "Reduction due to punctuations and stopwords = 4796.\n",
      "Reduction due to all numeral terms = 28\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4838\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6167\n",
      "Cleaning process: Initial size of tokens = 6167\n",
      "Reduction due to punctuations and stopwords = 4796.\n",
      "Reduction due to all numeral terms = 28\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 10\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4838\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 493\n",
      "Cleaning process: Initial size of tokens = 493\n",
      "Reduction due to punctuations and stopwords = 327.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 327\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 493\n",
      "Cleaning process: Initial size of tokens = 493\n",
      "Reduction due to punctuations and stopwords = 327.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 327\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4693\n",
      "Cleaning process: Initial size of tokens = 4693\n",
      "Reduction due to punctuations and stopwords = 3320.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3342\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4693\n",
      "Cleaning process: Initial size of tokens = 4693\n",
      "Reduction due to punctuations and stopwords = 3320.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3342\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1360\n",
      "Cleaning process: Initial size of tokens = 1360\n",
      "Reduction due to punctuations and stopwords = 844.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 849\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1360\n",
      "Cleaning process: Initial size of tokens = 1360\n",
      "Reduction due to punctuations and stopwords = 844.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 849\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1100\n",
      "Cleaning process: Initial size of tokens = 1100\n",
      "Reduction due to punctuations and stopwords = 777.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 777\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1100\n",
      "Cleaning process: Initial size of tokens = 1100\n",
      "Reduction due to punctuations and stopwords = 777.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 777\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 264\n",
      "Cleaning process: Initial size of tokens = 264\n",
      "Reduction due to punctuations and stopwords = 189.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 189\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 264\n",
      "Cleaning process: Initial size of tokens = 264\n",
      "Reduction due to punctuations and stopwords = 189.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 189\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1258\n",
      "Cleaning process: Initial size of tokens = 1258\n",
      "Reduction due to punctuations and stopwords = 869.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 870\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1258\n",
      "Cleaning process: Initial size of tokens = 1258\n",
      "Reduction due to punctuations and stopwords = 869.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 870\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4008\n",
      "Cleaning process: Initial size of tokens = 4008\n",
      "Reduction due to punctuations and stopwords = 3213.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3219\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4008\n",
      "Cleaning process: Initial size of tokens = 4008\n",
      "Reduction due to punctuations and stopwords = 3213.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3219\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 827\n",
      "Cleaning process: Initial size of tokens = 827\n",
      "Reduction due to punctuations and stopwords = 576.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 578\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 827\n",
      "Cleaning process: Initial size of tokens = 827\n",
      "Reduction due to punctuations and stopwords = 576.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 578\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 115\n",
      "Cleaning process: Initial size of tokens = 115\n",
      "Reduction due to punctuations and stopwords = 59.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 59\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 115\n",
      "Cleaning process: Initial size of tokens = 115\n",
      "Reduction due to punctuations and stopwords = 59.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 59\n",
      "Percentage = 51%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2714\n",
      "Cleaning process: Initial size of tokens = 2714\n",
      "Reduction due to punctuations and stopwords = 1976.\n",
      "Reduction due to all numeral terms = 29\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2022\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2714\n",
      "Cleaning process: Initial size of tokens = 2714\n",
      "Reduction due to punctuations and stopwords = 1976.\n",
      "Reduction due to all numeral terms = 29\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2022\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 581\n",
      "Cleaning process: Initial size of tokens = 581\n",
      "Reduction due to punctuations and stopwords = 394.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 397\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 581\n",
      "Cleaning process: Initial size of tokens = 581\n",
      "Reduction due to punctuations and stopwords = 394.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 397\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4386\n",
      "Cleaning process: Initial size of tokens = 4386\n",
      "Reduction due to punctuations and stopwords = 3305.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3321\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4386\n",
      "Cleaning process: Initial size of tokens = 4386\n",
      "Reduction due to punctuations and stopwords = 3305.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3321\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 117\n",
      "Cleaning process: Initial size of tokens = 117\n",
      "Reduction due to punctuations and stopwords = 69.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 70\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 117\n",
      "Cleaning process: Initial size of tokens = 117\n",
      "Reduction due to punctuations and stopwords = 69.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 70\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6035\n",
      "Cleaning process: Initial size of tokens = 6035\n",
      "Reduction due to punctuations and stopwords = 4658.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4666\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6035\n",
      "Cleaning process: Initial size of tokens = 6035\n",
      "Reduction due to punctuations and stopwords = 4658.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4666\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2976\n",
      "Cleaning process: Initial size of tokens = 2976\n",
      "Reduction due to punctuations and stopwords = 2219.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2231\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2976\n",
      "Cleaning process: Initial size of tokens = 2976\n",
      "Reduction due to punctuations and stopwords = 2219.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2231\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4997\n",
      "Cleaning process: Initial size of tokens = 4997\n",
      "Reduction due to punctuations and stopwords = 3621.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3634\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4997\n",
      "Cleaning process: Initial size of tokens = 4997\n",
      "Reduction due to punctuations and stopwords = 3621.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3634\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2785\n",
      "Cleaning process: Initial size of tokens = 2785\n",
      "Reduction due to punctuations and stopwords = 2204.\n",
      "Reduction due to all numeral terms = 26\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2239\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2785\n",
      "Cleaning process: Initial size of tokens = 2785\n",
      "Reduction due to punctuations and stopwords = 2204.\n",
      "Reduction due to all numeral terms = 26\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2239\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2401\n",
      "Cleaning process: Initial size of tokens = 2401\n",
      "Reduction due to punctuations and stopwords = 1712.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1718\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2401\n",
      "Cleaning process: Initial size of tokens = 2401\n",
      "Reduction due to punctuations and stopwords = 1712.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1718\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 189\n",
      "Cleaning process: Initial size of tokens = 189\n",
      "Reduction due to punctuations and stopwords = 112.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 112\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 189\n",
      "Cleaning process: Initial size of tokens = 189\n",
      "Reduction due to punctuations and stopwords = 112.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 112\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3544\n",
      "Cleaning process: Initial size of tokens = 3544\n",
      "Reduction due to punctuations and stopwords = 2532.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2553\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3544\n",
      "Cleaning process: Initial size of tokens = 3544\n",
      "Reduction due to punctuations and stopwords = 2532.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2553\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2412\n",
      "Cleaning process: Initial size of tokens = 2412\n",
      "Reduction due to punctuations and stopwords = 1569.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1586\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2412\n",
      "Cleaning process: Initial size of tokens = 2412\n",
      "Reduction due to punctuations and stopwords = 1569.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1586\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1442\n",
      "Cleaning process: Initial size of tokens = 1442\n",
      "Reduction due to punctuations and stopwords = 980.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 984\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1442\n",
      "Cleaning process: Initial size of tokens = 1442\n",
      "Reduction due to punctuations and stopwords = 980.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 984\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11282\n",
      "Cleaning process: Initial size of tokens = 11282\n",
      "Reduction due to punctuations and stopwords = 8929.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 29\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8995\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11282\n",
      "Cleaning process: Initial size of tokens = 11282\n",
      "Reduction due to punctuations and stopwords = 8929.\n",
      "Reduction due to all numeral terms = 27\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 29\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8995\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1345\n",
      "Cleaning process: Initial size of tokens = 1345\n",
      "Reduction due to punctuations and stopwords = 871.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 872\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1345\n",
      "Cleaning process: Initial size of tokens = 1345\n",
      "Reduction due to punctuations and stopwords = 871.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 872\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1384\n",
      "Cleaning process: Initial size of tokens = 1384\n",
      "Reduction due to punctuations and stopwords = 925.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 930\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1384\n",
      "Cleaning process: Initial size of tokens = 1384\n",
      "Reduction due to punctuations and stopwords = 925.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 930\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3831\n",
      "Cleaning process: Initial size of tokens = 3831\n",
      "Reduction due to punctuations and stopwords = 2460.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2486\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3831\n",
      "Cleaning process: Initial size of tokens = 3831\n",
      "Reduction due to punctuations and stopwords = 2460.\n",
      "Reduction due to all numeral terms = 13\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2486\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 995\n",
      "Cleaning process: Initial size of tokens = 995\n",
      "Reduction due to punctuations and stopwords = 571.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 574\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 995\n",
      "Cleaning process: Initial size of tokens = 995\n",
      "Reduction due to punctuations and stopwords = 571.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 574\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6190\n",
      "Cleaning process: Initial size of tokens = 6190\n",
      "Reduction due to punctuations and stopwords = 4560.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4569\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6190\n",
      "Cleaning process: Initial size of tokens = 6190\n",
      "Reduction due to punctuations and stopwords = 4560.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4569\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2039\n",
      "Cleaning process: Initial size of tokens = 2039\n",
      "Reduction due to punctuations and stopwords = 1304.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1315\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2039\n",
      "Cleaning process: Initial size of tokens = 2039\n",
      "Reduction due to punctuations and stopwords = 1304.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1315\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4312\n",
      "Cleaning process: Initial size of tokens = 4312\n",
      "Reduction due to punctuations and stopwords = 3468.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3481\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4312\n",
      "Cleaning process: Initial size of tokens = 4312\n",
      "Reduction due to punctuations and stopwords = 3468.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3481\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 227\n",
      "Cleaning process: Initial size of tokens = 227\n",
      "Reduction due to punctuations and stopwords = 130.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 130\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 227\n",
      "Cleaning process: Initial size of tokens = 227\n",
      "Reduction due to punctuations and stopwords = 130.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 130\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 691\n",
      "Cleaning process: Initial size of tokens = 691\n",
      "Reduction due to punctuations and stopwords = 361.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 367\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 691\n",
      "Cleaning process: Initial size of tokens = 691\n",
      "Reduction due to punctuations and stopwords = 361.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 367\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4277\n",
      "Cleaning process: Initial size of tokens = 4277\n",
      "Reduction due to punctuations and stopwords = 3041.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3048\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4277\n",
      "Cleaning process: Initial size of tokens = 4277\n",
      "Reduction due to punctuations and stopwords = 3041.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3048\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1311\n",
      "Cleaning process: Initial size of tokens = 1311\n",
      "Reduction due to punctuations and stopwords = 898.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 902\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1311\n",
      "Cleaning process: Initial size of tokens = 1311\n",
      "Reduction due to punctuations and stopwords = 898.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 902\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 167\n",
      "Cleaning process: Initial size of tokens = 167\n",
      "Reduction due to punctuations and stopwords = 88.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 89\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 167\n",
      "Cleaning process: Initial size of tokens = 167\n",
      "Reduction due to punctuations and stopwords = 88.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 89\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5661\n",
      "Cleaning process: Initial size of tokens = 5661\n",
      "Reduction due to punctuations and stopwords = 4100.\n",
      "Reduction due to all numeral terms = 33\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4153\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5661\n",
      "Cleaning process: Initial size of tokens = 5661\n",
      "Reduction due to punctuations and stopwords = 4100.\n",
      "Reduction due to all numeral terms = 33\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4153\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2041\n",
      "Cleaning process: Initial size of tokens = 2041\n",
      "Reduction due to punctuations and stopwords = 1111.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1118\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2041\n",
      "Cleaning process: Initial size of tokens = 2041\n",
      "Reduction due to punctuations and stopwords = 1111.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1118\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1181\n",
      "Cleaning process: Initial size of tokens = 1181\n",
      "Reduction due to punctuations and stopwords = 776.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 780\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1181\n",
      "Cleaning process: Initial size of tokens = 1181\n",
      "Reduction due to punctuations and stopwords = 776.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 780\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3260\n",
      "Cleaning process: Initial size of tokens = 3260\n",
      "Reduction due to punctuations and stopwords = 2369.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2375\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3260\n",
      "Cleaning process: Initial size of tokens = 3260\n",
      "Reduction due to punctuations and stopwords = 2369.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2375\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1317\n",
      "Cleaning process: Initial size of tokens = 1317\n",
      "Reduction due to punctuations and stopwords = 875.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 877\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1317\n",
      "Cleaning process: Initial size of tokens = 1317\n",
      "Reduction due to punctuations and stopwords = 875.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 877\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 375\n",
      "Cleaning process: Initial size of tokens = 375\n",
      "Reduction due to punctuations and stopwords = 223.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 224\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 375\n",
      "Cleaning process: Initial size of tokens = 375\n",
      "Reduction due to punctuations and stopwords = 223.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 224\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1772\n",
      "Cleaning process: Initial size of tokens = 1772\n",
      "Reduction due to punctuations and stopwords = 1213.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1229\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1772\n",
      "Cleaning process: Initial size of tokens = 1772\n",
      "Reduction due to punctuations and stopwords = 1213.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1229\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3085\n",
      "Cleaning process: Initial size of tokens = 3085\n",
      "Reduction due to punctuations and stopwords = 2340.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2348\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3085\n",
      "Cleaning process: Initial size of tokens = 3085\n",
      "Reduction due to punctuations and stopwords = 2340.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2348\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3736\n",
      "Cleaning process: Initial size of tokens = 3736\n",
      "Reduction due to punctuations and stopwords = 2477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to all numeral terms = 12\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2506\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3736\n",
      "Cleaning process: Initial size of tokens = 3736\n",
      "Reduction due to punctuations and stopwords = 2477.\n",
      "Reduction due to all numeral terms = 12\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2506\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4598\n",
      "Cleaning process: Initial size of tokens = 4598\n",
      "Reduction due to punctuations and stopwords = 3233.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3245\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4598\n",
      "Cleaning process: Initial size of tokens = 4598\n",
      "Reduction due to punctuations and stopwords = 3233.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3245\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1708\n",
      "Cleaning process: Initial size of tokens = 1708\n",
      "Reduction due to punctuations and stopwords = 1215.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1216\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1708\n",
      "Cleaning process: Initial size of tokens = 1708\n",
      "Reduction due to punctuations and stopwords = 1215.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1216\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 477\n",
      "Cleaning process: Initial size of tokens = 477\n",
      "Reduction due to punctuations and stopwords = 286.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 287\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 477\n",
      "Cleaning process: Initial size of tokens = 477\n",
      "Reduction due to punctuations and stopwords = 286.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 287\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4378\n",
      "Cleaning process: Initial size of tokens = 4378\n",
      "Reduction due to punctuations and stopwords = 3436.\n",
      "Reduction due to all numeral terms = 26\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 39\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3503\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4378\n",
      "Cleaning process: Initial size of tokens = 4378\n",
      "Reduction due to punctuations and stopwords = 3436.\n",
      "Reduction due to all numeral terms = 26\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 39\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3503\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2596\n",
      "Cleaning process: Initial size of tokens = 2596\n",
      "Reduction due to punctuations and stopwords = 2028.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2032\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2596\n",
      "Cleaning process: Initial size of tokens = 2596\n",
      "Reduction due to punctuations and stopwords = 2028.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2032\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3083\n",
      "Cleaning process: Initial size of tokens = 3083\n",
      "Reduction due to punctuations and stopwords = 2320.\n",
      "Reduction due to all numeral terms = 33\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 147\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2501\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3083\n",
      "Cleaning process: Initial size of tokens = 3083\n",
      "Reduction due to punctuations and stopwords = 2320.\n",
      "Reduction due to all numeral terms = 33\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 147\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2501\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 996\n",
      "Cleaning process: Initial size of tokens = 996\n",
      "Reduction due to punctuations and stopwords = 679.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 682\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 996\n",
      "Cleaning process: Initial size of tokens = 996\n",
      "Reduction due to punctuations and stopwords = 679.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 682\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1174\n",
      "Cleaning process: Initial size of tokens = 1174\n",
      "Reduction due to punctuations and stopwords = 795.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 807\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1174\n",
      "Cleaning process: Initial size of tokens = 1174\n",
      "Reduction due to punctuations and stopwords = 795.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 807\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2366\n",
      "Cleaning process: Initial size of tokens = 2366\n",
      "Reduction due to punctuations and stopwords = 1899.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1909\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2366\n",
      "Cleaning process: Initial size of tokens = 2366\n",
      "Reduction due to punctuations and stopwords = 1899.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1909\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5657\n",
      "Cleaning process: Initial size of tokens = 5657\n",
      "Reduction due to punctuations and stopwords = 4134.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4155\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5657\n",
      "Cleaning process: Initial size of tokens = 5657\n",
      "Reduction due to punctuations and stopwords = 4134.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4155\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 692\n",
      "Cleaning process: Initial size of tokens = 692\n",
      "Reduction due to punctuations and stopwords = 440.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 442\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 692\n",
      "Cleaning process: Initial size of tokens = 692\n",
      "Reduction due to punctuations and stopwords = 440.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 442\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1097\n",
      "Cleaning process: Initial size of tokens = 1097\n",
      "Reduction due to punctuations and stopwords = 721.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 721\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1097\n",
      "Cleaning process: Initial size of tokens = 1097\n",
      "Reduction due to punctuations and stopwords = 721.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 721\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1054\n",
      "Cleaning process: Initial size of tokens = 1054\n",
      "Reduction due to punctuations and stopwords = 724.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 724\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1054\n",
      "Cleaning process: Initial size of tokens = 1054\n",
      "Reduction due to punctuations and stopwords = 724.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 724\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1341\n",
      "Cleaning process: Initial size of tokens = 1341\n",
      "Reduction due to punctuations and stopwords = 913.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 916\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1341\n",
      "Cleaning process: Initial size of tokens = 1341\n",
      "Reduction due to punctuations and stopwords = 913.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 916\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2138\n",
      "Cleaning process: Initial size of tokens = 2138\n",
      "Reduction due to punctuations and stopwords = 1582.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1583\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2138\n",
      "Cleaning process: Initial size of tokens = 2138\n",
      "Reduction due to punctuations and stopwords = 1582.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1583\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 715\n",
      "Cleaning process: Initial size of tokens = 715\n",
      "Reduction due to punctuations and stopwords = 406.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 409\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 715\n",
      "Cleaning process: Initial size of tokens = 715\n",
      "Reduction due to punctuations and stopwords = 406.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 409\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Number of terms: 15211\n",
      "Cleaning process: Initial size of tokens = 15211\n",
      "Reduction due to punctuations and stopwords = 12011.\n",
      "Reduction due to all numeral terms = 122\n",
      "Reduction due to short terms = 17\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 30\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 12180\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 15211\n",
      "Cleaning process: Initial size of tokens = 15211\n",
      "Reduction due to punctuations and stopwords = 12011.\n",
      "Reduction due to all numeral terms = 122\n",
      "Reduction due to short terms = 17\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 30\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 12180\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3115\n",
      "Cleaning process: Initial size of tokens = 3115\n",
      "Reduction due to punctuations and stopwords = 2277.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2281\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3115\n",
      "Cleaning process: Initial size of tokens = 3115\n",
      "Reduction due to punctuations and stopwords = 2277.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2281\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3034\n",
      "Cleaning process: Initial size of tokens = 3034\n",
      "Reduction due to punctuations and stopwords = 2047.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2058\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3034\n",
      "Cleaning process: Initial size of tokens = 3034\n",
      "Reduction due to punctuations and stopwords = 2047.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2058\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2961\n",
      "Cleaning process: Initial size of tokens = 2961\n",
      "Reduction due to punctuations and stopwords = 1997.\n",
      "Reduction due to all numeral terms = 30\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 33\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2067\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2961\n",
      "Cleaning process: Initial size of tokens = 2961\n",
      "Reduction due to punctuations and stopwords = 1997.\n",
      "Reduction due to all numeral terms = 30\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 33\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2067\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 622\n",
      "Cleaning process: Initial size of tokens = 622\n",
      "Reduction due to punctuations and stopwords = 375.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 381\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 622\n",
      "Cleaning process: Initial size of tokens = 622\n",
      "Reduction due to punctuations and stopwords = 375.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 381\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2298\n",
      "Cleaning process: Initial size of tokens = 2298\n",
      "Reduction due to punctuations and stopwords = 1437.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1443\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2298\n",
      "Cleaning process: Initial size of tokens = 2298\n",
      "Reduction due to punctuations and stopwords = 1437.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1443\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5728\n",
      "Cleaning process: Initial size of tokens = 5728\n",
      "Reduction due to punctuations and stopwords = 4081.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4101\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5728\n",
      "Cleaning process: Initial size of tokens = 5728\n",
      "Reduction due to punctuations and stopwords = 4081.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4101\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5458\n",
      "Cleaning process: Initial size of tokens = 5458\n",
      "Reduction due to punctuations and stopwords = 3962.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3972\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5458\n",
      "Cleaning process: Initial size of tokens = 5458\n",
      "Reduction due to punctuations and stopwords = 3962.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3972\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1148\n",
      "Cleaning process: Initial size of tokens = 1148\n",
      "Reduction due to punctuations and stopwords = 752.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 755\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1148\n",
      "Cleaning process: Initial size of tokens = 1148\n",
      "Reduction due to punctuations and stopwords = 752.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 755\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3048\n",
      "Cleaning process: Initial size of tokens = 3048\n",
      "Reduction due to punctuations and stopwords = 2165.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2168\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3048\n",
      "Cleaning process: Initial size of tokens = 3048\n",
      "Reduction due to punctuations and stopwords = 2165.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2168\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3405\n",
      "Cleaning process: Initial size of tokens = 3405\n",
      "Reduction due to punctuations and stopwords = 2526.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2533\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3405\n",
      "Cleaning process: Initial size of tokens = 3405\n",
      "Reduction due to punctuations and stopwords = 2526.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2533\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 209\n",
      "Cleaning process: Initial size of tokens = 209\n",
      "Reduction due to punctuations and stopwords = 112.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 112\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 209\n",
      "Cleaning process: Initial size of tokens = 209\n",
      "Reduction due to punctuations and stopwords = 112.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 112\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 312\n",
      "Cleaning process: Initial size of tokens = 312\n",
      "Reduction due to punctuations and stopwords = 192.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 192\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 312\n",
      "Cleaning process: Initial size of tokens = 312\n",
      "Reduction due to punctuations and stopwords = 192.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 192\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3195\n",
      "Cleaning process: Initial size of tokens = 3195\n",
      "Reduction due to punctuations and stopwords = 2426.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2434\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3195\n",
      "Cleaning process: Initial size of tokens = 3195\n",
      "Reduction due to punctuations and stopwords = 2426.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2434\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2562\n",
      "Cleaning process: Initial size of tokens = 2562\n",
      "Reduction due to punctuations and stopwords = 1967.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1972\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2562\n",
      "Cleaning process: Initial size of tokens = 2562\n",
      "Reduction due to punctuations and stopwords = 1967.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1972\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4918\n",
      "Cleaning process: Initial size of tokens = 4918\n",
      "Reduction due to punctuations and stopwords = 3419.\n",
      "Reduction due to all numeral terms = 72\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3506\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4918\n",
      "Cleaning process: Initial size of tokens = 4918\n",
      "Reduction due to punctuations and stopwords = 3419.\n",
      "Reduction due to all numeral terms = 72\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3506\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1373\n",
      "Cleaning process: Initial size of tokens = 1373\n",
      "Reduction due to punctuations and stopwords = 1006.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1013\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1373\n",
      "Cleaning process: Initial size of tokens = 1373\n",
      "Reduction due to punctuations and stopwords = 1006.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1013\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4034\n",
      "Cleaning process: Initial size of tokens = 4034\n",
      "Reduction due to punctuations and stopwords = 2959.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2971\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4034\n",
      "Cleaning process: Initial size of tokens = 4034\n",
      "Reduction due to punctuations and stopwords = 2959.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2971\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5883\n",
      "Cleaning process: Initial size of tokens = 5883\n",
      "Reduction due to punctuations and stopwords = 4445.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4451\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5883\n",
      "Cleaning process: Initial size of tokens = 5883\n",
      "Reduction due to punctuations and stopwords = 4445.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4451\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 9017\n",
      "Cleaning process: Initial size of tokens = 9017\n",
      "Reduction due to punctuations and stopwords = 6819.\n",
      "Reduction due to all numeral terms = 69\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6910\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 9017\n",
      "Cleaning process: Initial size of tokens = 9017\n",
      "Reduction due to punctuations and stopwords = 6819.\n",
      "Reduction due to all numeral terms = 69\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6910\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11457\n",
      "Cleaning process: Initial size of tokens = 11457\n",
      "Reduction due to punctuations and stopwords = 9014.\n",
      "Reduction due to all numeral terms = 49\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 16\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9094\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 11457\n",
      "Cleaning process: Initial size of tokens = 11457\n",
      "Reduction due to punctuations and stopwords = 9014.\n",
      "Reduction due to all numeral terms = 49\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 16\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 9094\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1386\n",
      "Cleaning process: Initial size of tokens = 1386\n",
      "Reduction due to punctuations and stopwords = 931.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 941\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1386\n",
      "Cleaning process: Initial size of tokens = 1386\n",
      "Reduction due to punctuations and stopwords = 931.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 941\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1037\n",
      "Cleaning process: Initial size of tokens = 1037\n",
      "Reduction due to punctuations and stopwords = 645.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 647\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1037\n",
      "Cleaning process: Initial size of tokens = 1037\n",
      "Reduction due to punctuations and stopwords = 645.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 647\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1726\n",
      "Cleaning process: Initial size of tokens = 1726\n",
      "Reduction due to punctuations and stopwords = 1191.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1199\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1726\n",
      "Cleaning process: Initial size of tokens = 1726\n",
      "Reduction due to punctuations and stopwords = 1191.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1199\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4778\n",
      "Cleaning process: Initial size of tokens = 4778\n",
      "Reduction due to punctuations and stopwords = 3371.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3377\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4778\n",
      "Cleaning process: Initial size of tokens = 4778\n",
      "Reduction due to punctuations and stopwords = 3371.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3377\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6490\n",
      "Cleaning process: Initial size of tokens = 6490\n",
      "Reduction due to punctuations and stopwords = 5310.\n",
      "Reduction due to all numeral terms = 51\n",
      "Reduction due to short terms = 18\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 24\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5403\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6490\n",
      "Cleaning process: Initial size of tokens = 6490\n",
      "Reduction due to punctuations and stopwords = 5310.\n",
      "Reduction due to all numeral terms = 51\n",
      "Reduction due to short terms = 18\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 24\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5403\n",
      "Percentage = 83%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3107\n",
      "Cleaning process: Initial size of tokens = 3107\n",
      "Reduction due to punctuations and stopwords = 2227.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2234\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3107\n",
      "Cleaning process: Initial size of tokens = 3107\n",
      "Reduction due to punctuations and stopwords = 2227.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2234\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 319\n",
      "Cleaning process: Initial size of tokens = 319\n",
      "Reduction due to punctuations and stopwords = 168.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 170\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 319\n",
      "Cleaning process: Initial size of tokens = 319\n",
      "Reduction due to punctuations and stopwords = 168.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 170\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10105\n",
      "Cleaning process: Initial size of tokens = 10105\n",
      "Reduction due to punctuations and stopwords = 7797.\n",
      "Reduction due to all numeral terms = 47\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 12\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7871\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10105\n",
      "Cleaning process: Initial size of tokens = 10105\n",
      "Reduction due to punctuations and stopwords = 7797.\n",
      "Reduction due to all numeral terms = 47\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 12\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 7871\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4796\n",
      "Cleaning process: Initial size of tokens = 4796\n",
      "Reduction due to punctuations and stopwords = 3622.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3631\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4796\n",
      "Cleaning process: Initial size of tokens = 4796\n",
      "Reduction due to punctuations and stopwords = 3622.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3631\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8017\n",
      "Cleaning process: Initial size of tokens = 8017\n",
      "Reduction due to punctuations and stopwords = 5850.\n",
      "Reduction due to all numeral terms = 82\n",
      "Reduction due to short terms = 16\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 26\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5974\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8017\n",
      "Cleaning process: Initial size of tokens = 8017\n",
      "Reduction due to punctuations and stopwords = 5850.\n",
      "Reduction due to all numeral terms = 82\n",
      "Reduction due to short terms = 16\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 26\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5974\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2822\n",
      "Cleaning process: Initial size of tokens = 2822\n",
      "Reduction due to punctuations and stopwords = 1990.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1997\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2822\n",
      "Cleaning process: Initial size of tokens = 2822\n",
      "Reduction due to punctuations and stopwords = 1990.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1997\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5812\n",
      "Cleaning process: Initial size of tokens = 5812\n",
      "Reduction due to punctuations and stopwords = 4367.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4372\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5812\n",
      "Cleaning process: Initial size of tokens = 5812\n",
      "Reduction due to punctuations and stopwords = 4367.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4372\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1975\n",
      "Cleaning process: Initial size of tokens = 1975\n",
      "Reduction due to punctuations and stopwords = 1264.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1287\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1975\n",
      "Cleaning process: Initial size of tokens = 1975\n",
      "Reduction due to punctuations and stopwords = 1264.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1287\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 273\n",
      "Cleaning process: Initial size of tokens = 273\n",
      "Reduction due to punctuations and stopwords = 137.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 141\n",
      "Percentage = 52%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 273\n",
      "Cleaning process: Initial size of tokens = 273\n",
      "Reduction due to punctuations and stopwords = 137.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 141\n",
      "Percentage = 52%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8979\n",
      "Cleaning process: Initial size of tokens = 8979\n",
      "Reduction due to punctuations and stopwords = 6744.\n",
      "Reduction due to all numeral terms = 130\n",
      "Reduction due to short terms = 16\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6896\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 8979\n",
      "Cleaning process: Initial size of tokens = 8979\n",
      "Reduction due to punctuations and stopwords = 6744.\n",
      "Reduction due to all numeral terms = 130\n",
      "Reduction due to short terms = 16\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6896\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 487\n",
      "Cleaning process: Initial size of tokens = 487\n",
      "Reduction due to punctuations and stopwords = 326.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 326\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 487\n",
      "Cleaning process: Initial size of tokens = 487\n",
      "Reduction due to punctuations and stopwords = 326.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 326\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1502\n",
      "Cleaning process: Initial size of tokens = 1502\n",
      "Reduction due to punctuations and stopwords = 1042.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1047\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1502\n",
      "Cleaning process: Initial size of tokens = 1502\n",
      "Reduction due to punctuations and stopwords = 1042.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1047\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4129\n",
      "Cleaning process: Initial size of tokens = 4129\n",
      "Reduction due to punctuations and stopwords = 2887.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2890\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4129\n",
      "Cleaning process: Initial size of tokens = 4129\n",
      "Reduction due to punctuations and stopwords = 2887.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2890\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3555\n",
      "Cleaning process: Initial size of tokens = 3555\n",
      "Reduction due to punctuations and stopwords = 2530.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2534\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3555\n",
      "Cleaning process: Initial size of tokens = 3555\n",
      "Reduction due to punctuations and stopwords = 2530.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2534\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3455\n",
      "Cleaning process: Initial size of tokens = 3455\n",
      "Reduction due to punctuations and stopwords = 2596.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2606\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3455\n",
      "Cleaning process: Initial size of tokens = 3455\n",
      "Reduction due to punctuations and stopwords = 2596.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2606\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1489\n",
      "Cleaning process: Initial size of tokens = 1489\n",
      "Reduction due to punctuations and stopwords = 1068.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1072\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1489\n",
      "Cleaning process: Initial size of tokens = 1489\n",
      "Reduction due to punctuations and stopwords = 1068.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1072\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 820\n",
      "Cleaning process: Initial size of tokens = 820\n",
      "Reduction due to punctuations and stopwords = 481.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 484\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 820\n",
      "Cleaning process: Initial size of tokens = 820\n",
      "Reduction due to punctuations and stopwords = 481.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 484\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5053\n",
      "Cleaning process: Initial size of tokens = 5053\n",
      "Reduction due to punctuations and stopwords = 3502.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3511\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5053\n",
      "Cleaning process: Initial size of tokens = 5053\n",
      "Reduction due to punctuations and stopwords = 3502.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3511\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 850\n",
      "Cleaning process: Initial size of tokens = 850\n",
      "Reduction due to punctuations and stopwords = 602.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 604\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 850\n",
      "Cleaning process: Initial size of tokens = 850\n",
      "Reduction due to punctuations and stopwords = 602.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 604\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 756\n",
      "Cleaning process: Initial size of tokens = 756\n",
      "Reduction due to punctuations and stopwords = 483.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 484\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 756\n",
      "Cleaning process: Initial size of tokens = 756\n",
      "Reduction due to punctuations and stopwords = 483.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 484\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1823\n",
      "Cleaning process: Initial size of tokens = 1823\n",
      "Reduction due to punctuations and stopwords = 1261.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1265\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1823\n",
      "Cleaning process: Initial size of tokens = 1823\n",
      "Reduction due to punctuations and stopwords = 1261.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1265\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 107\n",
      "Cleaning process: Initial size of tokens = 107\n",
      "Reduction due to punctuations and stopwords = 64.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 64\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 107\n",
      "Cleaning process: Initial size of tokens = 107\n",
      "Reduction due to punctuations and stopwords = 64.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 64\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6437\n",
      "Cleaning process: Initial size of tokens = 6437\n",
      "Reduction due to punctuations and stopwords = 5202.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5223\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6437\n",
      "Cleaning process: Initial size of tokens = 6437\n",
      "Reduction due to punctuations and stopwords = 5202.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 10\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5223\n",
      "Percentage = 81%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2214\n",
      "Cleaning process: Initial size of tokens = 2214\n",
      "Reduction due to punctuations and stopwords = 1444.\n",
      "Reduction due to all numeral terms = 31\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1485\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2214\n",
      "Cleaning process: Initial size of tokens = 2214\n",
      "Reduction due to punctuations and stopwords = 1444.\n",
      "Reduction due to all numeral terms = 31\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1485\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 477\n",
      "Cleaning process: Initial size of tokens = 477\n",
      "Reduction due to punctuations and stopwords = 281.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 282\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 477\n",
      "Cleaning process: Initial size of tokens = 477\n",
      "Reduction due to punctuations and stopwords = 281.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 282\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6836\n",
      "Cleaning process: Initial size of tokens = 6836\n",
      "Reduction due to punctuations and stopwords = 5336.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5353\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6836\n",
      "Cleaning process: Initial size of tokens = 6836\n",
      "Reduction due to punctuations and stopwords = 5336.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 15\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5353\n",
      "Percentage = 78%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3457\n",
      "Cleaning process: Initial size of tokens = 3457\n",
      "Reduction due to punctuations and stopwords = 2500.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2520\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3457\n",
      "Cleaning process: Initial size of tokens = 3457\n",
      "Reduction due to punctuations and stopwords = 2500.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2520\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3776\n",
      "Cleaning process: Initial size of tokens = 3776\n",
      "Reduction due to punctuations and stopwords = 2616.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2624\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3776\n",
      "Cleaning process: Initial size of tokens = 3776\n",
      "Reduction due to punctuations and stopwords = 2616.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2624\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1942\n",
      "Cleaning process: Initial size of tokens = 1942\n",
      "Reduction due to punctuations and stopwords = 1279.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1290\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1942\n",
      "Cleaning process: Initial size of tokens = 1942\n",
      "Reduction due to punctuations and stopwords = 1279.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1290\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 741\n",
      "Cleaning process: Initial size of tokens = 741\n",
      "Reduction due to punctuations and stopwords = 475.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 476\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 741\n",
      "Cleaning process: Initial size of tokens = 741\n",
      "Reduction due to punctuations and stopwords = 475.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 476\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1609\n",
      "Cleaning process: Initial size of tokens = 1609\n",
      "Reduction due to punctuations and stopwords = 1093.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1093\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1609\n",
      "Cleaning process: Initial size of tokens = 1609\n",
      "Reduction due to punctuations and stopwords = 1093.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1093\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 562\n",
      "Cleaning process: Initial size of tokens = 562\n",
      "Reduction due to punctuations and stopwords = 361.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 362\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 562\n",
      "Cleaning process: Initial size of tokens = 562\n",
      "Reduction due to punctuations and stopwords = 361.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 362\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3951\n",
      "Cleaning process: Initial size of tokens = 3951\n",
      "Reduction due to punctuations and stopwords = 2799.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2814\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3951\n",
      "Cleaning process: Initial size of tokens = 3951\n",
      "Reduction due to punctuations and stopwords = 2799.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2814\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3247\n",
      "Cleaning process: Initial size of tokens = 3247\n",
      "Reduction due to punctuations and stopwords = 2706.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2716\n",
      "Percentage = 84%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3247\n",
      "Cleaning process: Initial size of tokens = 3247\n",
      "Reduction due to punctuations and stopwords = 2706.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2716\n",
      "Percentage = 84%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 285\n",
      "Cleaning process: Initial size of tokens = 285\n",
      "Reduction due to punctuations and stopwords = 154.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 154\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 285\n",
      "Cleaning process: Initial size of tokens = 285\n",
      "Reduction due to punctuations and stopwords = 154.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 154\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7456\n",
      "Cleaning process: Initial size of tokens = 7456\n",
      "Reduction due to punctuations and stopwords = 5924.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5932\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7456\n",
      "Cleaning process: Initial size of tokens = 7456\n",
      "Reduction due to punctuations and stopwords = 5924.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5932\n",
      "Percentage = 80%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 592\n",
      "Cleaning process: Initial size of tokens = 592\n",
      "Reduction due to punctuations and stopwords = 435.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 436\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 592\n",
      "Cleaning process: Initial size of tokens = 592\n",
      "Reduction due to punctuations and stopwords = 435.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 436\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1866\n",
      "Cleaning process: Initial size of tokens = 1866\n",
      "Reduction due to punctuations and stopwords = 1282.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1287\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1866\n",
      "Cleaning process: Initial size of tokens = 1866\n",
      "Reduction due to punctuations and stopwords = 1282.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1287\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1335\n",
      "Cleaning process: Initial size of tokens = 1335\n",
      "Reduction due to punctuations and stopwords = 910.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 933\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1335\n",
      "Cleaning process: Initial size of tokens = 1335\n",
      "Reduction due to punctuations and stopwords = 910.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 5\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 933\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2687\n",
      "Cleaning process: Initial size of tokens = 2687\n",
      "Reduction due to punctuations and stopwords = 1880.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1889\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2687\n",
      "Cleaning process: Initial size of tokens = 2687\n",
      "Reduction due to punctuations and stopwords = 1880.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1889\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4223\n",
      "Cleaning process: Initial size of tokens = 4223\n",
      "Reduction due to punctuations and stopwords = 3034.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3047\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4223\n",
      "Cleaning process: Initial size of tokens = 4223\n",
      "Reduction due to punctuations and stopwords = 3034.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3047\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1496\n",
      "Cleaning process: Initial size of tokens = 1496\n",
      "Reduction due to punctuations and stopwords = 942.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 947\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1496\n",
      "Cleaning process: Initial size of tokens = 1496\n",
      "Reduction due to punctuations and stopwords = 942.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 947\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1440\n",
      "Cleaning process: Initial size of tokens = 1440\n",
      "Reduction due to punctuations and stopwords = 1003.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1003\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1440\n",
      "Cleaning process: Initial size of tokens = 1440\n",
      "Reduction due to punctuations and stopwords = 1003.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1003\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4052\n",
      "Cleaning process: Initial size of tokens = 4052\n",
      "Reduction due to punctuations and stopwords = 2700.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2709\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4052\n",
      "Cleaning process: Initial size of tokens = 4052\n",
      "Reduction due to punctuations and stopwords = 2700.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2709\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 316\n",
      "Cleaning process: Initial size of tokens = 316\n",
      "Reduction due to punctuations and stopwords = 191.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 191\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 316\n",
      "Cleaning process: Initial size of tokens = 316\n",
      "Reduction due to punctuations and stopwords = 191.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 191\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4345\n",
      "Cleaning process: Initial size of tokens = 4345\n",
      "Reduction due to punctuations and stopwords = 3150.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3176\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4345\n",
      "Cleaning process: Initial size of tokens = 4345\n",
      "Reduction due to punctuations and stopwords = 3150.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3176\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1448\n",
      "Cleaning process: Initial size of tokens = 1448\n",
      "Reduction due to punctuations and stopwords = 932.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 934\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1448\n",
      "Cleaning process: Initial size of tokens = 1448\n",
      "Reduction due to punctuations and stopwords = 932.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1036\n",
      "Cleaning process: Initial size of tokens = 1036\n",
      "Reduction due to punctuations and stopwords = 684.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 691\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1036\n",
      "Cleaning process: Initial size of tokens = 1036\n",
      "Reduction due to punctuations and stopwords = 684.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 691\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1176\n",
      "Cleaning process: Initial size of tokens = 1176\n",
      "Reduction due to punctuations and stopwords = 725.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 730\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1176\n",
      "Cleaning process: Initial size of tokens = 1176\n",
      "Reduction due to punctuations and stopwords = 725.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 730\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 810\n",
      "Cleaning process: Initial size of tokens = 810\n",
      "Reduction due to punctuations and stopwords = 516.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 517\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 810\n",
      "Cleaning process: Initial size of tokens = 810\n",
      "Reduction due to punctuations and stopwords = 516.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 517\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4317\n",
      "Cleaning process: Initial size of tokens = 4317\n",
      "Reduction due to punctuations and stopwords = 3086.\n",
      "Reduction due to all numeral terms = 17\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3107\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4317\n",
      "Cleaning process: Initial size of tokens = 4317\n",
      "Reduction due to punctuations and stopwords = 3086.\n",
      "Reduction due to all numeral terms = 17\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3107\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3438\n",
      "Cleaning process: Initial size of tokens = 3438\n",
      "Reduction due to punctuations and stopwords = 2583.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2596\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3438\n",
      "Cleaning process: Initial size of tokens = 3438\n",
      "Reduction due to punctuations and stopwords = 2583.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2596\n",
      "Percentage = 76%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3628\n",
      "Cleaning process: Initial size of tokens = 3628\n",
      "Reduction due to punctuations and stopwords = 2775.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2778\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3628\n",
      "Cleaning process: Initial size of tokens = 3628\n",
      "Reduction due to punctuations and stopwords = 2775.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2778\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 662\n",
      "Cleaning process: Initial size of tokens = 662\n",
      "Reduction due to punctuations and stopwords = 397.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 397\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 662\n",
      "Cleaning process: Initial size of tokens = 662\n",
      "Reduction due to punctuations and stopwords = 397.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 397\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 676\n",
      "Cleaning process: Initial size of tokens = 676\n",
      "Reduction due to punctuations and stopwords = 462.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 462\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 676\n",
      "Cleaning process: Initial size of tokens = 676\n",
      "Reduction due to punctuations and stopwords = 462.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 462\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10839\n",
      "Cleaning process: Initial size of tokens = 10839\n",
      "Reduction due to punctuations and stopwords = 8551.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8576\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 10839\n",
      "Cleaning process: Initial size of tokens = 10839\n",
      "Reduction due to punctuations and stopwords = 8551.\n",
      "Reduction due to all numeral terms = 10\n",
      "Reduction due to short terms = 13\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 8576\n",
      "Percentage = 79%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1671\n",
      "Cleaning process: Initial size of tokens = 1671\n",
      "Reduction due to punctuations and stopwords = 1062.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1072\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1671\n",
      "Cleaning process: Initial size of tokens = 1671\n",
      "Reduction due to punctuations and stopwords = 1062.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1072\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4040\n",
      "Cleaning process: Initial size of tokens = 4040\n",
      "Reduction due to punctuations and stopwords = 2929.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2942\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4040\n",
      "Cleaning process: Initial size of tokens = 4040\n",
      "Reduction due to punctuations and stopwords = 2929.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2942\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4576\n",
      "Cleaning process: Initial size of tokens = 4576\n",
      "Reduction due to punctuations and stopwords = 3424.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3438\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4576\n",
      "Cleaning process: Initial size of tokens = 4576\n",
      "Reduction due to punctuations and stopwords = 3424.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3438\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1085\n",
      "Cleaning process: Initial size of tokens = 1085\n",
      "Reduction due to punctuations and stopwords = 701.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 712\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1085\n",
      "Cleaning process: Initial size of tokens = 1085\n",
      "Reduction due to punctuations and stopwords = 701.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 712\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3585\n",
      "Cleaning process: Initial size of tokens = 3585\n",
      "Reduction due to punctuations and stopwords = 2629.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2641\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3585\n",
      "Cleaning process: Initial size of tokens = 3585\n",
      "Reduction due to punctuations and stopwords = 2629.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2641\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3833\n",
      "Cleaning process: Initial size of tokens = 3833\n",
      "Reduction due to punctuations and stopwords = 2776.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2787\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3833\n",
      "Cleaning process: Initial size of tokens = 3833\n",
      "Reduction due to punctuations and stopwords = 2776.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2787\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2853\n",
      "Cleaning process: Initial size of tokens = 2853\n",
      "Reduction due to punctuations and stopwords = 2090.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2094\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2853\n",
      "Cleaning process: Initial size of tokens = 2853\n",
      "Reduction due to punctuations and stopwords = 2090.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2094\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5048\n",
      "Cleaning process: Initial size of tokens = 5048\n",
      "Reduction due to punctuations and stopwords = 3602.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3621\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Number of terms: 5048\n",
      "Cleaning process: Initial size of tokens = 5048\n",
      "Reduction due to punctuations and stopwords = 3602.\n",
      "Reduction due to all numeral terms = 16\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3621\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 212\n",
      "Cleaning process: Initial size of tokens = 212\n",
      "Reduction due to punctuations and stopwords = 113.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 113\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 212\n",
      "Cleaning process: Initial size of tokens = 212\n",
      "Reduction due to punctuations and stopwords = 113.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 113\n",
      "Percentage = 53%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1494\n",
      "Cleaning process: Initial size of tokens = 1494\n",
      "Reduction due to punctuations and stopwords = 965.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 967\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1494\n",
      "Cleaning process: Initial size of tokens = 1494\n",
      "Reduction due to punctuations and stopwords = 965.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 967\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7941\n",
      "Cleaning process: Initial size of tokens = 7941\n",
      "Reduction due to punctuations and stopwords = 6067.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6086\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7941\n",
      "Cleaning process: Initial size of tokens = 7941\n",
      "Reduction due to punctuations and stopwords = 6067.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6086\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1831\n",
      "Cleaning process: Initial size of tokens = 1831\n",
      "Reduction due to punctuations and stopwords = 1225.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1233\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1831\n",
      "Cleaning process: Initial size of tokens = 1831\n",
      "Reduction due to punctuations and stopwords = 1225.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1233\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4091\n",
      "Cleaning process: Initial size of tokens = 4091\n",
      "Reduction due to punctuations and stopwords = 2922.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2931\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4091\n",
      "Cleaning process: Initial size of tokens = 4091\n",
      "Reduction due to punctuations and stopwords = 2922.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2931\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2026\n",
      "Cleaning process: Initial size of tokens = 2026\n",
      "Reduction due to punctuations and stopwords = 1144.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1150\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2026\n",
      "Cleaning process: Initial size of tokens = 2026\n",
      "Reduction due to punctuations and stopwords = 1144.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1150\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5587\n",
      "Cleaning process: Initial size of tokens = 5587\n",
      "Reduction due to punctuations and stopwords = 3985.\n",
      "Reduction due to all numeral terms = 52\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4055\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5587\n",
      "Cleaning process: Initial size of tokens = 5587\n",
      "Reduction due to punctuations and stopwords = 3985.\n",
      "Reduction due to all numeral terms = 52\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4055\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3645\n",
      "Cleaning process: Initial size of tokens = 3645\n",
      "Reduction due to punctuations and stopwords = 2594.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2601\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3645\n",
      "Cleaning process: Initial size of tokens = 3645\n",
      "Reduction due to punctuations and stopwords = 2594.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2601\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4157\n",
      "Cleaning process: Initial size of tokens = 4157\n",
      "Reduction due to punctuations and stopwords = 2927.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2939\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4157\n",
      "Cleaning process: Initial size of tokens = 4157\n",
      "Reduction due to punctuations and stopwords = 2927.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2939\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1278\n",
      "Cleaning process: Initial size of tokens = 1278\n",
      "Reduction due to punctuations and stopwords = 803.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 803\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1278\n",
      "Cleaning process: Initial size of tokens = 1278\n",
      "Reduction due to punctuations and stopwords = 803.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 803\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 213\n",
      "Cleaning process: Initial size of tokens = 213\n",
      "Reduction due to punctuations and stopwords = 116.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 116\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 213\n",
      "Cleaning process: Initial size of tokens = 213\n",
      "Reduction due to punctuations and stopwords = 116.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 116\n",
      "Percentage = 54%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 656\n",
      "Cleaning process: Initial size of tokens = 656\n",
      "Reduction due to punctuations and stopwords = 410.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 411\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 656\n",
      "Cleaning process: Initial size of tokens = 656\n",
      "Reduction due to punctuations and stopwords = 410.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 411\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6243\n",
      "Cleaning process: Initial size of tokens = 6243\n",
      "Reduction due to punctuations and stopwords = 4516.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4523\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6243\n",
      "Cleaning process: Initial size of tokens = 6243\n",
      "Reduction due to punctuations and stopwords = 4516.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4523\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 206\n",
      "Cleaning process: Initial size of tokens = 206\n",
      "Reduction due to punctuations and stopwords = 117.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 117\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 206\n",
      "Cleaning process: Initial size of tokens = 206\n",
      "Reduction due to punctuations and stopwords = 117.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 117\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 865\n",
      "Cleaning process: Initial size of tokens = 865\n",
      "Reduction due to punctuations and stopwords = 502.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 505\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 865\n",
      "Cleaning process: Initial size of tokens = 865\n",
      "Reduction due to punctuations and stopwords = 502.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 505\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 629\n",
      "Cleaning process: Initial size of tokens = 629\n",
      "Reduction due to punctuations and stopwords = 410.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 414\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 629\n",
      "Cleaning process: Initial size of tokens = 629\n",
      "Reduction due to punctuations and stopwords = 410.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 414\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5540\n",
      "Cleaning process: Initial size of tokens = 5540\n",
      "Reduction due to punctuations and stopwords = 3834.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3851\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5540\n",
      "Cleaning process: Initial size of tokens = 5540\n",
      "Reduction due to punctuations and stopwords = 3834.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3851\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1855\n",
      "Cleaning process: Initial size of tokens = 1855\n",
      "Reduction due to punctuations and stopwords = 1209.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1220\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1855\n",
      "Cleaning process: Initial size of tokens = 1855\n",
      "Reduction due to punctuations and stopwords = 1209.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1220\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1143\n",
      "Cleaning process: Initial size of tokens = 1143\n",
      "Reduction due to punctuations and stopwords = 665.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 671\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1143\n",
      "Cleaning process: Initial size of tokens = 1143\n",
      "Reduction due to punctuations and stopwords = 665.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 671\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3997\n",
      "Cleaning process: Initial size of tokens = 3997\n",
      "Reduction due to punctuations and stopwords = 2760.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2771\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3997\n",
      "Cleaning process: Initial size of tokens = 3997\n",
      "Reduction due to punctuations and stopwords = 2760.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2771\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2404\n",
      "Cleaning process: Initial size of tokens = 2404\n",
      "Reduction due to punctuations and stopwords = 1602.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1621\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2404\n",
      "Cleaning process: Initial size of tokens = 2404\n",
      "Reduction due to punctuations and stopwords = 1602.\n",
      "Reduction due to all numeral terms = 9\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1621\n",
      "Percentage = 67%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6694\n",
      "Cleaning process: Initial size of tokens = 6694\n",
      "Reduction due to punctuations and stopwords = 4841.\n",
      "Reduction due to all numeral terms = 39\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4898\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6694\n",
      "Cleaning process: Initial size of tokens = 6694\n",
      "Reduction due to punctuations and stopwords = 4841.\n",
      "Reduction due to all numeral terms = 39\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 13\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4898\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 13743\n",
      "Cleaning process: Initial size of tokens = 13743\n",
      "Reduction due to punctuations and stopwords = 10613.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 18\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 10643\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 13743\n",
      "Cleaning process: Initial size of tokens = 13743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to punctuations and stopwords = 10613.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 18\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 10643\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 752\n",
      "Cleaning process: Initial size of tokens = 752\n",
      "Reduction due to punctuations and stopwords = 453.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 455\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 752\n",
      "Cleaning process: Initial size of tokens = 752\n",
      "Reduction due to punctuations and stopwords = 453.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 455\n",
      "Percentage = 61%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 569\n",
      "Cleaning process: Initial size of tokens = 569\n",
      "Reduction due to punctuations and stopwords = 340.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 340\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 569\n",
      "Cleaning process: Initial size of tokens = 569\n",
      "Reduction due to punctuations and stopwords = 340.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 340\n",
      "Percentage = 60%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1526\n",
      "Cleaning process: Initial size of tokens = 1526\n",
      "Reduction due to punctuations and stopwords = 1073.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1074\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1526\n",
      "Cleaning process: Initial size of tokens = 1526\n",
      "Reduction due to punctuations and stopwords = 1073.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1074\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1993\n",
      "Cleaning process: Initial size of tokens = 1993\n",
      "Reduction due to punctuations and stopwords = 1365.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1370\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1993\n",
      "Cleaning process: Initial size of tokens = 1993\n",
      "Reduction due to punctuations and stopwords = 1365.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1370\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1616\n",
      "Cleaning process: Initial size of tokens = 1616\n",
      "Reduction due to punctuations and stopwords = 1023.\n",
      "Reduction due to all numeral terms = 11\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1047\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1616\n",
      "Cleaning process: Initial size of tokens = 1616\n",
      "Reduction due to punctuations and stopwords = 1023.\n",
      "Reduction due to all numeral terms = 11\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1047\n",
      "Percentage = 65%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 221\n",
      "Cleaning process: Initial size of tokens = 221\n",
      "Reduction due to punctuations and stopwords = 124.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 125\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 221\n",
      "Cleaning process: Initial size of tokens = 221\n",
      "Reduction due to punctuations and stopwords = 124.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 125\n",
      "Percentage = 57%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3832\n",
      "Cleaning process: Initial size of tokens = 3832\n",
      "Reduction due to punctuations and stopwords = 2857.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2867\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3832\n",
      "Cleaning process: Initial size of tokens = 3832\n",
      "Reduction due to punctuations and stopwords = 2857.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2867\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2853\n",
      "Cleaning process: Initial size of tokens = 2853\n",
      "Reduction due to punctuations and stopwords = 2029.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2055\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2853\n",
      "Cleaning process: Initial size of tokens = 2853\n",
      "Reduction due to punctuations and stopwords = 2029.\n",
      "Reduction due to all numeral terms = 15\n",
      "Reduction due to short terms = 11\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2055\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 501\n",
      "Cleaning process: Initial size of tokens = 501\n",
      "Reduction due to punctuations and stopwords = 289.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 292\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 501\n",
      "Cleaning process: Initial size of tokens = 501\n",
      "Reduction due to punctuations and stopwords = 289.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 292\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 510\n",
      "Cleaning process: Initial size of tokens = 510\n",
      "Reduction due to punctuations and stopwords = 276.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 281\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 510\n",
      "Cleaning process: Initial size of tokens = 510\n",
      "Reduction due to punctuations and stopwords = 276.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 281\n",
      "Percentage = 55%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4331\n",
      "Cleaning process: Initial size of tokens = 4331\n",
      "Reduction due to punctuations and stopwords = 3159.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3176\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4331\n",
      "Cleaning process: Initial size of tokens = 4331\n",
      "Reduction due to punctuations and stopwords = 3159.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3176\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4750\n",
      "Cleaning process: Initial size of tokens = 4750\n",
      "Reduction due to punctuations and stopwords = 3255.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3270\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4750\n",
      "Cleaning process: Initial size of tokens = 4750\n",
      "Reduction due to punctuations and stopwords = 3255.\n",
      "Reduction due to all numeral terms = 5\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 7\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3270\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3760\n",
      "Cleaning process: Initial size of tokens = 3760\n",
      "Reduction due to punctuations and stopwords = 2723.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2738\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 3760\n",
      "Cleaning process: Initial size of tokens = 3760\n",
      "Reduction due to punctuations and stopwords = 2723.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2738\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 230\n",
      "Cleaning process: Initial size of tokens = 230\n",
      "Reduction due to punctuations and stopwords = 130.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 135\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 230\n",
      "Cleaning process: Initial size of tokens = 230\n",
      "Reduction due to punctuations and stopwords = 130.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 135\n",
      "Percentage = 59%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1165\n",
      "Cleaning process: Initial size of tokens = 1165\n",
      "Reduction due to punctuations and stopwords = 722.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 729\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1165\n",
      "Cleaning process: Initial size of tokens = 1165\n",
      "Reduction due to punctuations and stopwords = 722.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 3\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 729\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6117\n",
      "Cleaning process: Initial size of tokens = 6117\n",
      "Reduction due to punctuations and stopwords = 4539.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4557\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 6117\n",
      "Cleaning process: Initial size of tokens = 6117\n",
      "Reduction due to punctuations and stopwords = 4539.\n",
      "Reduction due to all numeral terms = 8\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 6\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4557\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1645\n",
      "Cleaning process: Initial size of tokens = 1645\n",
      "Reduction due to punctuations and stopwords = 1131.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1152\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1645\n",
      "Cleaning process: Initial size of tokens = 1645\n",
      "Reduction due to punctuations and stopwords = 1131.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 9\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1152\n",
      "Percentage = 70%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1678\n",
      "Cleaning process: Initial size of tokens = 1678\n",
      "Reduction due to punctuations and stopwords = 1049.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1053\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1678\n",
      "Cleaning process: Initial size of tokens = 1678\n",
      "Reduction due to punctuations and stopwords = 1049.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1053\n",
      "Percentage = 63%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 528\n",
      "Cleaning process: Initial size of tokens = 528\n",
      "Reduction due to punctuations and stopwords = 303.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 307\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 528\n",
      "Cleaning process: Initial size of tokens = 528\n",
      "Reduction due to punctuations and stopwords = 303.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 1\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 307\n",
      "Percentage = 58%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2702\n",
      "Cleaning process: Initial size of tokens = 2702\n",
      "Reduction due to punctuations and stopwords = 1947.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1950\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2702\n",
      "Cleaning process: Initial size of tokens = 2702\n",
      "Reduction due to punctuations and stopwords = 1947.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1950\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7104\n",
      "Cleaning process: Initial size of tokens = 7104\n",
      "Reduction due to punctuations and stopwords = 5297.\n",
      "Reduction due to all numeral terms = 17\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5327\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7104\n",
      "Cleaning process: Initial size of tokens = 7104\n",
      "Reduction due to punctuations and stopwords = 5297.\n",
      "Reduction due to all numeral terms = 17\n",
      "Reduction due to short terms = 12\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 5327\n",
      "Percentage = 75%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5957\n",
      "Cleaning process: Initial size of tokens = 5957\n",
      "Reduction due to punctuations and stopwords = 4281.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4301\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5957\n",
      "Cleaning process: Initial size of tokens = 5957\n",
      "Reduction due to punctuations and stopwords = 4281.\n",
      "Reduction due to all numeral terms = 6\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 8\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 4301\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2004\n",
      "Cleaning process: Initial size of tokens = 2004\n",
      "Reduction due to punctuations and stopwords = 1323.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1328\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2004\n",
      "Cleaning process: Initial size of tokens = 2004\n",
      "Reduction due to punctuations and stopwords = 1323.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1328\n",
      "Percentage = 66%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4040\n",
      "Cleaning process: Initial size of tokens = 4040\n",
      "Reduction due to punctuations and stopwords = 2929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2942\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4040\n",
      "Cleaning process: Initial size of tokens = 4040\n",
      "Reduction due to punctuations and stopwords = 2929.\n",
      "Reduction due to all numeral terms = 4\n",
      "Reduction due to short terms = 8\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2942\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5001\n",
      "Cleaning process: Initial size of tokens = 5001\n",
      "Reduction due to punctuations and stopwords = 3624.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3637\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 5001\n",
      "Cleaning process: Initial size of tokens = 5001\n",
      "Reduction due to punctuations and stopwords = 3624.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 9\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 2\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3637\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4971\n",
      "Cleaning process: Initial size of tokens = 4971\n",
      "Reduction due to punctuations and stopwords = 3654.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3665\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4971\n",
      "Cleaning process: Initial size of tokens = 4971\n",
      "Reduction due to punctuations and stopwords = 3654.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 7\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 3665\n",
      "Percentage = 74%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7966\n",
      "Cleaning process: Initial size of tokens = 7966\n",
      "Reduction due to punctuations and stopwords = 6152.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6163\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 7966\n",
      "Cleaning process: Initial size of tokens = 7966\n",
      "Reduction due to punctuations and stopwords = 6152.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 5\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 4\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 6163\n",
      "Percentage = 77%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2219\n",
      "Cleaning process: Initial size of tokens = 2219\n",
      "Reduction due to punctuations and stopwords = 1573.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1578\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 2219\n",
      "Cleaning process: Initial size of tokens = 2219\n",
      "Reduction due to punctuations and stopwords = 1573.\n",
      "Reduction due to all numeral terms = 1\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1578\n",
      "Percentage = 71%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1563\n",
      "Cleaning process: Initial size of tokens = 1563\n",
      "Reduction due to punctuations and stopwords = 1068.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1072\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1563\n",
      "Cleaning process: Initial size of tokens = 1563\n",
      "Reduction due to punctuations and stopwords = 1068.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 3\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1072\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4063\n",
      "Cleaning process: Initial size of tokens = 4063\n",
      "Reduction due to punctuations and stopwords = 2960.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2974\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 4063\n",
      "Cleaning process: Initial size of tokens = 4063\n",
      "Reduction due to punctuations and stopwords = 2960.\n",
      "Reduction due to all numeral terms = 7\n",
      "Reduction due to short terms = 6\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 1\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 2974\n",
      "Percentage = 73%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 551\n",
      "Cleaning process: Initial size of tokens = 551\n",
      "Reduction due to punctuations and stopwords = 338.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 340\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 551\n",
      "Cleaning process: Initial size of tokens = 551\n",
      "Reduction due to punctuations and stopwords = 338.\n",
      "Reduction due to all numeral terms = 2\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 340\n",
      "Percentage = 62%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1301\n",
      "Cleaning process: Initial size of tokens = 1301\n",
      "Reduction due to punctuations and stopwords = 872.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 879\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1301\n",
      "Cleaning process: Initial size of tokens = 1301\n",
      "Reduction due to punctuations and stopwords = 872.\n",
      "Reduction due to all numeral terms = 3\n",
      "Reduction due to short terms = 4\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 879\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/bulentozel/OpenMaker/blob/master/Semantics/data/corpuses/schwartz.json\n",
    "filepath = 'schwartz.json'\n",
    "\n",
    "data = read_data(filepath)\n",
    "corpus = extract_corpus(data)\n",
    "corpus = preprocess_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.79s.\n",
      "Fitting the NMF model (kullback-leibler) with tf-idf features, n_samples=494 and n_features=36456...\n",
      "done in 21.72s.\n"
     ]
    }
   ],
   "source": [
    "nmf, W_train, tfidf_train, tfidf_vectorizer = train_corpus(corpus, data, n_topics=3, betaloss = 'kullback-leibler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mmajor south new show materi\n",
      "\u001b[1mTopic #1: \u001b[0mimport devi goal speci gener\n",
      "\u001b[1mTopic #2: \u001b[0mgovern government speech scale place\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #3: \u001b[0mphysiolog tie unstabl score medicin\n",
      "\u001b[1mTopic #4: \u001b[0minterest redirect show worri support\n",
      "\u001b[1mTopic #5: \u001b[0mphilosoph other valu social uncertain\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #6: \u001b[0minstitut staff inher resembl report\n",
      "\u001b[1mTopic #7: \u001b[0mrole use structur maynard individu\n",
      "\u001b[1mTopic #8: \u001b[0minstruct person number properti reconfigur\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #9: \u001b[0mhelp argument compani hierarchi use\n",
      "\u001b[1mTopic #10: \u001b[0mvision opposit parentif toxic topic\n",
      "\u001b[1mTopic #11: \u001b[0mtrobe technic best resist socio\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #12: \u001b[0mphotographi brunei total capac valu\n",
      "\u001b[1mTopic #13: \u001b[0muse support certainli influenti taken\n",
      "\u001b[1mTopic #14: \u001b[0mprofit behavior seborga kyong frustrat\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #15: \u001b[0msing magnitud valu horror angel\n",
      "\u001b[1mTopic #16: \u001b[0mthu live polici still north\n",
      "\u001b[1mTopic #17: \u001b[0muse man turn seek attain\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #18: \u001b[0mpart make summar mose cingul\n",
      "\u001b[1mTopic #19: \u001b[0mshow two sanction seen specif\n",
      "\u001b[1mTopic #20: \u001b[0mweak fourth undesir without cours\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #21: \u001b[0mserv found esteem never kshatriya\n",
      "\u001b[1mTopic #22: \u001b[0manthoni sikh interconnect sweterlitsch sourc\n",
      "\u001b[1mTopic #23: \u001b[0mtheme molin spirit sometim mobil\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #24: \u001b[0mseeker seafront practic feel skin\n",
      "\u001b[1mTopic #25: \u001b[0mroman impuls outward shore five\n",
      "\u001b[1mTopic #26: \u001b[0mprincipl investig hybrid princess platform\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #27: \u001b[0mnoun serv dustbin respect type\n",
      "\u001b[1mTopic #28: \u001b[0mspecif polit john smaller left\n",
      "\u001b[1mTopic #29: \u001b[0mfield identifi practic templat dioxid\n",
      "\n",
      "\u001b[96m\u001b[1mGeneral\u001b[0m\n",
      "\u001b[1mTopic #30: \u001b[0mtwo western suprem learn note\n",
      "\u001b[1mTopic #31: \u001b[0mtechnic order europ opportun among\n",
      "\u001b[1mTopic #32: \u001b[0mtradit result system term structur\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "print_top_words(nmf, tfidf_vectorizer, n_top_words=5, n_topics=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution over sub-topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 0\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 1\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 2\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 3\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 4\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 5\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 6\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 7\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.05)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 8\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 9\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 10\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 11\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 12\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 13\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 14\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 15\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  \n",
      "\u001b[1mDoc 16\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.09)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 17\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 18\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.02)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 19\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 20\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 21\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 22\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 23\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 24\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 25\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 26\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 27\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.07)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 28\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 29\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.06)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  \n",
      "\u001b[1mDoc 30\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  \n",
      "\u001b[1mDoc 31\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 32\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.02)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 33\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 34\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 35\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 36\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 37\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 38\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 39\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 40\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 41\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  \n",
      "\u001b[1mDoc 42\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 43\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 44\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 45\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 46\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 47\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 48\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 49\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.07)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 50\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 51\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 52\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 53\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  \n",
      "\u001b[1mDoc 54\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  \n",
      "\u001b[1mDoc 55\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 56\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 57\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 58\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 59\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 60\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 61\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 62\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 63\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 64\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 65\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 66\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 67\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.07)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 68\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 69\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 70\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 71\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 72\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 73\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 74\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 75\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 76\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 77\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 78\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 79\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 80\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 81\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 82\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.07)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 83\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 84\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 85\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 86\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 87\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 88\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 89\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 90\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 91\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 92\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 93\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 94\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 95\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 96\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 97\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 98\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 99\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 100\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 101\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 102\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 103\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 104\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 105\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 106\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 107\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.07)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 108\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 109\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 110\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 111\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 112\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 113\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 114\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 115\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 116\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 117\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 118\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 119\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 120\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 121\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  \n",
      "\u001b[1mDoc 122\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 123\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 124\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 125\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 126\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 127\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 128\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 129\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 130\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 131\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 132\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 133\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 134\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 135\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 136\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 137\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 138\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 139\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 140\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 141\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 142\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 143\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 144\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 145\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 146\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 147\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 148\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  \n",
      "\u001b[1mDoc 149\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 150\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 151\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 152\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 153\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 154\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 155\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 156\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 157\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 158\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 159\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  \n",
      "\u001b[1mDoc 160\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 161\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 162\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 163\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 164\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 165\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 166\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  \n",
      "\u001b[1mDoc 167\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 168\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 169\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 170\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 171\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.05)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 172\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 173\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 174\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 175\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  \n",
      "\u001b[1mDoc 176\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 177\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 178\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 179\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  \n",
      "\u001b[1mDoc 180\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 181\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 182\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 183\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.07)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 184\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.04)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.03)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 185\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  \n",
      "\u001b[1mDoc 186\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.09)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 187\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.07)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 188\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 189\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 190\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 191\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 192\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.11)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 193\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 194\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.06)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 195\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 196\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 197\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  \n",
      "\u001b[1mDoc 198\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.06)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 199\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 200\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.03)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  \n",
      "\u001b[1mDoc 201\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 202\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.06)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 203\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 204\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 205\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 206\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  \n",
      "\u001b[1mDoc 207\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 208\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 209\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  \n",
      "\u001b[1mDoc 210\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 211\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  \n",
      "\u001b[1mDoc 212\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 213\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.08)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 214\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 215\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 216\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 217\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.07)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 218\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 219\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 220\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.04)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.01)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 221\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  \n",
      "\u001b[1mDoc 222\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.04)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.04)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 223\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.03)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 224\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 225\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 226\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 227\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 228\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 229\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.07)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 230\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.04)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 231\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 232\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 233\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 234\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 235\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 236\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 237\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.05)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.03)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 238\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.06)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 239\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 240\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  \n",
      "\u001b[1mDoc 241\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 242\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.05)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 243\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.05)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 244\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.05)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  \n",
      "\u001b[1mDoc 245\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  \n",
      "\u001b[1mDoc 246\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 247\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 248\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 249\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 250\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 251\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  \n",
      "\u001b[1mDoc 252\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 253\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 254\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 255\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 256\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.05)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 257\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 258\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 259\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 260\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 3, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 5, 0.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 4, 0.00)  \n",
      "\u001b[1mDoc 261\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.15)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 262\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.06)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 263\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 264\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  \n",
      "\u001b[1mDoc 265\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.12)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 266\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 267\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  \n",
      "\u001b[1mDoc 268\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  \n",
      "\u001b[1mDoc 269\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.08)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 270\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.12)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 271\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 272\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 273\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 274\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 275\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.12)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 276\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  \n",
      "\u001b[1mDoc 277\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  \n",
      "\u001b[1mDoc 278\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.16)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.01)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 279\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.08)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  \n",
      "\u001b[1mDoc 280\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 281\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  \n",
      "\u001b[1mDoc 282\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.17)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 283\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.09)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.01)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 284\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.14)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 285\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.17)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 286\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  \n",
      "\u001b[1mDoc 287\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.06)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 288\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  \n",
      "\u001b[1mDoc 289\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.09)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 290\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.15)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  \n",
      "\u001b[1mDoc 291\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.14)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 292\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.09)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 293\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  \n",
      "\u001b[1mDoc 294\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.11)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 295\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 296\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  \n",
      "\u001b[1mDoc 297\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.06)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 298\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.16)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 299\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.11)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 300\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.13)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  \n",
      "\u001b[1mDoc 301\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.06)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 302\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  \n",
      "\u001b[1mDoc 303\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 304\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.16)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 305\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.11)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 306\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.13)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 307\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 6, 0.05)  (\u001b[96m\u001b[1machievement\u001b[0m, 7, 0.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 8, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 308\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.30)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 309\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.25)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 310\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.15)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 311\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.58)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 312\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.28)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 313\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 314\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.33)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 315\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 316\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.00)  \n",
      "\u001b[1mDoc 317\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.29)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 318\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.15)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 319\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 320\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.23)  (\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 321\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.32)  (\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 322\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 323\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.15)  (\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 324\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 325\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.34)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 326\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 10, 0.00)  \n",
      "\u001b[1mDoc 327\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 328\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.00)  \n",
      "\u001b[1mDoc 329\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 330\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.18)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 331\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.01)  \n",
      "\u001b[1mDoc 332\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.21)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 333\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 334\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 335\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 336\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.22)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 337\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 338\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.13)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 339\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.16)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 340\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.19)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 341\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.10)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 342\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.26)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  \n",
      "\u001b[1mDoc 343\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 344\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 345\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  \n",
      "\u001b[1mDoc 346\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 347\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  \n",
      "\u001b[1mDoc 348\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.22)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 349\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.21)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 350\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 351\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 352\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 353\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.16)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 354\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.11)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 355\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 356\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.23)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 357\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.20)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 358\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 359\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.00)  \n",
      "\u001b[1mDoc 360\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 13, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.00)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 12, 0.00)  \n",
      "\u001b[1mDoc 361\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 362\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 363\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 364\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 365\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 366\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 367\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 368\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.15)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 369\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.00)  \n",
      "\u001b[1mDoc 370\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  \n",
      "\u001b[1mDoc 371\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 372\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 373\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 374\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.08)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 375\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 376\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  \n",
      "\u001b[1mDoc 377\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 378\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 379\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.22)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 380\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 381\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 382\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  \n",
      "\u001b[1mDoc 383\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.08)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  \n",
      "\u001b[1mDoc 384\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.09)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.01)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 385\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 386\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.11)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 387\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.13)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.01)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 388\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 389\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  \n",
      "\u001b[1mDoc 390\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 391\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 392\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.15)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 393\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 394\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  \n",
      "\u001b[1mDoc 395\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 396\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 397\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  \n",
      "\u001b[1mDoc 398\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 399\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.00)  \n",
      "\u001b[1mDoc 400\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 401\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.13)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.00)  \n",
      "\u001b[1mDoc 402\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.11)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 403\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.14)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 404\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.11)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 405\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 406\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 16, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 17, 0.00)  \n",
      "\u001b[1mDoc 407\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 408\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 409\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 410\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  \n",
      "\u001b[1mDoc 411\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  \n",
      "\u001b[1mDoc 412\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  \n",
      "\u001b[1mDoc 413\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 414\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.23)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 415\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 416\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 417\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.19)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 418\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.16)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 419\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.10)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 420\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 421\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.17)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 422\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.11)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  \n",
      "\u001b[1mDoc 423\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  \n",
      "\u001b[1mDoc 424\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.10)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 425\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.24)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 426\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.09)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 427\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  \n",
      "\u001b[1mDoc 428\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 429\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 430\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.22)  (\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 431\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 432\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 433\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 434\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 435\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.21)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 436\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  \n",
      "\u001b[1mDoc 437\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 19, 0.12)  (\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.06)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 438\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 439\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 20, 0.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 440\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 21, 0.31)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 441\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.35)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 442\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 21, 0.29)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 443\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  \n",
      "\u001b[1mDoc 444\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 21, 0.54)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  (\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 445\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.41)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 446\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.30)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 447\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.34)  (\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 448\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.25)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  \n",
      "\u001b[1mDoc 449\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.19)  (\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 450\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.20)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 451\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 21, 0.55)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 452\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.11)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 453\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.29)  (\u001b[96m\u001b[1mtradition\u001b[0m, 21, 0.00)  (\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 454\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.00)  \n",
      "\u001b[1mDoc 455\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.35)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 456\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 23, 0.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mtradition\u001b[0m, 22, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 457\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 24, 0.50)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 25, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 458\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 25, 0.63)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 459\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 25, 0.98)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 24, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 460\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 24, 0.47)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 461\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 26, 1.45)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 462\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 24, 0.31)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 463\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 24, 0.43)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 464\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 465\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.00)  \n",
      "\u001b[1mDoc 466\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  \n",
      "\u001b[1mDoc 467\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 468\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 469\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 470\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.11)  (\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 471\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.16)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 472\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 473\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.26)  (\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 474\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.24)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 475\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 476\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.19)  (\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 477\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 478\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.13)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 479\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 480\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.21)  (\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 481\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 482\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 483\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 484\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 485\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  \n",
      "\u001b[1mDoc 486\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  \n",
      "\u001b[1mDoc 487\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.26)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 488\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 489\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.15)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 490\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.11)  (\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  \n",
      "\u001b[1mDoc 491\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 28, 0.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 31, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n",
      "\u001b[1mDoc 492\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.12)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.00)  \n",
      "\u001b[1mDoc 493\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.13)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 32, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 27, 0.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 30, 0.00)  \n"
     ]
    }
   ],
   "source": [
    "res = [print_doc_topics(W_train, idx, 4) for idx,row in enumerate(W_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up sub topics\n",
    "W_train_cumul = cumulate_W(W_train, n_topics=3)\n",
    "W_train_norm = normalize_W(W_train_cumul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution over main Schwartz topics. (Summed over sub-topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 0\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.99)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 1\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 84.78)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 15.22)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 2\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 92.92)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.08)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 3\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.60)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.40)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 4\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.63)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.37)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 5\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 6\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 66.64)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 33.36)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 7\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.69)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.31)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 8\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 68.78)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 31.22)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 9\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 73.65)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 26.35)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 10\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 85.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 14.79)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 11\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 89.84)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 10.16)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 12\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 82.65)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 17.35)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 13\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 95.54)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.46)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 14\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.98)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 15\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 78.70)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 21.30)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 16\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 90.39)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 9.61)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 17\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 77.36)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 22.64)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 18\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.75)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.25)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 19\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 20\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 21\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 65.46)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 34.54)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 22\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 92.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.80)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 23\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 97.57)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.43)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 24\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 76.20)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 23.80)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 25\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 64.27)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 35.73)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 26\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 88.87)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 11.13)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 27\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.30)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.70)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 28\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 81.91)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 18.09)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 29\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 82.22)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 17.78)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 30\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 77.41)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.59)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 31\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 97.13)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 2.87)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 32\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 59.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 40.93)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 33\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 34\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 94.87)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.13)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 35\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.30)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.70)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 36\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 64.02)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 35.98)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 37\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 98.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 38\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 74.29)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 25.71)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 39\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 85.57)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 14.43)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 40\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 77.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.79)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 41\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 64.34)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 35.66)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 42\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 83.22)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 16.78)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 43\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.96)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 44\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 95.35)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.65)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 45\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 46\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 84.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 15.79)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 47\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 48\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 49\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 88.66)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 11.34)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 50\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.75)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.25)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 51\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 82.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 17.84)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 52\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 75.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 24.80)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 53\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 66.87)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 33.13)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 54\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 60.77)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 39.23)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 55\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 53.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 46.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 56\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 66.68)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 33.32)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 57\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 98.27)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.73)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 58\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 65.48)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 34.52)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 59\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 60\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 90.58)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.42)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 61\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 89.75)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 10.25)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 62\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 73.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 26.79)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 63\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 94.89)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 5.11)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 64\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.95)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 65\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 77.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 66\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 91.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 67\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 68\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 84.92)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 15.08)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 69\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 89.49)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 10.51)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 70\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 71\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 72\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.99)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 73\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.99)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 74\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.99)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 75\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 76\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 87.49)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 12.51)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 77\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 89.47)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 10.53)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 78\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 85.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 14.94)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 79\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.99)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 80\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 98.25)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 1.75)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 81\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 97.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.96)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 82\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 93.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 6.76)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 83\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 84\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 85\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 86\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 85.63)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 14.37)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 87\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 68.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 31.92)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 88\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 89\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 90\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 92.50)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.50)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 91\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 92\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 93\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 68.70)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 31.30)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 94\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 75.75)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 24.25)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 95\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 96\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 96.32)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 3.68)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 97\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.81)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.19)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 98\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 99\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 82.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 17.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 100\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 61.82)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 38.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 101\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 77.88)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.12)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 102\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.96)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.04)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 103\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.97)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.03)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 104\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 79.78)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 20.22)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 105\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 77.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.76)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 106\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 81.73)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 18.27)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 107\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.84)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 108\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 77.51)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.49)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 109\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 68.70)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 31.30)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 110\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 111\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 112\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 95.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.84)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 113\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.84)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.16)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 114\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 92.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.94)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 115\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.79)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.21)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 116\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 117\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 95.22)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 4.78)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 118\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 119\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 120\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 86.04)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 13.96)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 121\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 71.55)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 28.45)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 122\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 123\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 124\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 98.79)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.21)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 125\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 94.91)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.09)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 126\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 77.78)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.22)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 127\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 87.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 12.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 128\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 129\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 130\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 93.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 6.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 131\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 84.51)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 15.49)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 132\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 80.21)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 19.79)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 133\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 73.17)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 26.83)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 134\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 135\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 136\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 65.59)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 34.41)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 137\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 87.40)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 12.60)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 138\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.92)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.08)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 139\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 94.43)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 5.57)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 140\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 141\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 142\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 98.63)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.37)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 143\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 144\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 69.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 30.82)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 145\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 76.84)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 23.16)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 146\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 147\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 54.68)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 45.32)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 148\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 66.69)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 33.31)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 149\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 90.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.76)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 150\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 90.56)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.44)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 151\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 97.81)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.19)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 152\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 52.29)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 47.71)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 153\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 99.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.95)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 154\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 155\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 80.20)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 19.80)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 156\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 88.96)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 11.04)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 157\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 71.76)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 28.24)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 158\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 81.28)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 18.72)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 159\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 75.41)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 24.59)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 160\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 72.74)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 27.26)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 161\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 64.25)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 35.75)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 162\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 95.82)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 4.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 163\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 96.52)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 3.48)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 164\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 62.40)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 37.60)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 165\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 89.61)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 10.39)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 166\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.95)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 167\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 64.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 35.91)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 168\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.92)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.08)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 169\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 81.75)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 18.25)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 170\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 171\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 89.21)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 10.79)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 172\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 173\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 78.71)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 21.29)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 174\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.05)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.95)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 175\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 176\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 98.91)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.09)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 177\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 82.61)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 17.39)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 178\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 97.72)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 2.28)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 179\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 180\u001b[91m (universalism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 181\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 80.62)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 19.38)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 182\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 183\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 184\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 97.58)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.42)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 185\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 80.54)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 19.46)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 186\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 94.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.81)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 187\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.82)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 188\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 88.36)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 11.64)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 189\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 78.57)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 21.43)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 190\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 94.65)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.35)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 191\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 92.64)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.36)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 192\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 96.78)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.22)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 193\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 73.73)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 26.27)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 194\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.29)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.71)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 195\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.91)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.09)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 196\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 197\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 75.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 25.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 198\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 199\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 200\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 78.75)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 21.25)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 201\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 95.77)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.23)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 202\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.93)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 203\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 77.29)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 22.71)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 204\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 71.87)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 28.13)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 205\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 98.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 206\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 91.55)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 8.45)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 207\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 90.86)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.14)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 208\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 89.86)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 10.14)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 209\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.47)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.53)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 210\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 66.47)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 33.53)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 211\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 212\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 213\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 97.46)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.54)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 214\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 79.82)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 20.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 215\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 216\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 217\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 96.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.86)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 218\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 90.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.90)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 219\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 220\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 221\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 222\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 50.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 49.84)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 223\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 57.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 42.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 224\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 225\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 92.74)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.26)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 226\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 227\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.96)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.04)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 228\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 229\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 91.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 8.90)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 230\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.44)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.56)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 231\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 232\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 95.51)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.49)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 233\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 80.90)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 19.10)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 234\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 95.69)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.31)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 235\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 236\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 80.67)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 19.33)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 237\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 238\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 93.08)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 6.92)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 239\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 240\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 241\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 92.77)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.23)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 242\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 98.82)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 243\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.98)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 244\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 245\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 246\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 96.15)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.85)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 247\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 94.89)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.11)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 248\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 98.32)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.68)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 249\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 90.75)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.25)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 250\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 251\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 97.81)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.19)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 252\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 92.37)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.63)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 253\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 90.52)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.48)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 254\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.86)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.14)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 255\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 256\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 91.05)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 8.95)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 257\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 96.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.82)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 258\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 97.31)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.69)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 259\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.97)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.03)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 260\u001b[91m (hedonism)\t: \u001b[0m(\u001b[96m\u001b[1mhedonism\u001b[0m, 1, 99.79)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.21)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 261\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 98.27)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.73)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 262\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 263\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 264\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 96.82)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 265\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.70)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.30)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 266\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.88)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.12)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 267\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 268\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 98.64)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.36)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 269\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 270\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 271\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.26)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.74)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 272\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 94.67)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.33)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 273\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.97)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 274\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.86)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.14)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 275\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.68)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.32)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 276\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.76)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.24)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 277\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.42)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.58)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 278\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.49)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.51)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 279\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 280\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 98.26)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.74)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 281\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 282\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 283\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 96.41)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.59)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 284\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 91.23)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 8.77)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 285\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 286\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.18)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.82)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 287\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 288\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 90.90)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.10)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 289\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 290\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.66)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.34)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 291\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.97)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.03)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 292\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.63)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.37)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 293\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1machievement\u001b[0m, 2, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 294\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.86)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.14)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 295\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 98.45)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.55)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 296\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 297\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 298\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 299\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 300\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.48)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.52)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 301\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 302\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 98.59)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.41)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 303\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.76)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.24)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 304\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.65)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.35)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 305\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 99.78)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.22)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 306\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 97.30)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.70)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 307\u001b[91m (achievement)\t: \u001b[0m(\u001b[96m\u001b[1machievement\u001b[0m, 2, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 308\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 98.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.76)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 309\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.76)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.24)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 310\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.88)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.12)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 311\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 312\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.96)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.04)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 313\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.97)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.03)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 314\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.79)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.21)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 315\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.91)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.09)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 316\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 90.01)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.99)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 317\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.25)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.75)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 318\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.91)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.09)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 319\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 320\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 321\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 322\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.72)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.28)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 323\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 324\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 97.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.97)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 325\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 326\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 327\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 94.47)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.53)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 328\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 329\u001b[91m (power)\t: \u001b[0m(\u001b[96m\u001b[1mpower\u001b[0m, 3, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 330\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 331\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 81.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 18.80)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 332\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 99.96)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.04)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 333\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 94.68)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.32)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 334\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 335\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 336\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 337\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 338\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 339\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 340\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 341\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 342\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 99.29)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.71)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 343\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 344\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 345\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 96.87)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.13)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 346\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 99.93)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.07)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 347\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 98.90)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.10)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 348\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 349\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 97.16)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.84)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 350\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 96.35)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.65)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 351\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 352\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 96.50)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.50)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 353\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 354\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 93.72)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 6.28)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 355\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 97.67)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.33)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 356\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 96.72)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.28)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 357\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 358\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 95.33)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.67)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 359\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 98.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 360\u001b[91m (self-direction)\t: \u001b[0m(\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 361\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 362\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 91.78)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 8.22)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 363\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 364\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.76)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.24)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 365\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.93)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.07)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 366\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 90.31)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.69)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 367\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 87.39)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 12.61)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 368\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 88.44)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 11.56)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 369\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 98.29)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.71)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 370\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 371\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 372\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 76.36)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 23.64)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 373\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 94.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 374\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 375\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 96.03)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.97)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 376\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 377\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 93.42)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 6.58)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 378\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 94.57)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.43)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 379\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 96.55)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.45)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 380\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 67.62)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 32.38)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 381\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 93.40)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 6.60)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 382\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.26)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.74)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 383\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 384\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 96.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 385\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 386\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.96)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.04)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 387\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 97.30)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.70)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 388\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 87.77)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 12.23)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 389\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.46)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.54)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 390\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.92)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.08)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 391\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 392\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 95.65)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.35)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 393\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.81)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.19)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 394\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 100.00)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 395\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 93.58)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 6.42)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 396\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 397\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 99.93)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 0.07)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 398\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 99.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 399\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 400\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 98.88)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.12)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 401\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 98.83)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.17)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 402\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 92.93)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.07)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 403\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 404\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 405\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 95.24)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.76)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 406\u001b[91m (benevolence)\t: \u001b[0m(\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 407\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 97.92)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.08)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 408\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 409\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 410\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 97.77)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.23)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 411\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 412\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 413\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 414\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 415\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 97.57)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.43)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 416\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 95.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.90)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 417\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 418\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 92.14)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 7.86)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 419\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 420\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 421\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 422\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 423\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 98.69)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.31)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 424\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 425\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 426\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 90.19)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 9.81)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 427\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 99.95)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.05)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 428\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 99.20)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.80)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 429\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 430\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 95.82)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.18)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 431\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 96.31)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.69)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 432\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 99.37)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.63)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 433\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 97.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 434\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 435\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 97.65)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.35)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 436\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 99.65)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.35)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 437\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 438\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 439\u001b[91m (conformity)\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 440\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 441\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 98.30)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 1.70)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 442\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 97.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 443\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 95.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 444\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 445\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 99.68)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.32)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 446\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 96.89)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 3.11)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 447\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 99.88)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.12)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 448\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 449\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 450\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 451\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 99.94)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.06)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 452\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 453\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 454\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 455\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 99.99)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.01)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 456\u001b[91m (tradition)\t: \u001b[0m(\u001b[96m\u001b[1mtradition\u001b[0m, 7, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 457\u001b[91m (stimulation)\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 458\u001b[91m (stimulation)\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 99.57)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.43)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 459\u001b[91m (stimulation)\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 460\u001b[91m (stimulation)\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 99.07)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.93)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 461\u001b[91m (stimulation)\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 462\u001b[91m (stimulation)\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 463\u001b[91m (stimulation)\t: \u001b[0m(\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.00)  \n",
      "\u001b[1mDoc 464\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.28)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.72)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 465\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.96)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.04)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 466\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.92)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.08)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 467\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 468\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 97.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.98)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 469\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.90)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.10)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 470\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 471\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 472\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.28)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.72)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 473\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 474\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 475\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 476\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 477\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.26)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.74)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 478\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 479\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.98)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.02)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 480\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 481\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.02)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.98)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 482\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 97.10)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.90)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 483\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 94.67)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.33)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 484\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.88)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.12)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 485\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 97.71)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 2.29)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 486\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.94)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 487\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.09)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.91)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 488\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 94.76)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 5.24)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 489\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.68)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.32)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 490\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 99.06)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.94)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 491\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 95.88)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 4.12)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 492\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n",
      "\u001b[1mDoc 493\u001b[91m (security)\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 100.00)  (\u001b[96m\u001b[1mGeneral\u001b[0m, 10, 0.00)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.00)  \n"
     ]
    }
   ],
   "source": [
    "res = [print_cumulative_train_doc_topics(data, W_train_norm, idx, 3) for idx,row in enumerate(W_train_norm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Different Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate your documents, simply append them to _docs list_ as a whole string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two example documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "f = open(\"pope.txt\", \"r\") #Pope ted talk, https://www.ted.com/speakers/pope_francis\n",
    "pope = f.read()\n",
    "docs.append(pope)\n",
    "f.close()\n",
    "\n",
    "f = open(\"dod.txt\", \"r\")  # US Department of Defense, https://www.defense.gov/About/\n",
    "dod = f.read()\n",
    "docs.append(dod)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1339.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1339\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1339.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1339\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 557.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 577\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Configuring the text cleaner ...\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 557.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 577\n",
      "Percentage = 69%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "test_corpus = preprocess_corpus(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.00s.\n",
      "Fitting the NMF model (kullback-leibler) with tf-idf features, \n",
      "done in 0.33s.\n"
     ]
    }
   ],
   "source": [
    "W_test, tfidf_test = evaluate_docs(test_corpus, nmf, tfidf_vectorizer, betaloss = 'kullback-leibler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution over sub-topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 0\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 18, 0.03)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 15, 0.02)  (\u001b[96m\u001b[1mpower\u001b[0m, 9, 0.02)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 14, 0.02)  \n",
      "\u001b[1mDoc 1\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 29, 0.03)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 1, 0.02)  (\u001b[96m\u001b[1mpower\u001b[0m, 11, 0.01)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 25, 0.01)  \n"
     ]
    }
   ],
   "source": [
    "res = [print_doc_topics(W_test, idx, 4) for idx,row in enumerate(W_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test_cumul = cumulate_W(W_test, n_topics=3)\n",
    "W_test_norm = normalize_W(W_test_cumul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution over main Schwartz topics. (Summed over sub-topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 0\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 0.04)  (\u001b[96m\u001b[1mpower\u001b[0m, 3, 0.04)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 0.04)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 0.02)  \n",
      "\u001b[1mDoc 1\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 0.04)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 0.02)  (\u001b[96m\u001b[1mpower\u001b[0m, 3, 0.01)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 0.01)  \n"
     ]
    }
   ],
   "source": [
    "res = [print_cumulative_test_doc_topics(W_test_cumul, idx, 4) for idx,row in enumerate(W_test_cumul)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDoc 0\t: \u001b[0m(\u001b[96m\u001b[1mconformity\u001b[0m, 6, 21.98)  (\u001b[96m\u001b[1mpower\u001b[0m, 3, 19.88)  (\u001b[96m\u001b[1mbenevolence\u001b[0m, 5, 19.22)  (\u001b[96m\u001b[1mself-direction\u001b[0m, 4, 10.58)  \n",
      "\u001b[1mDoc 1\t: \u001b[0m(\u001b[96m\u001b[1msecurity\u001b[0m, 9, 37.78)  (\u001b[96m\u001b[1muniversalism\u001b[0m, 0, 18.08)  (\u001b[96m\u001b[1mpower\u001b[0m, 3, 13.56)  (\u001b[96m\u001b[1mstimulation\u001b[0m, 8, 9.84)  \n"
     ]
    }
   ],
   "source": [
    "res = [print_cumulative_test_doc_topics(W_test_norm, idx, 4) for idx,row in enumerate(W_test_norm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25bb7dc6780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_radar_chart(W_test_norm, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
